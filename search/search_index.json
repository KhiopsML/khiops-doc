{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"contact/","title":"Contact Us","text":""},{"location":"legalmatters/","title":"Legal matters","text":""},{"location":"legalmatters/#editor-and-hosting","title":"Editor and hosting:","text":"<p>Orange SA Public Company with registered share capital of Euro 10,640,226,396</p> <p>RCS Nanterre 380 129 866</p> <p>Headquarter: 111 quai du Pr\u00e9sident Roosevelt, 92130 Issy-les-Moulineaux</p> <p>Phone number : + 33 (0)1 44 44 22 22</p> <p>Publishing Director: Christel HEYDEMANN</p>"},{"location":"legalmatters/#trademarks-designs-domain-names","title":"Trademarks, Designs, Domain Names:","text":"<p>You shall not reproduce or display any trademarks, designs and domain names used on this website and owned by Orange SA and its subsidiaries, in whole or part of, in any medium to any other purposes without the prior written consent of Orange SA and its subsidiaries. Any reproduction or displaying shall constitute an infringement under intellectual property laws of France and international conventions.</p>"},{"location":"legalmatters/#disclaimer","title":"Disclaimer:","text":"<p>Orange shall not be held liable for any hyperlinks provide through this website to other websites or any existing internet resources.</p> <p>The source codes made available through this website to the internet users are governed by free and/or open source licenses. These source codes will remain available as long as the product in which they are included is distributed by Orange Group, and then for the next three (3) years starting from the launch of a new product version or from the expiration of the distribution of the said product. Prior to download and/or use one of these source codes, the internet user shall acknowledge and accept the terms and conditions of the associated license. Furthermore, the internet user acknowledges that these source codes are provided as is without any implied warranty in accordance with the provisions of the applicable license.</p>"},{"location":"references/","title":"Bibliography","text":"<p>More than a hundred articles about Khiops are available on this page. </p> <p>To go further, here's a selection of scientific papers organized according to a reading path which facilitates the understanding of the Auto-ML pipeline. It is highly recommended to read these papers in the suggested order, after reading the documentation presented on this website. The gray lines indicate additional information, which can be read at a later stage, and which will not prevent you from gaining an overall understanding of the pipeline. </p>"},{"location":"references/#optimal-encoding","title":"Optimal Encoding","text":"<ol> <li>Discretization models: MODL: a Bayes optimal discretization method for continuous attributes - download</li> <li>Grouping models: A Bayes optimal approach for partitioning the values of categorical attributes - download</li> <li>The regression case: A New Probabilistic Approach In Rank Regression with Optimal Bayesian Partitioning - download </li> </ol>"},{"location":"references/#auto-feature-engineering","title":"Auto Feature Engineering","text":"<ol> <li>Multi-table data: A scalable robust and automatic propositionalization approach for Bayesian classification of large mixed numerical and categorical data - download</li> <li>Decision trees: A Bayes Evaluation Criterion for Decision Trees - download</li> <li>Pair discretization: Optimum simultaneous discretization with data grid models in supervised classification: a Bayesian model selection approach - download </li> </ol>"},{"location":"references/#parsimonious-training","title":"Parsimonious Training","text":"<ol> <li>Fractional Naive Bayes (FNB): Non-convex optimization for a parsimonious weighted selective naive Bayes classifier - download</li> <li>Previous versions (Khiops &lt;v10): Compression-Based Averaging of Selective Naive Bayes Classifiers - download</li> </ol>"},{"location":"api-docs/python-api/","title":"Python API Docs","text":"<p>The documentation for the Khiops Python API is available here.</p>"},{"location":"api-docs/kdic/categorical-comparisons/","title":"Categorical Comparisons","text":"<p>These rules return boolean values encoded as 0 or 1 numerical values.</p>"},{"location":"api-docs/kdic/categorical-comparisons/#eqc","title":"EQc","text":"<pre><code>Numerical EQc(Categorical value1, Categorical value2)\n</code></pre> <p>Equality test between two categorical values.</p>"},{"location":"api-docs/kdic/categorical-comparisons/#neqc","title":"NEQc","text":"<pre><code>Numerical NEQc(Categorical value1, Categorical value2)\n</code></pre> <p>Inequality test between two categorical values.</p>"},{"location":"api-docs/kdic/categorical-comparisons/#gc","title":"Gc","text":"<pre><code>Numerical Gc(Categorical value1, Categorical value2)\n</code></pre> <p>Greater than test between two categorical values.</p>"},{"location":"api-docs/kdic/categorical-comparisons/#gec","title":"GEc","text":"<pre><code>Numerical GEc(Categorical value1, Categorical value2)\n</code></pre> <p>Greater than or equal test between two categorical values.</p>"},{"location":"api-docs/kdic/categorical-comparisons/#lc","title":"Lc","text":"<pre><code>Numerical Lc(Categorical value1, Categorical value2)\n</code></pre> <p>Less than test between two categorical values.</p>"},{"location":"api-docs/kdic/categorical-comparisons/#lec","title":"LEc","text":"<pre><code>Numerical LEc(Categorical value1, Categorical value2)\n</code></pre> <p>Less than or equal test between two categorical values.</p>"},{"location":"api-docs/kdic/coclustering-rules/","title":"Coclustering Rules","text":""},{"location":"api-docs/kdic/coclustering-rules/#coclustering-rules","title":"Coclustering Rules","text":"<p>A coclustering model is stored using a <code>DataGrid</code> structure. With a <code>DataGridDeployment</code> structure, it can be deployed on a given dimension variable, for a distribution conditional on the values on the rest coclustering variables. Other deployment rules can then be used to obtain the closest part for the deployed variable, the distance to all the other parts on the deployed variable, and to compute the aggregated frequencies on the parts of all the input variables.</p>"},{"location":"api-docs/kdic/coclustering-rules/#datagriddeployment","title":"DataGridDeployment","text":"<pre><code>Structure(DataGridDeployment) DataGridDeployment(\n  Structure(DataGrid) dataGrid, Numerical deployedVariableIndex,\n  Vector inputValues1, Vector inputValues2, ..., [Vector inputFrequencies]\n)\n</code></pre> <p>Builds a <code>DataGridDeployment</code> structure for a given deployment variable of a data grid. The input values (<code>Vector</code> for <code>Numerical</code> values and <code>VectorC</code> for <code>Categorical</code> values) correspond to the distribution of the input values. The frequency vector is optional.</p>"},{"location":"api-docs/kdic/coclustering-rules/#predictedpartindex","title":"PredictedPartIndex","text":"<pre><code>Numerical PredictedPartIndex(Structure(DataGridDeployment) DataGridDeployment)\n</code></pre> <p>Computes the index of the closest part of the deployed variable.</p>"},{"location":"api-docs/kdic/coclustering-rules/#predictedpartdistances","title":"PredictedPartDistances","text":"<pre><code>Structure(Vector) PredictedPartDistances(Structure(DataGridDeployment) DataGridDeployment)\n</code></pre> <p>Computes the distance to all the parts of the deployed variable.</p>"},{"location":"api-docs/kdic/coclustering-rules/#predictedpartfrequenciesat","title":"PredictedPartFrequenciesAt","text":"<pre><code>Structure(Vector) PredictedPartFrequenciesAt(\n  Structure(DataGridDeployment) DataGridDeployment, Numerical inputVariableIndex\n)\n</code></pre>"},{"location":"api-docs/kdic/data-copy-and-conversion/","title":"Data Copy and Conversion","text":""},{"location":"api-docs/kdic/data-copy-and-conversion/#copy","title":"Copy","text":"<pre><code>Numerical Copy(Numerical value)\n</code></pre> <p>Copy of a numerical value. Allows to rename a variable.</p>"},{"location":"api-docs/kdic/data-copy-and-conversion/#copyc","title":"CopyC","text":"<pre><code>Categorical CopyC(Categorical value)\n</code></pre> <p>Copy of a categorical value.</p>"},{"location":"api-docs/kdic/data-copy-and-conversion/#copyd","title":"CopyD","text":"<pre><code>Date CopyD(Date value)\n</code></pre> <p>Copy of a date value.</p>"},{"location":"api-docs/kdic/data-copy-and-conversion/#copyt","title":"CopyT","text":"<pre><code>Time CopyT(Time value)\n</code></pre> <p>Copy of a time value.</p>"},{"location":"api-docs/kdic/data-copy-and-conversion/#copyts","title":"CopyTS","text":"<pre><code>Timestamp CopyTS(Timestamp value)\n</code></pre> <p>Copy of a timestamp value.</p>"},{"location":"api-docs/kdic/data-copy-and-conversion/#copytstz","title":"CopyTSTZ","text":"<pre><code>TimestampTZ CopyTSTZ(Timestamp value)\n</code></pre> <p>Copy of a timestampTZ value.</p>"},{"location":"api-docs/kdic/data-copy-and-conversion/#asnumerical","title":"AsNumerical","text":"<pre><code>Numerical AsNumerical(Categorical value)\n</code></pre> <p>Conversion of a categorical value to a numerical value. If the value to be converted is a numerical value, the rule returns the converted value. If the input value is missing or is not a numerical value, the method returns the system missing value.</p>"},{"location":"api-docs/kdic/data-copy-and-conversion/#asnumericalerror","title":"AsNumericalError","text":"<pre><code>Categorical AsNumericalError(Categorical value)\n</code></pre> <p>Label of a conversion error when converting a categorical value to a numerical value. This rule allows to analyse the missing or erroneous values of a numerical variable. This can be done by using a categorical type for the numerical variable to analyse, then by creating a derived variable with the current derivation rules. Thus, statistics on missing or erroneous values can easily be collected.</p> <p>List of conversion errors:</p> <ul> <li>Unconverted end of string</li> <li>Underflow</li> <li>Overflow <code>-inf</code></li> <li>Overflow <code>+inf</code></li> <li>Conversion <code>OK</code></li> </ul>"},{"location":"api-docs/kdic/data-copy-and-conversion/#recodemissing","title":"RecodeMissing","text":"<pre><code>Numerical RecodeMissing(Numerical inputValue, Numerical replaceValue)\n</code></pre> <p>Returns the input value if it is different from the missing value and the replace value otherwise.</p>"},{"location":"api-docs/kdic/data-copy-and-conversion/#ascategorical","title":"AsCategorical","text":"<pre><code>Categorical AsCategorical(Numerical value)\n</code></pre> <p>Conversion of a numerical value to a categorical value. This allows to process the input numerical values as unordered categorical values, and thus to analyse the variable using a value grouping method rather than a discretization method.</p>"},{"location":"api-docs/kdic/data-preparation-rules/","title":"Data Preparation Rules","text":""},{"location":"api-docs/kdic/data-preparation-rules/#datagrid","title":"DataGrid","text":"<pre><code>Structure(DataGrid) DataGrid(Structure(Partition) partition1, ..., Structure(Frequencies))\n</code></pre> <p>Builds a <code>DataGrid</code> structure.</p> <p>The first parameters are partitions: the results of the rules <code>IntervalBounds</code>, <code>ValueGroups</code>, <code>ValueSetC</code> or <code>ValueSet</code>. The last argument are the data grid cell frequencies.</p> <p>Example</p> <p>The following contingency table (a two-dimensional data grid)</p> Interval Iris-setosa Iris-versicolor Iris-virginica <code>]-inf;0.75]</code> 38 0 0 <code>]0.75;1.55]</code> 0 33 0 <code>]1.55;+inf[</code> 0 3 34 <p>is encoded in a dictionary using a constructed variable:</p> <pre><code>Dictionary Example\n{\n  Structure(DataGrid) PPetalWidth = DataGrid(\n    IntervalBounds(0.75, 1.55),\n    ValueSetC(\"Iris-setosa\", \"Iris-versicolor\", \"Iris-virginica\"),\n    Frequencies(38, 0, 0, 0, 33, 3, 0, 0, 34)\n  );\n};\n</code></pre> <p>The cells are indexed column-first. In this example, contingency table indexation is as follows:</p> Interval Iris-setosa Iris-versicolor Iris-virginica <code>]-inf;0.75]</code> 1 4 7 <code>]0.75;1.55]</code> 2 5 8 <code>]1.55;+inf]</code> 3 6 9 <p>Several derivation rules use a data grid structure to retrieve a cell given a vector of input values. For example, the vector of values <code>(1.1, \"Iris-versicolor\")</code> falls into the cell of index 5.</p> <p>The value vector can be partially complete. In this case, the other dimension has default values corresponding to the first part of their dimension. For example, the single value vector <code>(1.1)</code> falls into the cell of index 2.</p> <p>The value types can be either <code>Numerical</code> or <code>Categorical</code> according to the data grid input partition types. In the following, this will be indicated using <code>SimpleType</code> for such parameter types.</p>"},{"location":"api-docs/kdic/data-preparation-rules/#intervalbounds","title":"IntervalBounds","text":"<pre><code>Structure(IntervalBounds) IntervalBounds(Numerical bound1, ...)\n</code></pre> <p>Builds a partition into interval.</p>"},{"location":"api-docs/kdic/data-preparation-rules/#valuegroup","title":"ValueGroup","text":"<pre><code>Structure(ValueGroup) ValueGroup(Categorical value1, ...)\n</code></pre> <p>Builds a group of values. The special value <code>*</code> cannot correspond to a value extracted from datasets (these ones are trimmed before being processed). This special value can be used as a \"garbage value\" to match any value that are not explicitly defined elsewhere.</p>"},{"location":"api-docs/kdic/data-preparation-rules/#valuegroups","title":"ValueGroups","text":"<pre><code>Structure(ValueGroups) ValueGroups(Structure(ValueGroup) valueGroup1, ...)\n</code></pre> <p>Builds a partition into groups of values. The special value <code>*</code> must be defined in exactly one value group, which is assigned to unknown values.</p>"},{"location":"api-docs/kdic/data-preparation-rules/#valuesetc","title":"ValueSetC","text":"<pre><code>Structure(ValueSetC) ValueSetC(Categorical value1, ...)\n</code></pre> <p>Builds a partition into categorical values.</p>"},{"location":"api-docs/kdic/data-preparation-rules/#valueset","title":"ValueSet","text":"<pre><code>Structure(ValueSet) ValueSet(Numerical value1, ...)\n</code></pre> <p>Builds a partition into numerical values.</p>"},{"location":"api-docs/kdic/data-preparation-rules/#frequencies","title":"Frequencies","text":"<pre><code>Structure(Frequencies) Frequencies(Numerical frequency1, ...)\n</code></pre> <p>Builds a vector of frequencies.</p>"},{"location":"api-docs/kdic/date-rules/","title":"Date Rules","text":"<p><code>Date</code> values are encoded in data table files using Khiops native format <code>YYYY-MM-DD</code>. Other formats are available, that allow to convert categorical values to date values.</p> <code>YYYY-MM-DD</code> <code>YYYY/MM/DD</code> <code>YYYY.MM.DD</code> <code>YYYYMMDD</code> <code>DD-MM-YYYY</code> <code>DD/MM/YYYY</code> <code>DD.MM.YYYY</code> <code>DDMMYYYY</code> <code>YYYY-DD-MM</code> <code>YYYY/DD/MM</code> <code>YYYY.DD.MM</code> <code>YYYYDDMM</code> <code>MM-DD-YYYY</code> <code>MM/DD/YYYY</code> <code>MM.DD.YYYY</code> <code>MMDDYYYY</code> <p>Valid dates range from <code>0001-01-01</code> to <code>4000-12-31</code>, with validity according to Gregorian calendar. <code>Date</code> rules return a missing value when their date operand is not valid or when a numerical operand is missing.</p>"},{"location":"api-docs/kdic/date-rules/#formatdate","title":"FormatDate","text":"<pre><code>Categorical FormatDate(Date value, Categorical dateFormat)\n</code></pre> <p>Format a date into a categorical value using a date format. Date format is a categorical constant value among the available date formats (for example: <code>YYYY-MM-DD</code>).</p>"},{"location":"api-docs/kdic/date-rules/#asdate","title":"AsDate","text":"<pre><code>Date AsDate(Categorical dateString, Categorical dateFormat)\n</code></pre> <p>Recode a categorical value into a date using a date format.</p> <p>Example</p> <pre><code>AsDate(\u201c2014-01-15\u201d, \u201cYYYY-MM-DD\u201d).\n</code></pre>"},{"location":"api-docs/kdic/date-rules/#year","title":"Year","text":"<pre><code>Numerical Year(Date value)\n</code></pre> <p>Year in a date value.</p>"},{"location":"api-docs/kdic/date-rules/#month","title":"Month","text":"<pre><code>Numerical Month(Date value)\n</code></pre> <p>Month in a date value.</p>"},{"location":"api-docs/kdic/date-rules/#day","title":"Day","text":"<pre><code>Numerical Day(Date value)\n</code></pre> <p>Day in a date value.</p>"},{"location":"api-docs/kdic/date-rules/#yearday","title":"YearDay","text":"<pre><code>Numerical YearDay(Date value)\n</code></pre> <p>Day in year in a date value.</p>"},{"location":"api-docs/kdic/date-rules/#weekday","title":"WeekDay","text":"<pre><code>Numerical WeekDay(Date value)\n</code></pre> <p>Day in week in a date value.</p> <p>Returns 1 for Monday, 2 for Tuesday ..., 7 for Sunday.</p>"},{"location":"api-docs/kdic/date-rules/#decimalyear","title":"DecimalYear","text":"<pre><code>Numerical DecimalYear(Date value)\n</code></pre> <p>Year in a date value, with decimal part for day in year.</p>"},{"location":"api-docs/kdic/date-rules/#absoluteday","title":"AbsoluteDay","text":"<pre><code>Numerical AbsoluteDay(Date value)\n</code></pre> <p>Total elapsed days since <code>2000-01-01</code>.</p>"},{"location":"api-docs/kdic/date-rules/#diffdate","title":"DiffDate","text":"<pre><code>Numerical DiffDate(Date value1, Date value2)\n</code></pre> <p>Difference in days between two date values.</p>"},{"location":"api-docs/kdic/date-rules/#adddays","title":"AddDays","text":"<pre><code>Date AddDays(Date value, Numerical dayNumber)\n</code></pre> <p>Adds a number of days to a date value.</p>"},{"location":"api-docs/kdic/date-rules/#isdatevalid","title":"IsDateValid","text":"<pre><code>Numerical IsDateValid(Date value)\n</code></pre> <p>Checks if a date value is valid.</p>"},{"location":"api-docs/kdic/date-rules/#builddate","title":"BuildDate","text":"<pre><code>Date BuildDate(Numerical year, Numerical month, Numerical day)\n</code></pre> <p>Builds a date value from year, month and day.</p> <p>The year must be between 1 and 9999. The month must be between 1 and 12. The day must be between 1 and 31, and consistent with the month and year.</p>"},{"location":"api-docs/kdic/entity-rules/","title":"Entity Rules","text":"<p>If the entity does not exists, the rules below return:</p> <ul> <li>An empty value (<code>\"\"</code>) for rules that return <code>Categorical</code></li> <li>A missing value for rules that return <code>Numerical</code></li> </ul>"},{"location":"api-docs/kdic/entity-rules/#exist","title":"Exist","text":"<pre><code>Numerical Exist(Entity entityValue)\n</code></pre> <p>Checks is an entity exists. Returns 0 or 1.</p> <p>Example</p> Diff from Base ExampleFull Example <pre><code>// Value is 1 if the address exists for the customer\nNumerical ExistingAddress = Exist(customerAddress);\n</code></pre> <pre><code>Root Dictionary Customer(customer_id)\n{\n  Categorical customer_id;\n  Numerical age;\n  Categorical sex;\n  Entity(Address) customerAddress; // 1-1 relationship\n  Table(Sale) sales;               // 1-n relationship\n  // Value is 1 if the address exists for the customer\n  Numerical ExistingAddress = Exist(customerAddress);\n};\n\nDictionary Address(customer_id)\n{\n  Categorical customer_id;\n  Categorical street;\n  Categorical city;\n  Categorical zipcode;\n  Categorical State;\n};\n\nDictionary Sale(customer_id)\n{\n  Categorical customer_id;\n  Categorical product;\n  Numerical cost;\n  Date purchaseDate;\n};\n</code></pre>"},{"location":"api-docs/kdic/entity-rules/#getvalue","title":"GetValue","text":"<pre><code>Numerical GetValue(Entity entityValue, Numerical value)\n</code></pre> <p>Access to a <code>Numerical</code> value of an entity. Returns a missing value if the entity does not exist.</p> <p>Example</p> Diff from Base ExampleFull Example <pre><code>// Street name length\nNumerical streetNameLength = GetValue(customerAddress, Length(street));\n</code></pre> <pre><code>Root Dictionary Customer(customer_id)\n{\n  Categorical customer_id;\n  Numerical age;\n  Categorical sex;\n  Entity(Address) customerAddress; // 1-1 relationship\n  Table(Sale) sales;               // 1-n relationship\n  // Street name length\n  Numerical streetNameLength = GetValue(customerAddress, Length(street));\n};\n\nDictionary Address(customer_id)\n{\n  Categorical customer_id;\n  Categorical street;\n  Categorical city;\n  Categorical zipcode;\n  Categorical State;\n};\n\nDictionary Sale(customer_id)\n{\n  Categorical customer_id;\n  Categorical product;\n  Numerical cost;\n  Date purchaseDate;\n};\n</code></pre>"},{"location":"api-docs/kdic/entity-rules/#getvaluec","title":"GetValueC","text":"<pre><code>Categorical GetValueC(Entity entityValue, Categorical value)\n</code></pre> <p>Access to a <code>Categorical</code> value of an entity. Returns an empty <code>Categorical</code> value if the entity does not exist.</p> <p>Example</p> Diff from Base ExampleFull Example <pre><code>Categorical city = GetValueC(customerAddress, city); // City from address\n</code></pre> <pre><code>Root Dictionary Customer(customer_id)\n{\n  Categorical customer_id;\n  Numerical age;\n  Categorical sex;\n  Entity(Address) customerAddress; // 1-1 relationship\n  Table(Sale) sales;               // 1-n relationship\n  Categorical city = GetValueC(customerAddress, city); // City from address\n};\n\nDictionary Address(customer_id)\n{\n  Categorical customer_id;\n  Categorical street;\n  Categorical city;\n  Categorical zipcode;\n  Categorical State;\n};\n\nDictionary Sale(customer_id)\n{\n  Categorical customer_id;\n  Categorical product;\n  Numerical cost;\n  Date purchaseDate;\n};\n</code></pre>"},{"location":"api-docs/kdic/entity-rules/#getvalued","title":"GetValueD","text":"<pre><code>Date GetValueD(Entity entityValue, Date value)\n</code></pre> <p>Access to a <code>Date</code> value of an entity. Returns empty <code>Date</code> value if the entity does not exist.</p>"},{"location":"api-docs/kdic/entity-rules/#getvaluet","title":"GetValueT","text":"<pre><code>Time GetValueT(Entity entityValue, Time value)\n</code></pre> <p>Access to a <code>Time</code> value of an entity. Returns an empty <code>Time</code> value if the entity does not exist.</p>"},{"location":"api-docs/kdic/entity-rules/#getvaluets","title":"GetValueTS","text":"<pre><code>Timestamp GetValueTS(Entity entityValue, Timestamp value)\n</code></pre> <p>Access to a <code>Timestamp</code> value of an entity. Returns an empty <code>Timestamp</code> value if the entity does not exist.</p>"},{"location":"api-docs/kdic/entity-rules/#getvaluetstz","title":"GetValueTSTZ","text":"<pre><code>TimestampTZ GetValueTSTZ(Entity entityValue, TimestampTZ value)\n</code></pre> <p>Access to a <code>TimestampTZ</code> value of an entity. Returns an empty <code>TimestampTZ</code> value if the entity does not exist.</p>"},{"location":"api-docs/kdic/entity-rules/#getentity","title":"GetEntity","text":"<pre><code>Entity GetEntity(Entity entityValue, Entity value)\n</code></pre> <p>Access to an <code>Entity</code> value of an entity.</p>"},{"location":"api-docs/kdic/entity-rules/#gettable","title":"GetTable","text":"<pre><code>Table GetTable(Entity entityValue, Table value)\n</code></pre> <p>Access to a <code>Table</code> value of an entity.</p>"},{"location":"api-docs/kdic/hash-map-rules/","title":"Hash Map Rules","text":""},{"location":"api-docs/kdic/hash-map-rules/#structure","title":"Structure","text":"<pre><code>Structure(HashMapC) HashMapC(Structure(VectorC) keyVector, Structure(VectorC) valueVector)\n</code></pre> <p>Builds a hash map of <code>Categorical</code> values indexed by <code>Categorical</code> keys. The operands must come from <code>VectorC</code> rules of the same size, with unique keys in the <code>keyVector</code>.</p>"},{"location":"api-docs/kdic/hash-map-rules/#tablehashmapc","title":"TableHashMapC","text":"<pre><code>Structure(HashMapC) TableHashMapC(Table table, Categorical key, Categorical value)\n</code></pre> <p>Builds a hash map of <code>Categorical</code> values from keys and values in a table. In case of duplicate keys in the table, only the first matching value is kept.</p>"},{"location":"api-docs/kdic/hash-map-rules/#valueatkeyc","title":"ValueAtKeyC","text":"<pre><code>Categorical ValueAtKeyC(Structure(HashMapC) hashMap, Categorical key)\n</code></pre> <p>Returns a value of a categorical hash map at given key. Returns <code>\"\"</code> if not found. It allows to efficiently recode a <code>Categorical</code> value into another <code>Categorical</code> value.</p> <p>Example</p> <pre><code>Dictionary Person\n{\n  Categorical Name;\n  Categorical Sex;\n  // Maps \"male\" -&gt; \"Mr\" and \"female\" -&gt; \"Mrs\"\n  Categorical Gender =\n    ValueAtKeyC(HashMapC(VectorC(\"male\", \"female\"), VectorC(\"Mr\", \"Mrs\")), Sex);\n}\n</code></pre>"},{"location":"api-docs/kdic/hash-map-rules/#hashmap","title":"HashMap","text":"<pre><code>Structure(HashMap) HashMap(Structure(VectorC) keyVector, Structure(Vector) valueVector)\n</code></pre> <p>Builds a hash map of numerical values indexed by keys. The operands must be the result of <code>VectorC</code> and <code>Vector</code> rules of the same size, with unique keys in the vector of keys.</p>"},{"location":"api-docs/kdic/hash-map-rules/#tablehashmap","title":"TableHashMap","text":"<pre><code>Structure(HashMap) TableHashMap(Table table, Categorical key, Numerical value)\n</code></pre> <p>Builds a hash map of <code>Numerical</code> values from keys and values in a table. In case of duplicate keys in the table, only the first matching value is kept.</p>"},{"location":"api-docs/kdic/hash-map-rules/#valueatkey","title":"ValueAtKey","text":"<pre><code>Numerical ValueAtKey(Structure(HashMap) hashMap, Categorical key)\n</code></pre> <p>Returns a value of a numerical hash map at given key. Returns missing value is not found. It allows to efficiently recode a <code>Categorical</code> value into a <code>Numerical</code> one.</p> <p>Example</p> <pre><code>Dictionary Person\n{\n  Categorical Name;\n  Categorical Sex;\n  // Maps \"male\" -&gt; 0 and \"female\" -&gt; 1\n  Numerical NumericGender = ValueAtKeyC(HashMapC(VectorC(\"male\", \"female\"), VectorC(0, 1)), Sex);\n}\n</code></pre>"},{"location":"api-docs/kdic/logical-operators/","title":"Logical Operators","text":"<p>These rules use boolean operands (numerical values with 0 for false and not 0 for true) and return boolean values encoded as 0 or 1 numerical values.</p>"},{"location":"api-docs/kdic/logical-operators/#and","title":"And","text":"<pre><code>Numerical And(Numerical boolean1, ...)\n</code></pre> <p>And logical operator.</p>"},{"location":"api-docs/kdic/logical-operators/#or","title":"Or","text":"<pre><code>Numerical Or(Numerical boolean1, ...)\n</code></pre> <p>Or logical operator.</p>"},{"location":"api-docs/kdic/logical-operators/#not","title":"Not","text":"<pre><code>Numerical Not(Numerical boolean)\n</code></pre> <p>Not logical operator.</p>"},{"location":"api-docs/kdic/logical-operators/#if","title":"If","text":"<pre><code>Numerical If(Numerical test, Numerical valueTrue, Numerical valueFalse)\n</code></pre> <p>Ternary operator returning second operand (true) or third operand (false) according to the condition in first operand.</p>"},{"location":"api-docs/kdic/logical-operators/#ifc","title":"IfC","text":"<pre><code>Categorical IfC(Numerical test, Categorical valueTrue, Categorical valueFalse)\n</code></pre> <p>Ternary operator returning second operand (true) or third operand (false) according to the condition in first operand.</p>"},{"location":"api-docs/kdic/logical-operators/#ifd","title":"IfD","text":"<pre><code>Date IfD(Numerical test, Date valueTrue, Date valueFalse)\n</code></pre> <p>Ternary operator returning second operand (true) or third operand (false) according to the condition in first operand.</p>"},{"location":"api-docs/kdic/logical-operators/#ift","title":"IfT","text":"<pre><code>Time IfT(Numerical test, Time valueTrue, Time valueFalse)\n</code></pre> <p>Ternary operator returning second operand (true) or third operand (false) according to the condition in first operand.</p>"},{"location":"api-docs/kdic/logical-operators/#ifts","title":"IfTS","text":"<pre><code>Timestamp IfTS(Numerical test, Timestamp valueTrue, Timestamp valueFalse)\n</code></pre> <p>Ternary operator returning second operand (true) or third operand (false) according to the condition in first operand.</p>"},{"location":"api-docs/kdic/logical-operators/#iftstz","title":"IfTSTZ","text":"<pre><code>TimestampTZ IfTSTZ(Numerical test, TimestampTZ valueTrue, TimestampTZ valueFalse)\n</code></pre> <p>Ternary operator returning second operand (true) or third operand (false) according to the condition in first operand.</p>"},{"location":"api-docs/kdic/logical-operators/#switch","title":"Switch","text":"<pre><code>Numerical Switch(Numerical test, Numerical valueDefault, Numerical value1, ..., Numerical valueK)\n</code></pre> <p>Switch operator that returns the numerical value corresponding to the index given by the test operand if it is between 1 and K. The default value is returned if the index if outside the bounds.</p>"},{"location":"api-docs/kdic/logical-operators/#switchc","title":"SwitchC","text":"<pre><code>Categorical SwitchC(Numerical test, Categorical valueDefault, Categorical value1, ..., Categorical valueK)\n</code></pre> <p>Switch operator that returns the categorical value corresponding to the index given by the test operand if it is between 1 and K. The default value is returned if the index if outside the bounds.</p>"},{"location":"api-docs/kdic/math-rules/","title":"Math Rules","text":"<p>Most of these rules return a numerical value from operators having one or many numerical operands. When one operand is missing or invalid, the result is missing.</p>"},{"location":"api-docs/kdic/math-rules/#formatnumerical","title":"FormatNumerical","text":"<pre><code>Categorical FormatNumerical(Numerical value, Numerical width, Numerical precision)\n</code></pre> <p>Returns a string formatted version of a numerical value, with given minimum digit number before separator and given precision after separator.</p> <p>Example</p> <pre><code>FormatNumerical(3.141592, 0, 8) // 3.14159200\nFormatNumerical(3.141592, 2, 5) // 03.14159\nFormatNumerical(3.141592, 0, 0) // 3\n</code></pre>"},{"location":"api-docs/kdic/math-rules/#sum","title":"Sum","text":"<pre><code>Numerical Sum(Numerical value1, ...)\n</code></pre> <p>Sum of numerical values.</p>"},{"location":"api-docs/kdic/math-rules/#minus","title":"Minus","text":"<pre><code>Numerical Minus(Numerical value)\n</code></pre> <p>Opposite value.</p>"},{"location":"api-docs/kdic/math-rules/#diff","title":"Diff","text":"<pre><code>Numerical Diff(Numerical value1, Numerical value2)\n</code></pre> <p>Difference between two numerical values.</p>"},{"location":"api-docs/kdic/math-rules/#product","title":"Product","text":"<pre><code>Numerical Product(Numerical value1, ...)\n</code></pre> <p>Product of numerical values.</p>"},{"location":"api-docs/kdic/math-rules/#divide","title":"Divide","text":"<pre><code>Numerical Divide(Numerical value1, Numerical value2)\n</code></pre> <p>Ratio of two numerical values.</p>"},{"location":"api-docs/kdic/math-rules/#index","title":"Index","text":"<pre><code>Numerical Index()\n</code></pre> <p>Integer index related to the line number of the current record from a data file (start at 1).</p>"},{"location":"api-docs/kdic/math-rules/#random","title":"Random","text":"<pre><code>Numerical Random()\n</code></pre> <p>Random number between 0 and 1.</p> <p>Each time a database is read, the random seed is forced to the same value, such that the sequence of random numbers will be the same.</p> <p>Note that the Random rule can be used several times in dictionaries, resulting in independent sequences of random numbers.</p>"},{"location":"api-docs/kdic/math-rules/#round","title":"Round","text":"<pre><code>Numerical Round(Numerical value)\n</code></pre> <p>Closest integer value.</p>"},{"location":"api-docs/kdic/math-rules/#floor","title":"Floor","text":"<pre><code>Numerical Floor(Numerical value)\n</code></pre> <p>Largest previous integer value.</p>"},{"location":"api-docs/kdic/math-rules/#ceil","title":"Ceil","text":"<pre><code>Numerical Ceil(Numerical value)\n</code></pre> <p>Smallest following integer value.</p>"},{"location":"api-docs/kdic/math-rules/#abs","title":"Abs","text":"<pre><code>Numerical Abs(Numerical value)\n</code></pre> <p>Absolute value.</p>"},{"location":"api-docs/kdic/math-rules/#sign","title":"Sign","text":"<pre><code>Numerical Sign(Numerical value)\n</code></pre> <p>Sign of a numerical value.</p> <p>Returns 1 for values greater or equal than 0, -1 for values less than 0.</p>"},{"location":"api-docs/kdic/math-rules/#mod","title":"Mod","text":"<pre><code>Numerical Mod(Numerical value1, Numerical value2)\n</code></pre> <p>Returns value1 mod value2. Precisely, <code>value1 \u2013 value2 * Floor(value1/value2)</code>.</p>"},{"location":"api-docs/kdic/math-rules/#log","title":"Log","text":"<pre><code>Numerical Log(Numerical value)\n</code></pre> <p>Natural logarithm.</p>"},{"location":"api-docs/kdic/math-rules/#exp","title":"Exp","text":"<pre><code>Numerical Exp(Numerical value)\n</code></pre> <p>Exponential value.</p>"},{"location":"api-docs/kdic/math-rules/#power","title":"Power","text":"<pre><code>Numerical Power(Numerical value1, Numerical value2)\n</code></pre> <p>Power function.</p>"},{"location":"api-docs/kdic/math-rules/#sqrt","title":"Sqrt","text":"<pre><code>Numerical Sqrt(Numerical value)\n</code></pre> <p>Square root function.</p>"},{"location":"api-docs/kdic/math-rules/#sin","title":"Sin","text":"<pre><code>Numerical Sin(Numerical value)\n</code></pre> <p>Sine function.</p>"},{"location":"api-docs/kdic/math-rules/#cos","title":"Cos","text":"<pre><code>Numerical Cos(Numerical value)\n</code></pre> <p>Cosine function.</p>"},{"location":"api-docs/kdic/math-rules/#tan","title":"Tan","text":"<pre><code>Numerical Tan(Numerical value)\n</code></pre> <p>Tangent function.</p>"},{"location":"api-docs/kdic/math-rules/#asin","title":"ASin","text":"<pre><code>Numerical ASin(Numerical value)\n</code></pre> <p>Arc-sine function.</p>"},{"location":"api-docs/kdic/math-rules/#acos","title":"ACos","text":"<pre><code>Numerical ACos(Numerical value)\n</code></pre> <p>Arc-cosine function.</p>"},{"location":"api-docs/kdic/math-rules/#atan","title":"ATan","text":"<pre><code>Numerical ATan(Numerical value)\n</code></pre> <p>Arc-tangent function.</p>"},{"location":"api-docs/kdic/math-rules/#pi","title":"Pi","text":"<pre><code>Numerical Pi()\n</code></pre> <p>Pi constant.</p>"},{"location":"api-docs/kdic/math-rules/#mean","title":"Mean","text":"<pre><code>Numerical Mean(Numerical value1, ...)\n</code></pre> <p>Mean of numerical values.</p>"},{"location":"api-docs/kdic/math-rules/#stddev","title":"StdDev","text":"<pre><code>Numerical StdDev(Numerical value1, ...)\n</code></pre> <p>Standard deviation of numerical values.</p>"},{"location":"api-docs/kdic/math-rules/#min","title":"Min","text":"<pre><code>Numerical Min(Numerical value1, ...)\n</code></pre> <p>Min of numerical values (among non-missing values).</p>"},{"location":"api-docs/kdic/math-rules/#max","title":"Max","text":"<pre><code>Numerical Max(Numerical value1, ...)\n</code></pre> <p>Max of numerical values (among non-missing values).</p>"},{"location":"api-docs/kdic/math-rules/#argmin","title":"ArgMin","text":"<pre><code>Numerical ArgMin(Numerical value1, ...)\n</code></pre> <p>Index of the min value in a numerical series. The index starts at 1 and relates to the first value that is equal to the min.</p>"},{"location":"api-docs/kdic/math-rules/#argmax","title":"ArgMax","text":"<pre><code>Numerical ArgMax(Numerical value1, ...)\n</code></pre> <p>Index of the max value in a numerical series. The index starts at 1 and relates to the first value that is equal to the max.</p>"},{"location":"api-docs/kdic/multi-table-rules-introduction/","title":"Multi-Table Rules Overview","text":"<p>The multi-table rules allow to extract values from entities in 0-1 relationship (<code>Entity</code>) or 0-n relationship (<code>Table</code>) with their owner.</p> <p>Below you find dictionaries describing a multi-table schema The main object is a <code>Customer</code> which can have at most one <code>Address</code> and several <code>Sales</code> entries. This example is used to illustrate the multi-table rules in this section, with variables added in the root dictionary to extract information from the address and sales.</p> <pre><code>Root Dictionary Customer (customer_id)\n{\n  Categorical customer_id;\n  Numerical age;\n  Categorical sex;\n  Entity(Address) customerAddress; // 0-1 relationship\n  Table(Sale) sales;               // 0-n relationship\n};\n\nDictionary Address (customer_id)\n{\n  Categorical customer_id;\n  Categorical street;\n  Categorical city;\n  Categorical zipcode;\n  Categorical State;\n};\n\nDictionary Sale (customer_id)\n{\n  Categorical customer_id;\n  Categorical product;\n  Numerical cost;\n  Date purchaseDate;\n};\n</code></pre>"},{"location":"api-docs/kdic/numerical-comparisons/","title":"Numerical Comparisons","text":"<p>These rules return boolean values encoded as 0 or 1 numerical values. Note that the missing value is treated as a value that is inferior to any valid value in comparisons.</p>"},{"location":"api-docs/kdic/numerical-comparisons/#eq","title":"EQ","text":"<pre><code>Numerical EQ(Numerical value1, Numerical value2)\n</code></pre> <p>Equality test between two numerical values.</p>"},{"location":"api-docs/kdic/numerical-comparisons/#neq","title":"NEQ","text":"<pre><code>Numerical NEQ(Numerical value1, Numerical value2)\n</code></pre> <p>Inequality test between two numerical values.</p>"},{"location":"api-docs/kdic/numerical-comparisons/#g","title":"G","text":"<pre><code>Numerical G(Numerical value1, Numerical value2)\n</code></pre> <p>Greater than test between two numerical values.</p>"},{"location":"api-docs/kdic/numerical-comparisons/#ge","title":"GE","text":"<pre><code>Numerical GE(Numerical value1, Numerical value2)\n</code></pre> <p>Greater than or equal test between two numerical values.</p>"},{"location":"api-docs/kdic/numerical-comparisons/#l","title":"L","text":"<pre><code>Numerical L(Numerical value1, Numerical value2)\n</code></pre> <p>Less than test between two numerical values.</p>"},{"location":"api-docs/kdic/numerical-comparisons/#le","title":"LE","text":"<pre><code>Numerical LE(Numerical value1, Numerical value2)\n</code></pre> <p>Less than or equal test between two numerical values.</p>"},{"location":"api-docs/kdic/predictor-rules/","title":"Predictor Rules","text":""},{"location":"api-docs/kdic/predictor-rules/#nbclassifier","title":"NBClassifier","text":"<pre><code>Structure(Classifier) NBClassifier(Structure(DataGridStats) dataGridStats1, ...)\n</code></pre> <p>Builds a Naive Bayes <code>Classifier</code> structure from a set of data grids stats that encode the target conditional probabilities. Each data grid stats results from a preparation model (data grid) and input values, which allows the computation of conditional probabilities.</p>"},{"location":"api-docs/kdic/predictor-rules/#snbclassifier","title":"SNBClassifier","text":"<p><pre><code>Structure(Classifier) SNBClassifier(\n  Structure(Vector) variableWeights, Structure(DataGridStats) dataGridStats1, ...\n)\n</code></pre> Builds a Selective Naive Bayes <code>Classifier</code> structure. The first parameter is weight <code>Vector</code> for the selected variables. The remaining parameters are the same as for the <code>NBClassifier</code> rule.</p>"},{"location":"api-docs/kdic/predictor-rules/#targetvalue","title":"TargetValue","text":"<pre><code>Categorical TargetValue(Structure(Classifier) classifier)\n</code></pre> <p>Computes a <code>Classifier</code>'s most probable target value.</p>"},{"location":"api-docs/kdic/predictor-rules/#targetprob","title":"TargetProb","text":"<pre><code>Numerical TargetProb(Structure(Classifier) classifier)\n</code></pre> <p>Computes the <code>Classifier</code>s probability of the most probable target value.</p>"},{"location":"api-docs/kdic/predictor-rules/#targetprobat","title":"TargetProbAt","text":"<pre><code>Numerical TargetProbAt(Structure(Classifier) classifier, Categorical targetValue)\n</code></pre> <p>Computes the <code>Classifier</code> probability (score) of a given target value.</p>"},{"location":"api-docs/kdic/predictor-rules/#biasedtargetvalue","title":"BiasedTargetValue","text":"<pre><code>Categorical BiasedTargetValue(Structure(Classifier) classifier, Structure(Vector) biasValues)\n</code></pre> <p>Computes the <code>Classifier</code> highest score target value, after adding a bias to each initial target value score.</p>"},{"location":"api-docs/kdic/predictor-rules/#nbrankregressor","title":"NBRankRegressor","text":"<pre><code>Structure(RankRegressor) NBRankRegressor(Structure(DataGridStats) dataGridStats1, ...)\n</code></pre> <p>Builds a Naive Bayes <code>RankRegressor</code> structure from a set of data grids stats.</p>"},{"location":"api-docs/kdic/predictor-rules/#snbrankregressor","title":"SNBRankRegressor","text":"<pre><code>Structure(RankRegressor) SNBRankRegressor(\n  Structure(Vector) variableWeights, Structure(DataGridStats) dataGridStats1, ...\n)\n</code></pre> <p>Builds a Selective Naive Bayes <code>RankRegressor</code>. The first parameter is weight <code>Vector</code> for the selected variables. The remaining parameters are the same as for the <code>NBRankRegressor</code> rule.</p>"},{"location":"api-docs/kdic/predictor-rules/#targetrankmean","title":"TargetRankMean","text":"<pre><code>Numerical TargetRankMean(Structure(RankRegressor) rankRegressor)\n</code></pre> <p>Computes the <code>RankRegressor</code> target rank mean.</p>"},{"location":"api-docs/kdic/predictor-rules/#targetrankstandarddeviation","title":"TargetRankStandardDeviation","text":"<pre><code>Numerical TargetRankStandardDeviation(Structure(RankRegressor) rankRegressor)\n</code></pre> <p>Computes the <code>RankRegressor</code> target rank's standard deviation.</p>"},{"location":"api-docs/kdic/predictor-rules/#targetrankdensityat","title":"TargetRankDensityAt","text":"<pre><code>Numerical TargetRankDensityAt(Structure(RankRegressor) rankRegressor, Numerical rank)\n</code></pre> <p>Computes the <code>RankRegressor</code> density of the target rank for a given normalized rank (between 0 and 1).</p>"},{"location":"api-docs/kdic/predictor-rules/#targetrankcumulativeprobat","title":"TargetRankCumulativeProbAt","text":"<pre><code>Numerical TargetRankCumulativeProbAt(Structure(RankRegressor) rankRegressor, Numerical rank)\n</code></pre> <p>Computes the <code>RankRegressor</code> probability that the target rank is below a given normalized rank.</p>"},{"location":"api-docs/kdic/predictor-rules/#nbregressor","title":"NBRegressor","text":"<pre><code>Structure(Regressor) NBRegressor(\n  Structure(RankRegressor) nbRankRegressor, Structure(DataGrid) targetValues\n)\n</code></pre> <p>Builds a Naive Bayes <code>Regressor</code> structure. The first parameter is a Naive Bayes <code>RankRegressor</code>. The second parameter is the distribution of the numerical target values, encoded as a univariate numerical data grid based on a vector of values partition.</p>"},{"location":"api-docs/kdic/predictor-rules/#snbregressor","title":"SNBRegressor","text":"<pre><code>Structure(Regressor) SNBRegressor(\n  Structure(RankRegressor) snbRankRegressor, Structure(DataGrid) targetValues\n)\n</code></pre> <p>Builds a Selective Naive Bayes <code>Regressor</code> structure from a <code>RankRegressor</code>.The first parameter is a Naive Bayes <code>RankRegressor</code>. The second parameter is the distribution of the numerical target values, encoded as a univariate numerical data grid based on a vector of values partition.</p>"},{"location":"api-docs/kdic/predictor-rules/#targetmean","title":"TargetMean","text":"<pre><code>Numerical TargetMean(Structure(Regressor) regressor)\n</code></pre> <p>Computes the <code>Regressor</code> mean target value.</p>"},{"location":"api-docs/kdic/predictor-rules/#targetstandarddeviation","title":"TargetStandardDeviation","text":"<pre><code>Numerical TargetStandardDeviation(Structure(Regressor) regressor)\n</code></pre> <p>Computes the <code>Regressor</code> standard deviation of the target value.</p>"},{"location":"api-docs/kdic/predictor-rules/#targetdensityat","title":"TargetDensityAt","text":"<pre><code>Numerical TargetDensityAt(Structure(Regressor) regressor, Numerical value)\n</code></pre> <p>Computes the <code>Regressor</code> density of the target for a given value.</p>"},{"location":"api-docs/kdic/recoding-rules/","title":"Recoding Rules","text":"<p>Many recoding methods take a partition as a first parameter and a value of the same type (<code>Numerical</code> or <code>Categorical</code>) as a second one. They retrieve the part of the partition containing the value, and output an index, which may be</p> <ul> <li>an integer value ranging from 1 to S, where S is the partition size,</li> <li>or an identifier (like <code>I1</code>), which is a generic <code>Categorical</code> value,</li> <li>or a label (like <code>]-\u221e; 2]</code>), which is a comprehensible <code>Categorical</code> value.</li> </ul>"},{"location":"api-docs/kdic/recoding-rules/#ininterval","title":"InInterval","text":"<pre><code>Numerical InInterval(Structure(IntervalBounds) interval, Numerical inputValue)\n</code></pre> <p>Returns <code>1</code> if the input value belongs to the interval, <code>0</code> otherwise. The interval bounds must contain exactly two bounds, for intervals of type <code>]lowerBound; upperBound]</code>. For left-open or right-open intervals, use comparison derivation rules, such as <code>G</code>, <code>GE</code>, <code>L</code> or <code>LE</code>.</p>"},{"location":"api-docs/kdic/recoding-rules/#ingroup","title":"InGroup","text":"<pre><code>Numerical InGroup(Structure(ValueGroup) valueGroup, Categorical inputValue)\n</code></pre> <p>Returns 1 if the input value belongs to the value group, 0 otherwise.</p>"},{"location":"api-docs/kdic/recoding-rules/#cellindex","title":"CellIndex","text":"<pre><code>Numerical CellIndex(Structure(DataGrid) dataGrid, SimpleType inputValue1, ...)\n</code></pre> <p>Computes the <code>Numerical</code> cell index of a list of values given a data grid. The list of values are either numerical <code>Numerical</code> or <code>Categorical</code> according to the data grid input partition types.</p>"},{"location":"api-docs/kdic/recoding-rules/#cellid","title":"CellId","text":"<pre><code>Categorical CellId(Structure(DataGrid) dataGrid, SimpleType inputValue1, ...)\n</code></pre> <p>Computes the <code>Categorical</code> cell identifier of a list of values given a data grid.</p>"},{"location":"api-docs/kdic/recoding-rules/#celllabel","title":"CellLabel","text":"<pre><code>Categorical CellLabel(Structure(DataGrid) dataGrid, SimpleType inputValue1, ...)\n</code></pre> <p>Computes the cell label of a list of values given a data grid.</p>"},{"location":"api-docs/kdic/recoding-rules/#valueindexdg","title":"ValueIndexDG","text":"<pre><code>Numerical ValueIndexDG(Structure(DataGrid) dataGrid, SimpleType inputValue)\n</code></pre> <p>Computes the <code>Numerical</code> index of a value given a univariate data grid.</p>"},{"location":"api-docs/kdic/recoding-rules/#partindexat","title":"PartIndexAt","text":"<pre><code>Numerical PartIndexAt(Structure(DataGrid) dataGrid, Numerical index, SimpleType inputValue)\n</code></pre> <p>Computes the <code>Numerical</code> part index of a value given a data grid and an index of dimension in the data grid. The index must be between 1 and the number of dimension in the data grid, and the value of the type (Numerical or Categorical) relative to the given dimension of the data grid.</p>"},{"location":"api-docs/kdic/recoding-rules/#partidat","title":"PartIdAt","text":"<pre><code>Categorical PartIdAt(Structure(DataGrid) dataGrid, Numerical index, SimpleType inputValue)\n</code></pre> <p>Computes the categorical part identifier of a value given a data grid and an index of dimension in the data grid.</p>"},{"location":"api-docs/kdic/recoding-rules/#valuerank","title":"ValueRank","text":"<pre><code>Numerical ValueRank(Structure(DataGrid) dataGrid, Numerical inputValue)\n</code></pre> <p>Computes the average normalized rank of a numerical value given a univariate numerical data grid.</p>"},{"location":"api-docs/kdic/recoding-rules/#inversevaluerank","title":"InverseValueRank","text":"<pre><code>Numerical InverseValueRank(Structure(DataGrid) dataGrid, Numerical inputRank)\n</code></pre> <p>Computes the average value related to a normalized rank given a univariate numerical data grid.</p>"},{"location":"api-docs/kdic/recoding-rules/#datagridstats","title":"DataGridStats","text":"<pre><code>Structure(DataGridStats) DataGridStats(Structure(DataGrid) dataGrid, SimpleType inputValue1, ...)\n</code></pre> <p>Computes statistics (conditional probabilities) for a list of values given a data grid. The number of input values correspond of the inputs variables, which might be inferior to the number of dimensions in that data grid. The remaining dimensions in the data grid correspond to the output variables.</p>"},{"location":"api-docs/kdic/recoding-rules/#sourceconditionalinfo","title":"SourceConditionalInfo","text":"<pre><code>Numerical SourceConditionalInfo(Structure(DataGridStats), Numerical outputIndex)\n</code></pre> <p>Computes the source conditional info (negative log of the conditional probability) for the input cell related to the data grids stats and for the index of the target cell of the data grid given as a parameter (index starts at 1).</p>"},{"location":"api-docs/kdic/recoding-rules/#intervalid","title":"IntervalId","text":"<pre><code>Categorical IntervalId(Structure(IntervalBounds) intervalBounds, Numerical value)\n</code></pre> <p>Computes the <code>Categorical</code> part identifier of a <code>Numerical</code> value given a partition into intervals.</p>"},{"location":"api-docs/kdic/recoding-rules/#valueid","title":"ValueId","text":"<pre><code>Categorical ValueId(Structure(ValueSet) values, Numerical value)\n</code></pre> <p>Computes the <code>Categorical</code> part identifier of a <code>Numerical</code> value given a partition into a set of numerical values.</p>"},{"location":"api-docs/kdic/recoding-rules/#groupid","title":"GroupId","text":"<pre><code>Categorical GroupId(Structure(ValueGroups) valueGroups, Categorical value)\n</code></pre> <p>Computes the <code>Categorical</code> part identifier of a <code>Categorical</code> value given a partition into a set of groups of categorical values.</p>"},{"location":"api-docs/kdic/recoding-rules/#valueidc","title":"ValueIdC","text":"<pre><code>Categorical ValueIdC(Structure(ValueSetC) values, Categorical value)\n</code></pre> <p>Computes the <code>Categorical</code> part identifier of a <code>Categorical</code> value given a partition into a set of categorical values. If value is not found in the value set, returns the part identifier of the special value <code>*</code> if it is defined, 1 otherwise.</p>"},{"location":"api-docs/kdic/recoding-rules/#intervalindex","title":"IntervalIndex","text":"<pre><code>Numerical IntervalIndex(Structure(IntervalBounds) intervalBounds, Numerical value)\n</code></pre> <p>Computes the <code>Numerical</code> part index of a <code>Numerical</code> value given a partition into intervals.</p>"},{"location":"api-docs/kdic/recoding-rules/#valueindex","title":"ValueIndex","text":"<pre><code>Numerical ValueIndex(Structure(ValueSet) values, Numerical value)\n</code></pre> <p>Computes the <code>Numerical</code> part index of a <code>Numerical</code> value given a partition of numerical value sets.</p>"},{"location":"api-docs/kdic/recoding-rules/#groupindex","title":"GroupIndex","text":"<pre><code>Numerical GroupIndex(Structure(ValueGroups) valueGroups, Categorical value)\n</code></pre> <p>Computes the <code>Numerical</code> part index of a <code>Categorical</code> value given a partition of groups of categorical values.</p>"},{"location":"api-docs/kdic/recoding-rules/#valueindexc","title":"ValueIndexC","text":"<pre><code>Numerical ValueIndexC(Structure(ValueSetC) values, Categorical value)\n</code></pre> <p>Computes the <code>Numerical</code> part index of a <code>Categorical</code> value given a partition into a set of <code>Categorical</code> values. If the value is not found in the value set, it returns the index of the special value <code>*</code> if it is defined, 1 otherwise.</p>"},{"location":"api-docs/kdic/string-rules/","title":"String Rules","text":"<p>If a missing value is used as operand for a rule returning a <code>Categorical</code> value, the return value is the empty string.</p>"},{"location":"api-docs/kdic/string-rules/#length","title":"Length","text":"<pre><code>Numerical Length(Categorical value)\n</code></pre> <p>Length in chars of a categorical value.</p>"},{"location":"api-docs/kdic/string-rules/#left","title":"Left","text":"<pre><code>Categorical Left(Categorical value, Numerical charNumber)\n</code></pre> <p>Extraction of the left substring of a categorical value.</p> <p>If <code>charNumber</code> is less than 0, returns an empty value.</p> <p>If <code>charNumber</code> is beyond the value length, returns the input value.</p>"},{"location":"api-docs/kdic/string-rules/#right","title":"Right","text":"<pre><code>Categorical Right(Categorical value, Numerical charNumber)\n</code></pre> <p>Extraction of the right substring of a categorical value</p> <p>If <code>charNumber</code> is less than 0, returns an empty value.</p> <p>If <code>charNumber</code> is beyond the value length, returns the input value.</p>"},{"location":"api-docs/kdic/string-rules/#middle","title":"Middle","text":"<pre><code>Categorical Middle(Categorical value, Numerical startChar, Numerical charNumber)\n</code></pre> <p>Extraction of the middle substring of a categorical value.</p> <p>If <code>startChar</code> is not valid (must start at 1), returns and empty value.</p> <p>If <code>charNumber</code> is less than 0, returns an empty value.</p> <p>If the end of the extraction is beyond the value length, returns the end of the input value.</p>"},{"location":"api-docs/kdic/string-rules/#tokenlength","title":"TokenLength","text":"<pre><code>Numerical TokenLength(Categorical value, Categorical separators)\n</code></pre> <p>Length in tokens of a categorical value.</p> <p>A token is a non-empty substring that does not contain any separator character. The tokens are separated by one or many separator characters, which definition is given in the separator parameter.</p> <p>If the separator parameter is empty, there is at most one token in the input value.</p> <p>Example</p> <p>Using separators <code>\" ,\"</code> (blank and comma), the categorical value <code>\" Numbers: 1, 2, 3.14, 4,5\"</code> contains exactly six tokens: <code>Numbers:</code>,  <code>1</code> <code>2</code> <code>3.14</code> <code>4</code> <code>5</code>.</p>"},{"location":"api-docs/kdic/string-rules/#tokenleft","title":"TokenLeft","text":"<pre><code>Categorical TokenLeft(Categorical value, Categorical separators, Numerical tokenNumber)\n</code></pre> <p>Extraction of the left tokens in a categorical value.</p> <p>If several tokens are extracted, they remain separated by the initial separator characters used in the input value.</p> <p>If the <code>tokenNumber</code> is less than 0, returns an empty value.</p> <p>If the number of tokens is beyond the token length, returns the input value (cleaned from its begin and end separators).</p>"},{"location":"api-docs/kdic/string-rules/#tokenright","title":"TokenRight","text":"<pre><code>Categorical TokenRight(Categorical value, Categorical separators, Numerical tokenNumber)\n</code></pre> <p>Extraction of the right tokens in a categorical value.</p> <p>If several tokens are extracted, they remain separated by the initial separator characters used in the input value.</p> <p>If the <code>tokenNumber</code> is less than 0, returns an empty value.</p> <p>If the number of tokens is beyond the token length, returns the input value (cleaned from its begin and end separators).</p>"},{"location":"api-docs/kdic/string-rules/#tokenmiddle","title":"TokenMiddle","text":"<pre><code>Categorical TokenMiddle(\n    Categorical value, Categorical separators, Numerical startToken, Numerical tokenNumber\n)\n</code></pre> <p>Extraction of the middle tokens in a categorical value.</p> <p>If <code>startToken</code> is not valid (must start at 1), returns and empty value.</p> <p>If several tokens are extracted, they remain separated by the initial separator characters used in the input value.</p> <p>If the <code>tokenNumber</code> is less than 0, returns an empty value.</p> <p>If the number of tokens is beyond the token length, returns the input value (cleaned from its begin and end separators).</p>"},{"location":"api-docs/kdic/string-rules/#translate","title":"Translate","text":"<pre><code>Categorical Translate(\n    Categorical value, Structure(VectorC) searchValues, Structure(VectorC) replaceValues\n)\n</code></pre> <p>Replace substrings in a categorical value. The replacement is performed in sequence with each search value in the first parameter vector replaced by its corresponding value in the second parameter vector.</p> <p>Example</p> <p>The following rule allows to replace accented characters with regular characters:</p> <pre><code>Translate(inputValue, VectorC(\"\u00e9\", \"\u00e8\", \"\u00ea\", \"\u00e0\", \"\u00ef\", \"\u00e7\"), VectorC(\"e\", \"e\", \"e\", \"a\", \"i\", \"c\"))\n</code></pre>"},{"location":"api-docs/kdic/string-rules/#search","title":"Search","text":"<pre><code>Numerical Search(Categorical value, Numerical startChar, Categorical searchValue)\n</code></pre> <p>Searches the position of a substring in a categorical value.</p> <p>If <code>startChar</code> is not valid (must start at 1), returns -1.</p> <p>If the substring is not found, returns -1.</p>"},{"location":"api-docs/kdic/string-rules/#replace","title":"Replace","text":"<pre><code>Categorical Replace(\n    Categorical value, Numerical startChar, Categorical searchValue, Categorical replaceValue\n)\n</code></pre> <p>Replaces a substring in a categorical value.</p> <p>If <code>startChar</code> is not valid (must start at 1), returns the input value.</p> <p>If the substring is not found, returns the input value, otherwise returns the modified value.</p>"},{"location":"api-docs/kdic/string-rules/#replaceall","title":"ReplaceAll","text":"<pre><code>Categorical ReplaceAll(\n    Categorical value, Numerical startChar, Categorical searchValue, Categorical replaceValue\n)\n</code></pre> <p>Replaces all substring in a categorical value.</p> <p>It is the same as the <code>Replace</code> rule, except that Replace applies only to the first found searched values, whereas <code>ReplaceAll</code> applies to all found searched values</p>"},{"location":"api-docs/kdic/string-rules/#regexmatch","title":"RegexMatch","text":"<pre><code>Numerical RegexMatch(Categorical value, Categorical regexValue)\n</code></pre> <p>Returns 1 if the entire value matches the regex, 0 otherwise.</p> <p>The syntax for regular expressions is that of ECMAScript syntax (JavaScript).</p> <p>For more details, see the reference.</p>"},{"location":"api-docs/kdic/string-rules/#regexsearch","title":"RegexSearch","text":"<pre><code>Numerical RegexSearch(Categorical value, Numerical startChar, Categorical regexValue)\n</code></pre> <p>Searches the position of a regular expression in a categorical value.</p> <p>If <code>startChar</code> is not valid (must start at 1), returns -1.</p> <p>If the regular expression is not found, returns -1.</p>"},{"location":"api-docs/kdic/string-rules/#regexreplace","title":"RegexReplace","text":"<pre><code>Categorical RegexReplace(\n    Categorical value, Numerical startChar, Categorical regexValue, Categorical replaceValue\n)\n</code></pre> <p>Replaces a regular expression in a categorical value.</p> <p>If <code>startChar</code> is not valid (must start at 1), returns the input value.</p> <p>If the regular expression is not found, returns the input value, otherwise returns the modified value.</p>"},{"location":"api-docs/kdic/string-rules/#regexreplaceall","title":"RegexReplaceAll","text":"<pre><code>Categorical RegexReplaceAll(Categorical value, Numerical startChar, Categorical regexValue, Categorical replaceValue)\n</code></pre> <p>Replaces all found regular expression in a categorical value.</p> <p>It is the same as the <code>RegexReplace</code> rule, except that <code>RegexReplace</code> applies only to the first found searched values, whereas <code>RegexReplaceAll</code> applies to all found searched values</p>"},{"location":"api-docs/kdic/string-rules/#toupper","title":"ToUpper","text":"<pre><code>Categorical ToUpper(Categorical value)\n</code></pre> <p>Conversion to upper case of a categorical value.</p>"},{"location":"api-docs/kdic/string-rules/#tolower","title":"ToLower","text":"<pre><code>Categorical ToLower(Categorical value)\n</code></pre> <p>Conversion to lower case of a categorical value.</p>"},{"location":"api-docs/kdic/string-rules/#concat","title":"Concat","text":"<pre><code>Categorical Concat(Categorical value1,...)\n</code></pre> <p>Concatenation of categorical values.</p>"},{"location":"api-docs/kdic/string-rules/#hash","title":"Hash","text":"<pre><code>Numerical Hash(Categorical value, Numerical max)\n</code></pre> <p>Computes a hash value of a categorical value, between 0 and max-1.</p>"},{"location":"api-docs/kdic/string-rules/#encrypt","title":"Encrypt","text":"<pre><code>Categorical Encrypt(Categorical value, Categorical key)\n</code></pre> <p>Encryption of a categorical value using an encryption key.</p> <p>The encryption method used a \"randomized\" version of the input value. This is not a public encryption method, and it is convenient for basic use such as making the data anonymous. The encrypted value contains only alphanumeric characters. No reverse encryption method is provided.</p> <p>Warning</p> <p>Non printable characters are first replaced by blank characters, prior to encryption.</p>"},{"location":"api-docs/kdic/structures-introduction/","title":"Technical Structures","text":"<p>This section summarizes the derivation rules that are used internally by Khiops to:</p> <ul> <li>store data preparation and predictor models in dictionary files, and</li> <li>to compute any model output values such as variable recoding, prediction, density estimation.</li> </ul> <p>The rules in this section are not intended to be used by the user to construct new variables, but rather to have a quick understanding of them.</p>"},{"location":"api-docs/kdic/structures-introduction/#overview","title":"Overview","text":"<p>Khiops exploits several technical types to store the results of data preparation and modeling into dictionaries:</p> <ul> <li>Vectors:<ul> <li><code>Structure(VectorC)</code>: <code>Categorical</code> values vector.</li> <li><code>Structure(Vector)</code>: <code>Numerical</code> values vector.</li> </ul> </li> <li>Hash Maps:<ul> <li><code>Structure(HashMapC)</code>: <code>Categorical</code> values hash map.</li> <li><code>Structure(HashMap)</code>: <code>Numerical</code> values hash map.</li> </ul> </li> <li>Data Preparation:<ul> <li><code>Structure(DataGrid)</code>: A data grid is defined by its input partitions (discretizations or value   groupings) and by the frequencies of the cells of the cross-product of the input partitions.</li> <li>Partitions:<ul> <li><code>Structure(IntervalBounds)</code>: <code>Numerical</code> partition, defined by a sorted list of interval   bounds.</li> <li><code>Structure(ValueGroups)</code>: <code>Categorical</code> partition, defined by a set of exclusive groups of   values (<code>Structure(ValueGroup)</code>).</li> <li><code>Structure(ValueGroup)</code>: Set of exclusive categorical values.</li> <li><code>Structure(ValueSet)</code>: <code>Numerical</code> partition, defined by a set of exclusive numerical values.</li> <li><code>Structure(ValueSetC)</code>: <code>Categorical</code> partition, defined by a set of exclusive categorical   values.</li> </ul> </li> <li><code>Structure(Frequencies)</code>: Frequency vector.</li> </ul> </li> <li>Data Preparation Statistics:<ul> <li><code>Structure(DataGridStats)</code>: The <code>DataGridStats</code> structure is defined by a data grid and a set   a variables related to the input partitions of the data grid; the statistics are computed with   respect to the data grid cell related to the input values.</li> </ul> </li> <li>Predictors:<ul> <li><code>Structure(Classifier)</code>: Classifier specification.</li> <li><code>Structure(Regressor)</code>: Regressor specification.</li> <li><code>Structure(RankRegressor)</code>: Rank regressor specification.</li> </ul> </li> <li>Coclustering Deployment:</li> <li><code>Structure(DataGridDeployment)</code>: The <code>DataGridDeployment</code> structure is defined by a data grid,     the index of the deployed variable and a set a vectors of values (<code>Vector</code> and <code>VectorC</code>) for     each input partitions of the data grid except the deployed variable, plus an optional vector of     frequencies; this structure provides predictions related to the deployed variable, given the     input distribution of the other variables.</li> </ul>"},{"location":"api-docs/kdic/table-management-rules/","title":"Table Management Rules","text":"<p>We modify our <code>Customer</code> example by replacing the <code>sales</code> table with three tables with the same <code>Sale</code> schema, one for each month from January through March. <pre><code>Root Dictionary Customer(customer_id)\n{\n  Categorical customer_id;\n  Numerical age;\n  Categorical sex;\n  Entity(Address) customerAddress; // 0-1 relationship\n  Table(Sale) salesJanuary;        // 0-n relationship\n  Table(Sale) salesFebruary;       // 0-n relationship\n  Table(Sale) salesMarch;          // 0-n relationship\n};\n\n// the same as the first Customer example\n// ...\n</code></pre></p>"},{"location":"api-docs/kdic/table-management-rules/#tableat","title":"TableAt","text":"<pre><code>Entity TableAt(Table table, Numerical rank)\n</code></pre> <p>Extraction of an entity of a table at a given rank.</p> <p>Example</p> Delta from the Base ExampleFull Example <pre><code>Entity(Sale) firstJanuarySale = TableAt(salesJanuary, 1); // First sale of January\n</code></pre> <pre><code>Root Dictionary Customer(customer_id)\n{\n  Categorical customer_id;\n  Numerical age;\n  Categorical sex;\n  Entity(Address) customerAddress; // 0-1 relationship\n  Table(Sale) salesJanuary;        // 0-n relationship\n  Table(Sale) salesFebruary;       // 0-n relationship\n  Table(Sale) salesMarch;          // 0-n relationship\n  Entity(Sale) firstJanuarySale = TableAt(salesJanuary, 1); // First sale of January\n};\n\nDictionary Address(customer_id)\n{\n  Categorical customer_id;\n  Categorical street;\n  Categorical city;\n  Categorical zipcode;\n  Categorical State;\n};\n\nDictionary Sale(customer_id)\n{\n  Categorical customer_id;\n  Categorical product;\n  Numerical cost;\n  Date purchaseDate;\n};\n</code></pre>"},{"location":"api-docs/kdic/table-management-rules/#tableatkey","title":"TableAtKey","text":"<pre><code>Entity TableAtKey(Table table, Categorical keyField1, Categorical keyField2, ...)\n</code></pre> <p>Extraction of an entity of a table at a given key. The number of key fields must match that of the dictionary of the table.</p>"},{"location":"api-docs/kdic/table-management-rules/#tableextraction","title":"TableExtraction","text":"<pre><code>Table TableExtraction(Table table, Numerical firstRank, Numerical lastRank)\n</code></pre> <p>Extraction of a sub-table containing the entities of the table between <code>firstRank</code> and <code>lastRank</code>. Ranks below 1 or beyond the size of the table are ignored.</p> <p>Example</p> Delta from the Base ExampleFull Example <pre><code>Table(Sale) firstTenJanuarySales = TableExtraction( salesJanuary, 1, 10 ); // First 10 sales of January\n</code></pre> <pre><code>Root Dictionary Customer(customer_id)\n{\n  Categorical customer_id;\n  Numerical age;\n  Categorical sex;\n  Entity(Address) customerAddress; // 0-1 relationship\n  Table(Sale) salesJanuary;        // 0-n relationship\n  Table(Sale) salesFebruary;       // 0-n relationship\n  Table(Sale) salesMarch;          // 0-n relationship\n  Table(Sale) firstTenJanuarySales = TableExtraction(salesJanuary, 1, 10); // First 10 sales of January\n};\n\nDictionary Address(customer_id)\n{\n  Categorical customer_id;\n  Categorical street;\n  Categorical city;\n  Categorical zipcode;\n  Categorical State;\n};\n\nDictionary Sale(customer_id)\n{\n  Categorical customer_id;\n  Categorical product;\n  Numerical cost;\n  Date purchaseDate;\n};\n</code></pre>"},{"location":"api-docs/kdic/table-management-rules/#tableselection","title":"TableSelection","text":"<pre><code>Table TableSelection(Table table, Numerical selectionCriterion)\n</code></pre> <p>Selection of a sub-table containing the entities of the table that meet the selection criterion.</p> <p>Example</p> Delta from the Base ExampleFull Example <pre><code>// January sales for product \"Foo\"\nTable(Sale) januaryProductFooSales = TableSelection(salesJanuary, EQc(product,\u201dFoo\u201d));\n// Sales for first week of year\nTable(Sale) week1Sales = TableSelection(salesJanuary, LE(YearDay(purchaseDate), 7)));\n</code></pre> <pre><code>Root Dictionary Customer(customer_id)\n{\n  Categorical customer_id;\n  Numerical age;\n  Categorical sex;\n  Entity(Address) customerAddress; // 0-1 relationship\n  Table(Sale) salesJanuary;        // 0-n relationship\n  Table(Sale) salesFebruary;       // 0-n relationship\n  Table(Sale) salesMarch;          // 0-n relationship\n  // January sales for product \"Foo\"\n  Table(Sale) januaryProductFooSales = TableSelection(salesJanuary, EQc(product,\u201dFoo\u201d));\n  // Sales for first week of year\n  Table(Sale) week1Sales = TableSelection(salesJanuary, LE(YearDay(purchaseDate), 7)));\n};\n\nDictionary Address(customer_id)\n{\n  Categorical customer_id;\n  Categorical street;\n  Categorical city;\n  Categorical zipcode;\n  Categorical State;\n};\n\nDictionary Sale(customer_id)\n{\n  Categorical customer_id;\n  Categorical product;\n  Numerical cost;\n  Date purchaseDate;\n};\n</code></pre>"},{"location":"api-docs/kdic/table-management-rules/#tableselectfirst","title":"TableSelectFirst","text":"<pre><code>Entity TableSelectFirst(Table table, Numerical selectionCriterion)\n</code></pre> <p>Return the first entity of the table that meet the selection criterion. It is equivalent to combining the <code>TableSelection</code> and <code>TableAt</code> rules at the first rank.</p> <p>Example</p> Delta from the Base ExampleFull Example <pre><code>// First sale among those of the year's first week\nEntity(Sale) firstWeek1Sales = TableSelectFirst( salesJanuary, LE(YearDay(purchaseDate), 7)) );\n</code></pre> <pre><code>Root Dictionary Customer(customer_id)\n{\n  Categorical customer_id;\n  Numerical age;\n  Categorical sex;\n  Entity(Address) customerAddress; // 0-1 relationship\n  Table(Sale) salesJanuary;        // 0-n relationship\n  Table(Sale) salesFebruary;       // 0-n relationship\n  Table(Sale) salesMarch;          // 0-n relationship\n  // First sale among those of the year's first week\n  Entity(Sale) firstWeek1Sales = TableSelectFirst(salesJanuary, LE(YearDay(purchaseDate), 7)));\n};\n\nDictionary Address(customer_id)\n{\n  Categorical customer_id;\n  Categorical street;\n  Categorical city;\n  Categorical zipcode;\n  Categorical State;\n};\n\nDictionary Sale(customer_id)\n{\n  Categorical customer_id;\n  Categorical product;\n  Numerical cost;\n  Date purchaseDate;\n};\n</code></pre>"},{"location":"api-docs/kdic/table-management-rules/#tablesort","title":"TableSort","text":"<pre><code>Table TableSort(Table table, SimpleType sortValue1, SimpleType sortValue2, ...)\n</code></pre> <p>Sorts a table by increasing order according to a list of one to many sort values. Each sort value is of type <code>Numerical</code>, <code>Categorical</code>, <code>Time</code>, <code>Date</code>, <code>Timestamp</code> or <code>TimestampTZ</code>, and can be either a native table variable or the result of a rule.</p>"},{"location":"api-docs/kdic/table-management-rules/#entityset","title":"EntitySet","text":"<pre><code>Table EntitySet(Entity entity1, Entity entity2, ...)\n</code></pre> <p>Builds a table from a set of entities. All the entities in the operands must have the same <code>Dictionary</code> definition; the result table will also have the same <code>Dictionary</code> definition.</p> <p>Example</p> Delta from the Base ExampleFull Example <pre><code>// Sales table for the first quarter\nTable(Sale) salesSamples = EntitySet(\n  TableAt(salesJanuary, 1), TableAt(salesFebruary, 1), TableAt(salesMarch, 1)\n);\n</code></pre> <pre><code>Root Dictionary Customer(customer_id)\n{\n  Categorical customer_id;\n  Numerical age;\n  Categorical sex;\n  Entity(Address) customerAddress; // 0-1 relationship\n  Table(Sale) salesJanuary;        // 0-n relationship\n  Table(Sale) salesFebruary;       // 0-n relationship\n  Table(Sale) salesMarch;          // 0-n relationship\n  // Sales table for the first quarter\n  Table(Sale) salesSamples = EntitySet(\n    TableAt(salesJanuary, 1), TableAt(salesFebruary, 1), TableAt(salesMarch, 1)\n  );\n};\n\nDictionary Address(customer_id)\n{\n  Categorical customer_id;\n  Categorical street;\n  Categorical city;\n  Categorical zipcode;\n  Categorical State;\n};\n\nDictionary Sale(customer_id)\n{\n  Categorical customer_id;\n  Categorical product;\n  Numerical cost;\n  Date purchaseDate;\n};\n</code></pre>"},{"location":"api-docs/kdic/table-management-rules/#tableunion","title":"TableUnion","text":"<pre><code>Table TableUnion(Table table1, Table table2, ...)\n</code></pre> <p>Union of a set of tables. The union table contains the entities that belong to one of the table operands.</p> <p>Example</p> Delta from the Base ExampleFull Example <pre><code>// Sales table for the first quarter\nTable(Sale) salesQuarter1 = TableUnion(salesJanuary, salesFebruary, salesMarch);\n</code></pre> <pre><code>Root Dictionary Customer(customer_id)\n{\n  Categorical customer_id;\n  Numerical age;\n  Categorical sex;\n  Entity(Address) customerAddress; // 0-1 relationship\n  Table(Sale) salesJanuary;        // 0-n relationship\n  Table(Sale) salesFebruary;       // 0-n relationship\n  Table(Sale) salesMarch;          // 0-n relationship\n  // Sales table for the first quarter\n  Table(Sale) salesQuarter1 = TableUnion(salesJanuary, salesFebruary, salesMarch);\n  );\n};\n\nDictionary Address(customer_id)\n{\n  Categorical customer_id;\n  Categorical street;\n  Categorical city;\n  Categorical zipcode;\n  Categorical State;\n};\n\nDictionary Sale(customer_id)\n{\n  Categorical customer_id;\n  Categorical product;\n  Numerical cost;\n  Date purchaseDate;\n};\n</code></pre>"},{"location":"api-docs/kdic/table-management-rules/#tableintersection","title":"TableIntersection","text":"<pre><code>Table TableIntersection(Table table1, Table table2, ...)\n</code></pre> <p>Intersection of a set of tables. The intersection table contains the entities that belong to all the table operands.</p> <p>Example</p> Delta from the Base ExampleFull Example <pre><code>// Sales of the first quarter\nTable(Sale) salesQuarter1 = TableUnion(salesJanuary, salesFebruary, salesMarch);\n// January sales (same as salesJanuary)\nTable(Sale) salesMonth1 = TableIntersection(salesJanuary, salesQuarter1);\n</code></pre> <pre><code>Root Dictionary Customer(customer_id)\n{\n  Categorical customer_id;\n  Numerical age;\n  Categorical sex;\n  Entity(Address) customerAddress; // 0-1 relationship\n  Table(Sale) salesJanuary;        // 0-n relationship\n  Table(Sale) salesFebruary;       // 0-n relationship\n  Table(Sale) salesMarch;          // 0-n relationship\n  // Sales of the first quarter\n  Table(Sale) salesQuarter1 = TableUnion(salesJanuary, salesFebruary, salesMarch);\n  // January sales (same as salesJanuary)\n  Table(Sale) salesMonth1 = TableIntersection(salesJanuary, salesQuarter1);\n  );\n};\n\nDictionary Address(customer_id)\n{\n  Categorical customer_id;\n  Categorical street;\n  Categorical city;\n  Categorical zipcode;\n  Categorical State;\n};\n\nDictionary Sale(customer_id)\n{\n  Categorical customer_id;\n  Categorical product;\n  Numerical cost;\n  Date purchaseDate;\n};\n</code></pre>"},{"location":"api-docs/kdic/table-management-rules/#tabledifference","title":"TableDifference","text":"<pre><code>Table TableDifference(Table table1, Table table2)\n</code></pre> <p>Difference between two tables. The difference table contains the entities that belong to either of the two table operands, but not to their intersection.</p> <p>Example</p> Delta from the Base ExampleFull Example <pre><code>// Sales table for months 1 and 2\nTable(Sale) salesMonth1and2 = TableUnion(salesJanuary, salesFebruary);\n// Sales table for months 2 and 3\nTable(Sale) salesMonth2and3 = TableUnion(salesFebruary, salesMarch);\n// Sales table for the first and the third month\n// Note that this is the same as TableUnion(salesJanuary, salesFebruary)\nTable(Sale) salesMonth2and3 = TableDifference(salesMonth1and2, salesMonth2and3);\n</code></pre> <pre><code>Root Dictionary Customer(customer_id)\n{\n  Categorical customer_id;\n  Numerical age;\n  Categorical sex;\n  Entity(Address) customerAddress; // 0-1 relationship\n  Table(Sale) salesJanuary;        // 0-n relationship\n  Table(Sale) salesFebruary;       // 0-n relationship\n  Table(Sale) salesMarch;          // 0-n relationship\n  // Sales table for months 1 and 2\n  Table(Sale) salesMonth1and2 = TableUnion(salesJanuary, salesFebruary);\n  // Sales table for months 2 and 3\n  Table(Sale) salesMonth2and3 = TableUnion(salesFebruary, salesMarch);\n  // Sales table for the first and the third month\n  // Note that this is the same as TableUnion(salesJanuary, salesFebruary)\n  Table(Sale) salesMonth2and3 = TableDifference(salesMonth1and2, salesMonth2and3);\n};\n\nDictionary Address(customer_id)\n{\n  Categorical customer_id;\n  Categorical street;\n  Categorical city;\n  Categorical zipcode;\n  Categorical State;\n};\n\nDictionary Sale(customer_id)\n{\n  Categorical customer_id;\n  Categorical product;\n  Numerical cost;\n  Date purchaseDate;\n};\n</code></pre>"},{"location":"api-docs/kdic/table-management-rules/#tablesubunion","title":"TableSubUnion","text":"<pre><code>Table TableSubUnion(Table table, Table subTable)\n</code></pre> <p>Union of the sub-tables of a table. This applies in the case of a snowflake schema.</p> <p>Example</p> <p>For this example we use a different and slightly more complex version of the <code>Customer</code> database, specifically we have the following schema <pre><code>Customer\n|\n+--0-n-- Service\n|        |\n|        +--0-n-- Usage\n|\n+--0-1-- Address\n</code></pre> So we have a Customer that has signed up for one or more services. For each service, the customer may have one or more usages. The customer has a unique address as before.</p> <p>We use the <code>TableSubUnion</code> rule to obtain all usages for a given customer.</p> <pre><code>Root Dictionary Customer(id_customer)\n{\n  Categorical id_customer;\n  Categorical Name;\n  Table(Service) Services;\n  Entity(Address) Address;\n  // Table of all usages for all services\n  Table(Usage) allUsages = TableSubUnion(services, usages);\n};\n\nDictionary Address(id_customer)\n{\n  Categorical id_customer;\n  Numerical StreetNumber;\n  Categorical StreetName;\n  Categorical id_city;\n};\n\nDictionary Service(id_customer, id_product)\n{\n  Categorical id_customer;\n  Categorical id_product;\n  Date SubscriptionDate;\n  Table(Usage) Usages;\n};\n\nDictionary Usage(id_customer, id_product)\n{\n  Categorical id_customer;\n  Categorical id_product;\n  Date Date;\n  Time Time;\n  Numerical Duration;\n};\n</code></pre>"},{"location":"api-docs/kdic/table-management-rules/#tablesubintersection","title":"TableSubIntersection","text":"<pre><code>Table TableSubIntersection(Table table, Table subTable)\n</code></pre> <p>Intersection of the sub-tables of a table.</p>"},{"location":"api-docs/kdic/table-rules/","title":"Table Rules","text":"<p>Table rules return an empty (categorical) or missing (numerical) value when the table is empty.</p> <p>If the table is empty, the rules below return:</p> <ul> <li>An empty value (<code>\"\"</code>) for rules that return <code>Categorical</code></li> <li>A missing value for rules that return <code>Numerical</code></li> </ul>"},{"location":"api-docs/kdic/table-rules/#tablecount","title":"TableCount","text":"<pre><code>Numerical TableCount(Table table)\n</code></pre> <p>Size of a table.</p> <p>Example</p> Diff from Base ExampleFull Example <pre><code>// Number of sales for a customer\nNumerical saleNumber = TableCount(sales);\n</code></pre> <pre><code>Root Dictionary Customer(customer_id)\n{\n  Categorical customer_id;\n  Numerical age;\n  Categorical sex;\n  Entity(Address) customerAddress; // 0-1 relationship\n  Table(Sale) sales;               // 0-n relationship\n  // Number of sales for a customer\n  Numerical saleNumber = TableCount(sales);\n};\n\nDictionary Address(customer_id)\n{\n  Categorical customer_id;\n  Categorical street;\n  Categorical city;\n  Categorical zipcode;\n  Categorical State;\n};\n\nDictionary Sale(customer_id)\n{\n  Categorical customer_id;\n  Categorical product;\n  Numerical cost;\n  Date purchaseDate;\n};\n</code></pre>"},{"location":"api-docs/kdic/table-rules/#tablecountdistinct","title":"TableCountDistinct","text":"<pre><code>Numerical TableCountDistinct(Table table, Categorical value)\n</code></pre> <p>Number of distinct values for a <code>Categorical</code> value in a table. A missing value is considered as a special value (empty) and counted as well.</p> <p>Example</p> Diff from Base ExampleFull Example <pre><code>// Number of different products in a customer's sales\nNumerical saleProductNumber = TableCountDistinct(sales, product);\n</code></pre> <pre><code>Root Dictionary Customer(customer_id)\n{\n  Categorical customer_id;\n  Numerical age;\n  Categorical sex;\n  Entity(Address) customerAddress; // 0-1 relationship\n  Table(Sale) sales;               // 0-n relationship\n  // Number of different products in customer's sales\n  Numerical saleProductNumber = TableCountDistinct(sales, product);\n};\n\nDictionary Address(customer_id)\n{\n  Categorical customer_id;\n  Categorical street;\n  Categorical city;\n  Categorical zipcode;\n  Categorical State;\n};\n\nDictionary Sale(customer_id)\n{\n  Categorical customer_id;\n  Categorical product;\n  Numerical cost;\n  Date purchaseDate;\n};\n</code></pre>"},{"location":"api-docs/kdic/table-rules/#tableentropy","title":"TableEntropy","text":"<pre><code>Numerical TableEntropy(Table table, Categorical value)\n</code></pre> <p>Entropy of a <code>Categorical</code> value in a table. The entropy of a categorical value is analogous to the variance of a <code>Numerical</code> value. It is large in case of all values having the same frequency in the table, and small in the case of few frequent values.</p> <p>Example</p> Diff from Base ExampleFull Example <pre><code>// Entropy of the distribution of products of a customer's sales\nNumerical saleProductEntropy = TableEntropy(sales, product);\n</code></pre> <pre><code>Root Dictionary Customer(customer_id)\n{\n  Categorical customer_id;\n  Numerical age;\n  Categorical sex;\n  Entity(Address) customerAddress; // 0-1 relationship\n  Table(Sale) sales;               // 0-n relationship\n  // Entropy of the distribution of products of a customer's sales\n  Numerical saleProductEntropy = TableEntropy(sales, product);\n};\n\nDictionary Address(customer_id)\n{\n  Categorical customer_id;\n  Categorical street;\n  Categorical city;\n  Categorical zipcode;\n  Categorical State;\n};\n\nDictionary Sale(customer_id)\n{\n  Categorical customer_id;\n  Categorical product;\n  Numerical cost;\n  Date purchaseDate;\n};\n</code></pre>"},{"location":"api-docs/kdic/table-rules/#tablemode","title":"TableMode","text":"<pre><code>Categorical TableMode(Table table, Categorical value)\n</code></pre> <p>Most frequent value for a <code>Categorical</code> value in a table. In case of ties in frequency, the method returns the first value by lexicographic order.</p> <p>Example</p> Diff from Base ExampleFull Example <pre><code>// Most frequent product in a customer's sales\nCategorical saleMainProduct = TableMode(sales, product);\n</code></pre> <pre><code>Root Dictionary Customer(customer_id)\n{\n  Categorical customer_id;\n  Numerical age;\n  Categorical sex;\n  Entity(Address) customerAddress; // 0-1 relationship\n  Table(Sale) sales;               // 0-n relationship\n  // Most frequent product in a customer's sales\n  Categorical saleMainProduct = TableMode(sales, product);\n};\n\nDictionary Address(customer_id)\n{\n  Categorical customer_id;\n  Categorical street;\n  Categorical city;\n  Categorical zipcode;\n  Categorical State;\n};\n\nDictionary Sale(customer_id)\n{\n  Categorical customer_id;\n  Categorical product;\n  Numerical cost;\n  Date purchaseDate;\n};\n</code></pre>"},{"location":"api-docs/kdic/table-rules/#tablemodeat","title":"TableModeAt","text":"<pre><code>Categorical TableModeAt(Table table, Categorical value, Numerical rank)\n</code></pre> <p>N-th most frequent value for a <code>Categorical</code> value in a table. Returns an empty value for ranks beyond the number of different values.</p> <p>Example</p> Diff from Base ExampleFull Example <pre><code>// Second most frequent product in a customer's sales\nCategorical saleSecondMainProduct = TableModeAt(sales, product, 2);\n</code></pre> <pre><code>Root Dictionary Customer(customer_id)\n{\n  Categorical customer_id;\n  Numerical age;\n  Categorical sex;\n  Entity(Address) customerAddress; // 0-1 relationship\n  Table(Sale) sales;               // 0-n relationship\n  // Second most frequent product in a customer's sales\n  Categorical saleSecondMainProduct = TableModeAt(sales, product, 2);\n};\n\nDictionary Address(customer_id)\n{\n  Categorical customer_id;\n  Categorical street;\n  Categorical city;\n  Categorical zipcode;\n  Categorical State;\n};\n\nDictionary Sale(customer_id)\n{\n  Categorical customer_id;\n  Categorical product;\n  Numerical cost;\n  Date purchaseDate;\n};\n</code></pre>"},{"location":"api-docs/kdic/table-rules/#tablemean","title":"TableMean","text":"<pre><code>Numerical TableMean(Table table, Numerical value)\n</code></pre> <p>Mean of numerical values in a table.</p> <p>This rule (and the other similar ones) takes only the non missing values into account. Its returns missing if the table is empty or if all the values are missing.</p> <p>Example</p> Diff from Base ExampleFull Example <pre><code>// Mean product cost for a customer's sales\nNumerical saleMeanCost = TableMean(sales, cost);\n</code></pre> <pre><code>Root Dictionary Customer(customer_id)\n{\n  Categorical customer_id;\n  Numerical age;\n  Categorical sex;\n  Entity(Address) customerAddress; // 0-1 relationship\n  Table(Sale) sales;               // 0-n relationship\n  // Mean product cost for a customer's sales\n  Numerical saleMeanCost = TableMean(sales, cost);\n};\n\nDictionary Address(customer_id)\n{\n  Categorical customer_id;\n  Categorical street;\n  Categorical city;\n  Categorical zipcode;\n  Categorical State;\n};\n\nDictionary Sale(customer_id)\n{\n  Categorical customer_id;\n  Categorical product;\n  Numerical cost;\n  Date purchaseDate;\n};\n</code></pre>"},{"location":"api-docs/kdic/table-rules/#tablestddev","title":"TableStdDev","text":"<pre><code>Numerical TableStdDev(Table table, Numerical value)\n</code></pre> <p>Standard deviation of numerical values in a table.</p> <p>Example</p> Diff from Base ExampleFull Example <pre><code>// Standard deviation of product costs for a customer's sales\nNumerical saleStdDevCost = TableStdDev(sales, cost);\n</code></pre> <pre><code>Root Dictionary Customer(customer_id)\n{\n  Categorical customer_id;\n  Numerical age;\n  Categorical sex;\n  Entity(Address) customerAddress; // 0-1 relationship\n  Table(Sale) sales;               // 0-n relationship\n  // Standard deviation of product costs for a customer's sales\n  Numerical saleStdDevCost = TableStdDev(sales, cost);\n};\n\nDictionary Address(customer_id)\n{\n  Categorical customer_id;\n  Categorical street;\n  Categorical city;\n  Categorical zipcode;\n  Categorical State;\n};\n\nDictionary Sale(customer_id)\n{\n  Categorical customer_id;\n  Categorical product;\n  Numerical cost;\n  Date purchaseDate;\n};\n</code></pre>"},{"location":"api-docs/kdic/table-rules/#tablemedian","title":"TableMedian","text":"<pre><code>Numerical TableMedian(Table table, Numerical value)\n</code></pre> <p>Median of numerical values in a table.</p> <p>Example</p> Diff from Base ExampleFull Example <pre><code>// Cost standard deviation for a customer's sales\nNumerical saleMedianCost = TableMedian(sales, cost);\n</code></pre> <pre><code>Root Dictionary Customer(customer_id)\n{\n  Categorical customer_id;\n  Numerical age;\n  Categorical sex;\n  Entity(Address) customerAddress; // 0-1 relationship\n  Table(Sale) sales;               // 0-n relationship\n  // Cost standard deviation for a customer's sales\n  Numerical saleStdDevCost = TableStdDev(sales, cost);\n};\n\nDictionary Address(customer_id)\n{\n  Categorical customer_id;\n  Categorical street;\n  Categorical city;\n  Categorical zipcode;\n  Categorical State;\n};\n\nDictionary Sale(customer_id)\n{\n  Categorical customer_id;\n  Categorical product;\n  Numerical cost;\n  Date purchaseDate;\n};\n</code></pre>"},{"location":"api-docs/kdic/table-rules/#tablemin","title":"TableMin","text":"<pre><code>Numerical TableMin(Table table, Numerical value)\n</code></pre> <p>Min of numerical values in a table.</p> <p>Example</p> Diff from Base ExampleFull Example <pre><code>// Minimum product cost for a customer's sales\nNumerical saleMinCost = TableMin(sales, cost);\n</code></pre> <pre><code>Root Dictionary Customer(customer_id)\n{\n  Categorical customer_id;\n  Numerical age;\n  Categorical sex;\n  Entity(Address) customerAddress; // 0-1 relationship\n  Table(Sale) sales;               // 0-n relationship\n  // Minimum product cost for a customer's sales\n  Numerical saleMinCost = TableMin(sales, cost);\n};\n\nDictionary Address(customer_id)\n{\n  Categorical customer_id;\n  Categorical street;\n  Categorical city;\n  Categorical zipcode;\n  Categorical State;\n};\n\nDictionary Sale(customer_id)\n{\n  Categorical customer_id;\n  Categorical product;\n  Numerical cost;\n  Date purchaseDate;\n};\n</code></pre>"},{"location":"api-docs/kdic/table-rules/#tablemax","title":"TableMax","text":"<pre><code>Numerical TableMax(Table table, Numerical value)\n</code></pre> <p>Max of numerical values in a table.</p> <p>Example</p> Diff from Base ExampleFull Example <pre><code>// Maximum product cost for a customer's sales\nNumerical saleMaxCost = TableMax(sales, cost);\n// Year's day of last purchase in sales\nNumerical saleLastYearDay = TableMax(sales, YearDay(purchaseDate));\n</code></pre> <pre><code>Root Dictionary Customer(customer_id)\n{\n  Categorical customer_id;\n  Numerical age;\n  Categorical sex;\n  Entity(Address) customerAddress; // 0-1 relationship\n  Table(Sale) sales;               // 0-n relationship\n  // Maximum product cost for a customer's sales\n  Numerical saleMaxCost = TableMax(sales, cost);\n  // Year's day of last customer sale\n  Numerical saleLastYearDay = TableMax(sales, YearDay(purchaseDate));\n};\n\nDictionary Address(customer_id)\n{\n  Categorical customer_id;\n  Categorical street;\n  Categorical city;\n  Categorical zipcode;\n  Categorical State;\n};\n\nDictionary Sale(customer_id)\n{\n  Categorical customer_id;\n  Categorical product;\n  Numerical cost;\n  Date purchaseDate;\n};\n</code></pre>"},{"location":"api-docs/kdic/table-rules/#tablesum","title":"TableSum","text":"<pre><code>Numerical TableSum(Table table, Numerical value)\n</code></pre> <p>Sum of numerical values in a table.</p> <p>Example</p> Diff from Base ExampleFull Example <pre><code>// Total cost of a customer's sales\nNumerical saleTotalCost = TableSum(sales, cost);\n</code></pre> <pre><code>Root Dictionary Customer(customer_id)\n{\n  Categorical customer_id;\n  Numerical age;\n  Categorical sex;\n  Entity(Address) customerAddress; // 0-1 relationship\n  Table(Sale) sales;               // 0-n relationship\n  // Total cost of a customer's sales\n  Numerical saleTotalCost = TableSum(sales, cost);\n};\n\nDictionary Address(customer_id)\n{\n  Categorical customer_id;\n  Categorical street;\n  Categorical city;\n  Categorical zipcode;\n  Categorical State;\n};\n\nDictionary Sale(customer_id)\n{\n  Categorical customer_id;\n  Categorical product;\n  Numerical cost;\n  Date purchaseDate;\n};\n</code></pre>"},{"location":"api-docs/kdic/time-rules/","title":"Time Rules","text":"<p><code>Time</code> values are encoded in data table files using Khiops native format <code>HH:MM:SS.</code>. Other formats are available, that allow to convert categorical values to time values. The <code>.</code> at the end of the format means that fractions of seconds are optional. The use of parenthesis in time formats means that the corresponding digit is optional when it is null (e.g. <code>9:30:8</code> with format <code>(H)H:(M)M:(S)S</code> corresponds to <code>09:30:08</code> with format <code>HH:MM:SS</code>):</p> <code>HH:MM:SS</code> <code>HH:MM:SS.</code> <code>HH:MM</code> <code>HH.MM.SS</code> <code>HH.MM.SS.</code> <code>HH.MM</code> <code>(H)H:(M)M:(S)S</code> <code>(H)H:(M)M:(S)S.</code> <code>(H)H:(M)M</code> <code>(H)H.(M)M.(S)S</code> <code>(H)H.(M)M.(S)S.</code> <code>(H)H.(M)M</code> <code>HHMMSS</code> <code>HHMMSS.</code> <code>HHMM</code> <p>Valid times range from <code>00:00:00</code> to <code>23:59:59</code>, with optional fractions of seconds up to 1/10000 of a second. Time rules return a missing value when their time operand is not valid or when a numerical operand is missing.</p>"},{"location":"api-docs/kdic/time-rules/#formattime","title":"FormatTime","text":"<pre><code>Categorical FormatTime(Time value, Categorical timeFormat)\n</code></pre> <p>Formats a time into a categorical value using a time format. Time format is a categorical constant value among the available time formats (for example: <code>HH:MM:SS</code>).</p>"},{"location":"api-docs/kdic/time-rules/#astime","title":"AsTime","text":"<pre><code>Time AsTime(Categorical timeString, Categorical timeFormat)\n</code></pre> <p>Recodes a categorical value into a time using a time format.</p> <p>Example</p> <pre><code>AsTime(\u201c18:25:00\u201d, \u201cHH:MM:SS\u201d)\n</code></pre>"},{"location":"api-docs/kdic/time-rules/#hour","title":"Hour","text":"<pre><code>Numerical Hour(Time value)\n</code></pre> <p>Hour in a time value.</p>"},{"location":"api-docs/kdic/time-rules/#minute","title":"Minute","text":"<pre><code>Numerical Minute(Time value)\n</code></pre> <p>Minute in a time value.</p>"},{"location":"api-docs/kdic/time-rules/#second","title":"Second","text":"<pre><code>Numerical Second(Time value)\n</code></pre> <p>Second in a time value.</p>"},{"location":"api-docs/kdic/time-rules/#daysecond","title":"DaySecond","text":"<pre><code>Numerical DaySecond(Time value)\n</code></pre> <p>Total second in a time value, since 00:00:00.</p>"},{"location":"api-docs/kdic/time-rules/#decimaltime","title":"DecimalTime","text":"<pre><code>Numerical DecimalTime(Time value)\n</code></pre> <p>Decimal day in a time value, between 0.0 and 23.9999.</p> <p>Precisely, <code>DecimalTime := DaySecond/(24 * 60 * 60)</code>.</p>"},{"location":"api-docs/kdic/time-rules/#difftime","title":"DiffTime","text":"<pre><code>Numerical DiffTime(Time value1, Time value2)\n</code></pre> <p>Difference in seconds between two time values.</p>"},{"location":"api-docs/kdic/time-rules/#istimevalid","title":"IsTimeValid","text":"<pre><code>Numerical IsTimeValid(Time value)\n</code></pre> <p>Checks if a time value is valid.</p>"},{"location":"api-docs/kdic/time-rules/#buildtime","title":"BuildTime","text":"<pre><code>Time BuildTime(Numerical hour, Numerical minute, Numerical second)\n</code></pre> <p>Builds a time value from <code>hour</code>, <code>minute</code> and <code>second</code>.</p> <p>The rule uses the floor value of hour and minute, and the full value of second. The hour must be between 0 and 23. The minute must be between 0 and 59. The second must be between 0 and 59, with optional fraction of second.</p>"},{"location":"api-docs/kdic/timestamp-rules/","title":"Timestamp Rules","text":"<p><code>Timestamp</code> values are encoded in data table files using Khiops native format <code>YYYY-MM-DD HH:MM:SS.</code>. Other formats are available, that allow to convert categorical values to timestamp values:</p> <ul> <li><code>&lt;Date format&gt; &lt;Time format&gt;</code>: A date format followed by a blank and a time format</li> <li><code>&lt;Date format&gt;-&lt;Time format&gt;</code>: A date format followed by a <code>-</code> and a time format</li> <li><code>&lt;Date format&gt;_&lt;Time format&gt;</code>: A date format followed by an <code>_</code> and a time format</li> <li><code>&lt;Date format&gt;T&lt;Time format&gt;</code>: A date format followed by a <code>T</code> and a time format</li> <li><code>&lt;Date format&gt;&lt;Time format&gt;</code>: A date format directly followed by a time format. Valid only in the   case of date and time formats with not separator character (e.g. <code>YYYYMMDDHHMMSS</code>).</li> </ul> <p><code>Timestamp</code> rules return a missing value when their timestamp operand is not valid or when a numerical operand is missing.</p>"},{"location":"api-docs/kdic/timestamp-rules/#formattimestamp","title":"FormatTimestamp","text":"<pre><code>Categorical FormatTimestamp(Timestamp value, Categorical timestampFormat)\n</code></pre> <p>Formats a timestamp into a categorical value using a timestamp format. Timestamp format is a categorical constant value among the available timestamp formats (for example: <code>YYYY-MM-DD HH:MM:SS</code>).</p>"},{"location":"api-docs/kdic/timestamp-rules/#astimestamp","title":"AsTimestamp","text":"<pre><code>Timestamp AsTimestamp(Categorical timestampString, Categorical timestampFormat)\n</code></pre> <p>Recodes a categorical value into a timestamp value using a timestamp format.</p> <p>Example</p> <pre><code>AsTimestamp(\u201c2014-01-15 18:25:00\u201d, \u201cYYYY-MM-DD HH:MM:SS\u201d).\n</code></pre>"},{"location":"api-docs/kdic/timestamp-rules/#getdate","title":"GetDate","text":"<pre><code>Date GetDate(Timestamp value)\n</code></pre> <p>Date in a timestamp value.</p>"},{"location":"api-docs/kdic/timestamp-rules/#gettime","title":"GetTime","text":"<pre><code>Time GetTime(Timestamp value)\n</code></pre> <p>Time in a timestamp value.</p>"},{"location":"api-docs/kdic/timestamp-rules/#decimalyearts","title":"DecimalYearTS","text":"<pre><code>Numerical DecimalYearTS(Timestamp value)\n</code></pre> <p>Year in a timestamp value, with decimal part for day in year, at a timestamp precision.</p>"},{"location":"api-docs/kdic/timestamp-rules/#absolutesecond","title":"AbsoluteSecond","text":"<pre><code>Numerical AbsoluteSecond(Timestamp value)\n</code></pre> <p>Total elapsed seconds since <code>2000-01-01 00:00:00</code>.</p>"},{"location":"api-docs/kdic/timestamp-rules/#decimalweekday","title":"DecimalWeekDay","text":"<pre><code>Numerical DecimalWeekDay(Timestamp value)\n</code></pre> <p>Week day of the date of the timestamp value, plus decimal day of the time.</p> <p>Precisely, <code>DecimalWeekDay := WeekDay(date) + DecimalTime(time)/24</code>.</p>"},{"location":"api-docs/kdic/timestamp-rules/#difftimestamp","title":"DiffTimestamp","text":"<pre><code>Numerical DiffTimestamp(Timestamp value1, Timestamp value2)\n</code></pre> <p>Difference in seconds between two timestamp values.</p>"},{"location":"api-docs/kdic/timestamp-rules/#addseconds","title":"AddSeconds","text":"<pre><code>Timestamp AddSeconds(Timestamp value, Numerical secondNumber)\n</code></pre> <p>Adds a number of seconds to a timestamp value.</p>"},{"location":"api-docs/kdic/timestamp-rules/#istimestampvalid","title":"IsTimestampValid","text":"<pre><code>Numerical IsTimestampValid(Timestamp value)\n</code></pre> <p>Checks if a timestamp value is valid.</p>"},{"location":"api-docs/kdic/timestamp-rules/#buildtimestamp","title":"BuildTimestamp","text":"<pre><code>Timestamp BuildTimestamp(Date dateValue, Time timeValue)\n</code></pre> <p>Builds a timestamp from a date and time values.</p>"},{"location":"api-docs/kdic/timestamp-tz-rules/","title":"TimestampTZ Rules","text":"<p><code>TimestampTZ</code> values are encoded in data table files using Khiops native format <code>YYYY-MM-DD HH:MM:SS.zzzzzz</code>.</p> <p><code>TimestampTZ</code> values consist of a local timestamp value together with time zone information, using the ISO 8601 time zone format:</p> <ul> <li><code>&lt;Timestamp format&gt;zzzzz</code>:<ul> <li>Basic time zone format (Z or +hhmm or \u2013hhmm)</li> </ul> </li> <li><code>&lt;Timestamp format&gt;zzzzzz</code>:<ul> <li>Extended time zone format (Z or +hh:mm or \u2013hh:mm), with hours and   minutes separated by <code>:</code></li> </ul> </li> </ul> <p><code>TimestampTZ</code> values are time zone-aware whereas Timestamp values are not.</p> <p><code>TimestampTZ</code> values can be transformed to <code>Timestamp</code> values using either the <code>LocalTimestamp</code> or <code>UtcTimestamp</code> rules. Then, the <code>Timestamp</code> rules that extract information can be used (ex: <code>GetDate</code>, <code>GetTime</code>, <code>DecimalYearTS</code>, <code>DecimalWeekDay</code>, <code>AbsoluteSecond</code>).</p> <p>TimestampTZ rules return a missing value when their timestampTZ operand is not valid or when a numerical operand is missing or invalid.</p>"},{"location":"api-docs/kdic/timestamp-tz-rules/#formattimestamptz","title":"FormatTimestampTZ","text":"<pre><code>Categorical FormatTimestampTZ(TimestampTZ value, Categorical timestampTZFormat)\n</code></pre> <p>Formats a <code>TimestampTZ</code> value into a <code>Categorical</code> value using a <code>TimestampTZ</code> format. <code>TimestampTZ</code> format is a categorical constant value among the available <code>TimestampTZ</code> formats (for example: <code>YYYY-MM-DD HH:MM:SSzzzzzz</code>).</p>"},{"location":"api-docs/kdic/timestamp-tz-rules/#astimestamptz","title":"AsTimestampTZ","text":"<pre><code>TimestampTZ AsTimestampTZ(Categorical timestampTZString, Categorical timestampFormat)\n</code></pre> <p>Recodes a categorical value into a <code>TimestampTZ</code> value using a <code>TimestampTZ</code> format.</p> <p>Example</p> <pre><code>AsTimestampTZ(\u201c2014-01-15 18:25:00+02:00\u201d, \u201cYYYY-MM-DD HH:MM:SSzzzzzz\u201d)\n</code></pre>"},{"location":"api-docs/kdic/timestamp-tz-rules/#utctimestamp","title":"UtcTimestamp","text":"<pre><code>Timestamp UtcTimestamp(TimestampTZ value)\n</code></pre> <p>Builds a <code>Timestamp</code> value from a <code>TimestampTZ</code> value after conversion to UTC time zone.</p> <p>Example</p> <p>Applying <code>UtcTimestamp</code> to <code>2020-03-21 12:15:30+02:00</code> returns the timestamp <code>2020-03-21   10:15:30</code>.</p>"},{"location":"api-docs/kdic/timestamp-tz-rules/#localtimestamp","title":"LocalTimestamp","text":"<pre><code>Timestamp LocalTimestamp(TimestampTZ value)\n</code></pre> <p>Builds a <code>Timestamp</code> value from a <code>TimestampTZ</code> value after conversion to local time zone.</p> <p>Example</p> <p>Applying <code>LocalTimestamp</code> to the <code>TimestampTZ</code> value  <code>2020-03-21 12:15:30+02:00</code> returns the <code>Timestamp</code> value <code>2020-03-21 12:15:30</code>.</p>"},{"location":"api-docs/kdic/timestamp-tz-rules/#settimezoneminutes","title":"SetTimeZoneMinutes","text":"<pre><code>TimestampTZ SetTimeZoneMinutes(Timestamp value, Numerical minutes)\n</code></pre> <p>Modify the time zone information of a timestampTZ value. The minutes must be between -12*60 and +14*60.</p> <p>Example</p> <p>Applying <code>SetTimestampMinutes</code> to the <code>TimestampTZ</code> <code>2020-03-21 12:15:30-03:00</code> with 120 for the <code>minutes</code> argument returns the <code>TimestampTZ</code> <code>2020-03-21 12:15:30+02:00</code>.</p>"},{"location":"api-docs/kdic/timestamp-tz-rules/#gettimezoneminutes","title":"GetTimeZoneMinutes","text":"<pre><code>Numerical GetTimeZoneMinutes(TimestampTZ value)\n</code></pre> <p>Returns the total minutes of the time zone (+- (hh * 60 + mm)) from a timestampTZ value.</p> <p>Example</p> <p>Applying <code>GetTimestampMinutes</code> to the timestampTZ <code>2020-03-21 12:15:30+02:00</code> returns 120.</p>"},{"location":"api-docs/kdic/timestamp-tz-rules/#difftimestamptz","title":"DiffTimestampTZ","text":"<pre><code>Numerical DiffTimestampTZ(TimestampTZ value1, TimestampTZ value2)\n</code></pre> <p>Difference in seconds between two timestampTZ values.</p>"},{"location":"api-docs/kdic/timestamp-tz-rules/#addsecondststz","title":"AddSecondsTSTZ","text":"<pre><code>Timestamp AddSecondsTSTZ(TimestampTZ value, Numerical secondNumber)\n</code></pre> <p>Adds a number of seconds to a timestampTZ value.</p>"},{"location":"api-docs/kdic/timestamp-tz-rules/#istimestamptzvalid","title":"IsTimestampTZValid","text":"<pre><code>Numerical IsTimestampTZValid(TimestampTZ value)\n</code></pre> <p>Returns 1 if a <code>TimestampTZ</code> value is valid.</p>"},{"location":"api-docs/kdic/timestamp-tz-rules/#buildtimestamptz","title":"BuildTimestampTZ","text":"<pre><code>TimestampTZ BuildTimestampTZ(Timestamp timestampValue, Time timeValue)\n</code></pre> <p>Builds a timestampTZ value from a timestamp value and time zone information in minutes. The minutes must be between <code>-12*60</code> and <code>+14*60</code>.</p>"},{"location":"api-docs/kdic/vector-rules/","title":"Vector Rules","text":""},{"location":"api-docs/kdic/vector-rules/#vector-rules","title":"Vector Rules","text":""},{"location":"api-docs/kdic/vector-rules/#vectorc","title":"VectorC","text":"<pre><code>Structure(VectorC) VectorC(Categorical value1, ...)\n</code></pre> <p>Builds a vector of <code>Categorical</code> values. The operands must be literal values (no variables or rules).</p>"},{"location":"api-docs/kdic/vector-rules/#tablevectorc","title":"TableVectorC","text":"<pre><code>Structure(VectorC) TableVectorC(Table table, Categorical value)\n</code></pre> <p>Builds a vector of <code>Categorical</code> values from values in a table.</p>"},{"location":"api-docs/kdic/vector-rules/#valueatc","title":"ValueAtC","text":"<pre><code>Categorical ValueAtC(Structure(VectorC) vector, Numerical index)\n</code></pre> <p>Returns a value of a <code>Categorical</code> vector at given index (index starts at 1). Returns <code>\"\"</code> if the index is out of bounds.</p>"},{"location":"api-docs/kdic/vector-rules/#vector","title":"Vector","text":"<pre><code>Structure(Vector) Vector(Numerical value1, ...)\n</code></pre> <p>Builds a vector of <code>Numerical</code> values. The operands must be literal values (no variables or rules).</p>"},{"location":"api-docs/kdic/vector-rules/#tablevector","title":"TableVector","text":"<pre><code>Structure(Vector) TableVector(Table table, Numerical value)\n</code></pre> <p>Builds a vector of <code>Numerical</code> values from values in a table.</p>"},{"location":"api-docs/kdic/vector-rules/#valueat","title":"ValueAt","text":"<pre><code>Numerical ValueAt(Structure(Vector) vector, Numerical index)\n</code></pre> <p>Returns the value of a <code>Numerical</code> vector at given index (indexes starts at 1). Returns a missing value if the index is out of bounds.</p>"},{"location":"learn/autofeature_engineering/","title":"Auto Feature Engineering","text":""},{"location":"learn/autofeature_engineering/#multi-table-feature-construction","title":"Multi-table Feature Construction","text":"<p>Auto Feature Engineering corresponds to the first (A) step of the Auto-ML pipeline implemented by Khiops. It is an optional pre-processing step to handle multi-table input data. Feature engineering is usually performed manually by data scientists, and is a time-consuming and risky task that can lead to overfitting. Khiops is able to automatically extract a large amount of aggregates from the secondary tables which are constructed and selected to avoid overfitting. These selected aggregates are all informative for the learning task at hand.</p> <p> </p> <p>In the rest of the pipeline, the aggregates are recoded in step (B), as well as the native variables. Then the encoding models (discretization and univariate grouping) are combined in step (C) in order to learn a parsimonious predictive model, by selecting relatively few variables.   </p> <p>Purpose</p> <p>This section presents the key ideas of Auto Feature Engineering. For an easier understanding,  it is highly recommended to first read the following sections: (i) An original formalism, (ii) Optimal Encoding. </p>"},{"location":"learn/autofeature_engineering/#definition","title":"Definition","text":"<p>The Auto Feature Engineering task, also known as propositionalization, consists in \"flattening\" multi-table data by summarizing the useful information contained in the secondary tables, through new columns inserted in the main table.</p> <p> </p> <p>Input:</p> <p>Multi-table data consists of: (i) a root table, where each row represents a training example, (ii) and several secondary tables that potentially contain several records for each example. For example, the customers of a telecommunications company can be described by a root table that contains native variables (name, first name, address, age etc.) and several secondary tables containing detailed information about each customer (contracts, communication details, use of services such as VOD etc). Notice that the target variable is contained in the root table. In our example, this target variable could describe whether a customer is churner or not.</p> <p>Output: </p> <p>The purpose of Auto Feature Engineering is to enrich the root table with informative aggregates, which are useful to predict the target variable. These aggregates are computed from the secondary tables. For example, the call rate to a foreign country could be a useful aggregate to predict customer churn. As a result, each row of the enriched root table describes a training example. This table can therefore be used by any learning algorithm.</p> <p>Key idea #1</p> <p>Auto Feature Engineering is a supervised learning task  that aims at finding the most useful aggregates to predict the target variable. </p>"},{"location":"learn/autofeature_engineering/#model-parameters","title":"Model Parameters","text":"<p>As before, the first modeling step is to define the family of models \\(\\mathcal{H}\\) which contains all the learnable hypotheses \\(h \\in \\mathcal{H}\\). In the case of Auto Feature Engineering, this family is an extension of the optimal encoding models which perform two tasks: (i) choosing the variable to encode, which can either be a native variable or an aggregate generated from the secondary tables; (ii) encoding this variable by a discretization model or a grouping model depending on its type.  </p> <p>Khiops uses a language similar to SQL to build aggregates. Formally, it is a collection of functions that can be composed with each other an unlimited number of times, provided that the operand and return types of each function are consistent. Here are the main functions used:    </p> Name Return type Operands Description \\(Count\\) Numerical Table Number of records in a table \\(CountDistinct\\) Numerical Table, Categorical Number of distinct values \\(Mode\\) Categorical Table, Categorical Most frequent value \\(Mean\\) Numerical Table, Numerical Mean value \\(StdDev\\) Numerical Table, Numerical Standard deviation \\(Median\\) Numerical Table, Numerical Median value \\(Min\\) Numerical Table, Numerical Min value \\(Max\\) Numerical Table, Numerical Max value \\(Sum\\) Numerical Table, Numerical Sum of values \\(Selection\\) Table Table, (Cat, Num) Selection from a table given a selection criterion \\(YearDay\\) Numerical Date Day in year \\(WeekDay\\) Numerical Date Day in week \\(DecimalTime\\) Numerical Time Decimal hour in day <p>In the remainder of this section, we consider a very simple example of multi-table data describing the customers of a company:</p> <p> </p> <p>The root table contains three columns: (i) the name of the client which is a native categorical variable, (ii) the age of the client which is a native numeric variable, (iii) and the client's identifier which serves as the join key. This key is not an explanatory variable used by the model; it simply identifies the rows of the secondary table corresponding to each client. This multi-table data contains a single secondary table describing the usages of the customers. This table consists of three columns: (i) the join key, (ii) the name of the product used, (iii) and the date of its use.</p> <p>Here are some examples of aggregates that can be generated from this multi-table data:</p> <ol> <li>\\(Mode\\)(Usages, Product)</li> <li>\\(Max\\)(Usages, \\(YearDay\\)(useDate))</li> <li>\\(Count\\)(\\(Selection\\)(Usages, \\(YearDay\\)(useDate) in [1;90] and Product=\"VOD\"))$</li> </ol> <p>Interpretation:</p> <ol> <li>The first aggregate corresponds to the most frequently used product for each customer,</li> <li>The second aggregate combines two functions and represents the day of the year of the last usage, </li> <li>The third aggregate is more complex: it corresponds to the number of VOD uses during the first quarter.  </li> </ol> <p>Similarly, the aggregates generated by Khiops are named by their formula and can be interpreted by the user.</p> <p>Key idea #2</p> <p>The choice of the functions that compose an aggregate and the order in which they are combined are additional parameters which enrich the discretization and grouping models.  </p> <p>As shown in the previous examples, the generated aggregates can be more or less complex, i.e. be composed of a varying number of functions. As the number of compositions is not limited, the aggregates can be arbitrarily complex, which poses a risk of overfitting. This risk is intrinsic to feature engineering, even when it is done manually by data scientists and business experts. For example, to predict customer churn, the following aggregate is certainly too complex to generalize its good performance on test data:</p> <ul> <li>\"the number of calls that a customer makes to Spain, on Friday nights between 10:31 and 10:38, having used the VOD service in the previous 40 minutes\".</li> </ul> <p>Key idea #3</p> <p>The cardinality of the family of models \\(\\mathcal{H}\\) is not bounded, and can potentially be infinite. There is therefore a risk of overfitting and the complexity of the generated aggregates must be regularized.</p>"},{"location":"learn/autofeature_engineering/#optimization-criterion","title":"Optimization Criterion","text":"<p>Information theory provides the same interpretation for both optimization criteria previously described for discretization and grouping models. Indeed, these two criteria can be rewritten as follows:</p> \\[-\\log(P(h).P(d|h)) = \\underbrace{L(h)}_{\\textbf{Prior}}  + \\underbrace{L(d_y | h, d_x)}_{\\textbf{Likelihood}} \\] <p>where the prior term \\(L(h)\\) refers to the coding length of the model, i.e. the number of bits needed to describe the model parameters. And the likelihood term \\(L(d_y | h, d_x)\\) is the coding length of the target variable of the training examples \\(d_y = \\{y_i\\}_{i \\in [1,n]}\\), knowing the model \\(h\\) and the explanatory variables of the entire training set \\(d_x = \\{x_i\\}_{i \\in [1,n]}\\). In other words, the encoding model (discretization or grouping) is considered as a compression of the information provided by the target variable of the training set.</p> <p>In the case of Auto Feature Engineering, the complexity of the aggregates is taken into account in the prior distribution, with a new term \\(L(h_a)\\), such as:</p> \\[-\\log(P(h).P(d|h)) = \\underbrace{ {\\color{red} L(h_a)} + L(h_e)}_{\\textbf{Prior}}  + \\underbrace{L(d_y | h_a, h_e, d_x)}_{\\textbf{Likelihood}} \\] <p>This term represents a construction cost that penalizes complex aggregates, in order to prevent overfitting. Notice that the model \\(h\\) is sliced in two, with \\(h_a\\) representing the constructed aggregate and \\(h_e\\) being the associated encoding model. The construction cost \\(L(h_a)\\) is defined recursively, following a hierarchical and uniform prior distribution:</p> \\[ {\\color{red} L(h_a)} = \\underbrace{\\vphantom{\\frac{1}{\\binom{N+I-1}{I-1}}} \\log(K+1)}_{\\substack{\\textbf{level 1:} \\\\ \\vphantom{a} \\\\ \\textrm{choice to generate} \\\\ \\textrm{an aggregate, or not.}}}  + \\underbrace{\\vphantom{\\frac{1}{\\binom{N+I-1}{I-1}}} \\log(R)}_{\\substack{\\textbf{level 2:} \\\\ \\vphantom{a} \\\\ \\textrm{choice of function $\\mathcal{R}$} \\\\ \\textrm{among $R$.}}}  + \\underbrace{\\sum \\limits_{op \\in \\mathcal{R}} {\\color{red} L(h_a)}}_{\\substack{\\textbf{level 3:} \\\\ \\vphantom{a} \\\\ \\textrm{choice of each operand} \\\\ \\textrm{of the function $\\mathcal{R}$.}}} \\] <p>The construction of an aggregate is made according to the following successive choices:</p> <ul> <li>level 1: generating an aggregate in addition to the \\(K\\) native variables of the root table is considered as an additional choice, so its probability is \\(\\frac{1}{K+1}\\);</li> <li>level 2: the choice of the function is random, the probability of using a particular rule \\(\\mathcal{R}\\) among \\(R\\) is thus of \\(\\frac{1}{R}\\);</li> <li>level 3: each operand of the selected function must be defined, they can be either a variable or the result of another function ...</li> </ul> <p>Notice that \\(L(h_a) = \\log(K)\\) if a native variable is selected instead of generating an aggregate. </p> <p>Key idea #4</p> <p>The prior distribution of aggregates has a particular form which prevents overfitting, i.e. it is recursive, hierarchical and uniform. Indeed, there are as many recursions in the prior as there are composed functions, which penalizes complex aggregates.  For a complex aggregate to be selected, the additional construction cost in the prior must be compensated by the likelihood, which measures how informative an aggregate is in predicting the target variable.</p>"},{"location":"learn/autofeature_engineering/#sampling-algorithm","title":"Sampling Algorithm","text":"<p>Two different algorithms are successively executed to implement Auto Feature Engineering: </p> <ol> <li>A sampling algorithm which randomly generates aggregates from the prior distribution described above;</li> <li>An optimization algorithm which trains the encoding models (discretization or grouping) for each candidate aggregate, and selects only informative ones. </li> </ol> <p>This section describes only (1) the sampling algorithm, as (2) the optimization algorithms for discretization and grouping models are described in the previous section.</p> <p>Algorithmic Challenge</p> <ul> <li>How to design an efficient sampling algorithm?</li> <li>How to randomly draw aggregates from a family \\(\\mathcal{H}\\) of infinite size, while the hardware resources (RAM and CPU) are limited? </li> </ul> <p>To answer this algorithmic challenge, several heuristics are used:</p> <ol> <li> <p>The set of aggregates can be represented in a compact way as a tree, where the branches factorize the successive choices defining the aggregates (i.e. choice of functions to compose, choice of operands). Although compact, this tree has a potentially infinite depth since the number of function compositions is not bounded. </p> </li> <li> <p>Due to limited hardware resources, this tree can only be explored partially. The trick is to use a finite number of tokens that move through the tree and which are distributed on the branches according to the a priori distribution. The tree is thus partially built in RAM, considering only the branches in which tokens circulate.   </p> </li> </ol> <p>The animation above shows the sampling algorithm in a very simple case, where the aggregate construction language is limited to three functions (Mode,Min,Max), and where the multi-table data considered is the same as in the previous example. Here is a description of the successive steps:</p> <ul> <li>Among the \\(100\\) considered tokens, \\(34\\) correspond to the choice to generate an aggregate in addition to the two native variables of the root table.  </li> <li>These \\(34\\) tokens are uniformly distributed over the \\(3\\) branches corresponding to the choice of the function to use.</li> <li>Then, there is only one possible choice of the 1st operand of these functions, which is the unique secondary table \"Usages\" of the multi-table data. </li> <li>Finally, the last step shows that some branches end (e.g. the function Mode necessarily takes as 2nd operand the categorical variable \"Product\") and other branches continue to grow through the composition of functions (e.g. the function Min is composed with the function YearDay). </li> </ul> <p>The user specifies a number of candidate aggregates one wants to generate, and the algorithm is repeated several times by increasing the number of tokens, until the wanted number of distinct aggregates is generated. For more details on this sampling algorithm, the interested reader can refer to the scientific publications.</p>"},{"location":"learn/autofeature_engineering/#technical-benefits","title":"Technical Benefits","text":"<p>Use in any Machine Learning pipeline </p> <p>Only part of the Khiops Auto ML pipeline can be reused. In particular, the Auto Feature Engineering step can be integrated into any Machine Learning pipeline. This saves a lot of time in data science projects by automatically generating a large number of informative aggregates. Then, any Machine Learning model can be trained from the root table enriched by the aggregates.  </p> <p>Improve business knowledge </p> <p>The aggregates produced by Khiops are labeled with their mathematical formula, which allows us to interpret them a posteriori. In practice, the analysis of the most informative aggregates allows us to better understand the data and to easily identify the useful information for the learning task at hand.</p>"},{"location":"learn/autofeature_engineering/#decisions-trees","title":"Decisions Trees","text":"<p>Decision Tree Feature Engineering corresponds to the last (A') step of the Auto-ML pipeline implemented by Khiops. It is an optional pre-processing step to build decisions trees from native variables and aggregates. Khiops is able to automatically build parameter free decisions trees from native variables and informative aggregates. </p> <p></p>"},{"location":"learn/autofeature_engineering/#definition_1","title":"Definition","text":"<p>A decision tree is a classification model which aims at predicting a categorical class variable from a set of numerical or categorical input variables. One advantage of decision trees is that they provide understandable models, based on decision rules. Khiops exploit a parameter-free Bayesian approach to build decision trees. This approach consist on an analytic formula for the evaluation of the posterior probability of a decision tree given the data. We thus transform the problem into an sampling problem in the space of decision tree models. Khiops build random informative Trees.</p> <p>Input:</p> <p>Flat-table data consists of: natives variables and aggregates.</p> <p>Output: </p> <p>Informative decision tree</p>"},{"location":"learn/autofeature_engineering/#parameters-of-the-model","title":"Parameters of the Model","text":"<p>A MODL binary decision tree model is defined by its structure, the distribution of the instances in this structure and the distribution of output values in the leaves. The structure of the decision tree model consists of the set of internal nodes (not leave node), the set of leaves and the relations between these nodes. The distribution of the instances in this structure is defined by the partition of the segmentation variable in each internal node and by the distribution of the classes in each leaf. A decision tree model T is thus defined by: </p> <ul> <li>the subset of variables \\(\\mathcal{K_T}\\) used by model T , that is the number of selected variables \\(K_T\\) and the choice of the \\(K_T\\) variables among input set of \\(K\\) variables,</li> <li>the number of child nodes \\(I_s\\),</li> <li>if Is = 0 then the node is an internal node,</li> <li>if Is = 1 then the node is a leaf,</li> <li>the distribution of the instances in each internal node s:</li> <li>the segmentation variable \\(X_s\\),</li> <li>the distribution of the instances on this parts (child nodes) \\(\\{ N_{s1.},N_{s2.}\\}\\)</li> <li>the distribution of the classes in each leaf l: \\(\\{N_{l.j} \\}_{1\u2264j\u2264J}\\)</li> </ul>"},{"location":"learn/autofeature_engineering/#optimization-criterion_1","title":"Optimization Criterion","text":"<p>Key idea #1</p> <p>The MODL decision tree criterion simply result from the expansion of the Bayes' formula. </p> <p>As MODL discretization the objective of the optimization criterion is to select the most probable model given the training data, denoted \\(T\\), by maximizing the probability \\(P(T|d)=P(T).P(d|T)/P(d)\\). The optimization criterion used to select the most probable supervised decision tree model can easily be interpreted: </p> \\[-\\log(P(T).P(d|T)) = \\overbrace{\\underbrace{ \\vphantom{\\sum\\limits_{k=1}^K}  \\log(K+1) +\\log \\binom{K+K_T-1}{K_T-1} }_{\\textbf{level 1}} + \\underbrace{ \\sum_{I_s=0,X_s Num. } \\log (2K_T (N_{s.}+1))}_{\\textbf{level 2}}+\\underbrace{ \\sum_{I_s=1, X_s Cat.} \\log (2K_TB(G_{X_s},2))}_{\\textbf{level 3}} + \\underbrace{\\sum\\limits_{l} \\log 2+\\log \\binom{N_l + J -1}{J-1}}_{\\textbf{level 4}}}^{\\textbf{Prior}}  + \\overbrace{\\sum\\limits_{l} \\log \\frac{N_l!}{N_{l1}!N_{l2}! \\dots N_{lJ}!}}^{\\textbf{Likelihood}} \\] <p>The first four terms correspond to the prior model distribution \\(-\\log(P(T))\\), which is hierarchical and uniform at each level of this hierarchy. Here is the meaning of each term:</p> <ul> <li>level 1: probability of \\(K_T\\) and probability to select \\(K_T\\) variables. All the values \\(K_T \\in [0, K]\\) being equiprobable with a probability equals to \\(1/(K+1)\\), with \\(\\binom{K + K_T-1}{K_T-1}\\) the number of subset of \\(K_T\\) variables; </li> <li>level 2: probability of node \\(s\\) to split into two nodes according \\(X_s\\) a numerical variable;</li> <li>level 3: probability of node \\(s\\) to split into two nodes according \\(X_s\\) a categorical variable, \\(B(G_{X_s},2)\\) is the number of divisions of the \\(G_{X_s}\\) values into 2 groups;</li> <li>level 4: given the previous parameters of split, probability of observing particular sub-effects by class and by intervals in leave node \\(l\\), with \\(\\binom{N_l + J -1}{J-1}\\) the number of possible sub-effects.   </li> </ul> <p>The last term represents the Likelihood \\(-\\log(P(d|T))\\) which enumerates the training sets that can be generated such that they are correctly described by the model. It is calculated by counting the permutations of the training examples within each leave (numerator) and the permutations of the labels (denominator). Note that the groups of examples located in each leave are considered to be independent (i.e. the sum corresponds to a probability product), which is the case when the training examples are i.i.d (independent and identically distributed), as always in Machine Learning.</p>"},{"location":"learn/autofeature_engineering/#random-trees-generation","title":"Random Trees Generation","text":"<p>The purpose of this section is to introduce the intuitions of the Random trees generation, for more details, you can refer to scientific publications.</p> <p>Khiops doesn't search to build best tree according to the decision tree criterion. The induction of an optimal decision tree from a data set is NP-hard. The exhaustive search algorithm is then excluded. We use an approximate criterion to generate random informative decision trees. To do this, we exploit a pre-pruning algorithm on a random subsample of \\(K_T\\) variables (\\(K_T=\\sqrt K\\)). The pre-pruning algorithm starts with the root node and searches for all the partitions from the \\(K_T\\) variables that improve the criterion. We choose randomly a partition and the leaf is split. For each leaf, the partition is performed according to the univariate MODL discretization or grouping methods, then the global cost of the tree is updated by accounting for this new partition. The algorithm continues to split the leaves until there are no more partitions that improve the tree criterion.</p>"},{"location":"learn/hardware_adaptation/","title":"Auto Hardware Adaptation","text":"<p>This section presents the strategy implemented in Khiops for adapting the algorithms to the available hardware resources.  Please note that this feature requires the use of Khiops dictionaries.</p>"},{"location":"learn/hardware_adaptation/#general-principles","title":"General principles","text":"<p>Key idea #1: Divide and conquer</p> <p>Khiops divides the data into a more or less fine-grained grid of files, depending on the learning task at hand and the available hardware resources. </p> <p>As shown in the following figure, the successive steps of the Auto ML pipeline are algorithms which process either rows or columns of the root table. For example, optimal encoding is a column-based algorithm, since each discretization or grouping model can be learned independently from each variable (or aggregate). On the other hand, once the pipeline is executed, making predictions is a row-based algorithm, since each example can be scored independently.     </p> <p>The goal is to ensure the proper execution of these algorithms, whatever</p> <ul> <li>the size of the processed data; </li> <li>the amount of available hardware resources.</li> </ul> <p>To do this, Khiops slices the data into a grid of files, each file representing a cell of the grid. Then, these files are grouped into slices, either horizontally for row-based algorithms, or vertically for column-based algorithms.</p> <p>Key idea #2</p> <p>A smart strategy adapts the grid size to each situation. </p> <p>As shown in the following figure, a finer division of the file grid can be decided for two reasons:</p> <ul> <li>either due to a lack of RAM, with the goal of loading slices in turn into memory and processing them sequentially. </li> <li>or due to the availability of numerous CPUs, in order to distribute the processing over the cores of a single computer, or over the workers of a computer cluster (using Message Passing Interface (MPI)).</li> </ul> <p>In practice, this strategy is coded in the following way. At each step of the Auto ML pipeline, the successive algorithms collect more and more precise statistics about the data, and the learning task at hand (the size of the encoding models, the selected variables ...). This information allows each algorithm to estimate its own needs in hardware resources (CPU and RAM), in the worst case and in the best case. The adaptation strategy maps the available resources to the needs of each algorithm and adapts the size of the file grid to each situation. For example, in a non-intuitive way, one should not use too many CPUs to process a small dataset, it is not optimal in terms of computation time. Thus, you are guaranteed that the Auto ML pipeline completes its execution correctly, whatever the hardware resources. </p>"},{"location":"learn/hardware_adaptation/#lets-run-an-experiment","title":"Let's run an experiment!","text":"<p>For this experiment, we use the Zeta dataset from the Large Scale Learning Challenge , which contains \\(500,000\\) training examples and \\(2000\\) numerical explicative variables. This is a binary classification problem. This data file takes up 9.3 GB on the hard disk, and this run was carried out on a Intel Xeon Gold 6150 CPU 2.70 Ghz. The experiment consists in training a classifier (steps B + C of the pipeline) and evaluating it, by varying the number of cores and the amount of RAM available. 70% of the examples are used for training and 30% for testing.</p> <p>The following figure plots the execution time in minutes, as the number of cores and the amount of RAM increase together. This figure shows that there is a smooth transition from out-of-core to distributed computing, demonstrating the efficiency of the adaptation strategy to the available hardware resources. This is made possible by thorough I/O optimization. Finally, you won't be penalized significantly if your hardware is undersized for the task at hand. Probably not enough to justify a new investment \ud83d\ude09.   </p> <p> </p> <p>This second more detailed figure shows execution time as a function of the number of cores (horizontal axis) and the amount of RAM available (vertical axis). Out-of-core calculations are represented by the orange rectangle, and once again the transition is smooth on both axes. The red rectangle represents extreme situations where there isn't enough RAM for processing. In this case, Khiops gives an indication of the minimal amount of RAM required to run the process, and above all does not run a long process that will not be completed. </p> <p> </p>"},{"location":"learn/hardware_adaptation/#conclusion","title":"Conclusion","text":"<p>Finally, the computation time is the adjustment variable guaranteeing the proper execution of the algorithms in all situations. In practice, this adaptation strategy allows significant savings: (i) by avoiding investment in additional hardware resources, (ii) by saving engineering time aimed at adapting the code and scaling the training data to the available hardware resources. </p>"},{"location":"learn/learning_models/","title":"Parsimonious Training","text":"<p>Parsimonious training is the last step (C) of the Auto ML pipeline implemented by Khiops, and consists in selecting the best subset of explicative variables (or aggregates) and weighting them within a Bayesian model:  </p> <p> </p> <p>As an input of this step, the learning algorithm is provided with the encoding models obtained in the previous step (B) for native variables and aggregates making up the root table. The goal of this step is to combine these encodings (discretization or grouping models) in order to build a multivariate predictive model.   </p> <p>Purpose</p> <p>This section presents the predictive models trained by Khiops in the case of classification. Equivalent models exist for regression problems, for which the interested reader can refer to the published scientific papers. Understanding the MODL approach is a prerequisite, so it is recommended to read the previous sections first.  </p>"},{"location":"learn/learning_models/#from-naive-to-non-naive-bayes","title":"From Naive to Non-naive Bayes","text":"<p>Khiops provides very competitive models which are both accurate and parsimonious, i.e. selecting very few variables (or aggregates). And yet, these models are derived from the Na\u00efve Bayes classifier which is not exactly recognized for its performance. This section focuses on the important improvements made by Khiops to these models, which are far from being naive \ud83d\ude09.</p> <p>The big picture</p> <p>The main idea is illustrated in the following figure, and consists in varying the size of the data representation during the pipeline. Indeed, the Auto Feature Engineering step explores a large number of aggregates which enrich the initial data representation, but which contain redundant information. Next, the parsimonious training step reduces the data representation by selecting a small number of informative and complementary variables. This last step improves model interpretability, as the contributions of selected variables are almost additive since their interactions are reduced to a minimum.</p> <p> </p>"},{"location":"learn/learning_models/#background","title":"Background","text":"<p>First of all, the expression \"Na\u00efve\" refers to an hypothesis made about the explicative variables. They are supposed to be independent from each other given the target, which is very rarely the case in real life, hence this pejorative nickname. This assumption is materialized by the red term in the following equation, which shows how is estimated the probability \\(p(y_j | \\{x_k\\}_{k \\in [1, K]})\\) that a new example \\(\\{x_k\\}_{k \\in [1, K]}\\) belongs to a particular class \\(y_j\\):</p> \\[p(y_j | \\{x_k\\}_{k \\in [1, K]}) = \\frac{p(y_j) {\\color{red}\\prod}^K_{k=1} p(x_k|y_j)}{p(\\{x_k\\}_{k \\in [1, K]})} = \\frac{p(y_j) \\prod^K_{k=1} p(x_k|y_j)}{\\color{blue} \\sum^J_{j=1} p(y_j) \\prod^K_{k=1} p(x_k|y_j) } \\] <p>Indeed, this product is performed on probabilities relative to each explicative variable, which relies on the independence hypothesis. Now, let's take a look at the other important terms:</p> <ul> <li>The term \\(p(y_j)\\) is the prior probability of the class \\(y_j\\). In practice, this term can easily be estimated from the training set, by counting the examples of each class. Here is an   illustration:  </li> </ul> <p> </p> <ul> <li>The term \\(p(x_k|y_j)\\) represents the probability of observing a particular value \\(x_k\\) of the K-th explicative variable, assuming the class to predict is \\(y_j\\). In practice, this term must be estimated by \\(K\\) univariate models which can take different forms. The following figure gives two examples. The left part gives the example of parametric distributions (e.g. Gaussian) each fitted on the examples of a given class. The right part shows how discretizations can be used to estimate this term in a frequentist way, by counting the examples of each class within each interval.  </li> </ul> <p> </p> <ul> <li>The term \\(p(\\{x_k\\}_{k \\in [1, K]})\\) represents the probability of observing the example at hand, and is not related to the target variable. To estimate calibrated conditional probabilities, this term can be replaced by the normalization factor represented in blue in the above equation.   </li> </ul> <p>Weakness of standard Na\u00efve Bayes</p> <p>When implemented in an basic way, the Na\u00efve Bayes classifier does not perform very well as the assumption of independence between variables is often not valid in real-life situations. The following weakness needs to be mitigated: </p> <ol> <li>the term \\(p(x_k|y_j)\\) is generally not estimated with enough care; </li> <li>the dependencies between explicative variables cannot be modeled;</li> <li>explicative variables can provide redundant information, which is very bad for performance.</li> </ol>"},{"location":"learn/learning_models/#khiops-brings-major-improvements","title":"Khiops Brings Major Improvements","text":"<p>Each of these shortcomings is addressed:</p> <p>#1 : Optimal Encoding</p> <p>The encoding models (discretization and grouping) are non-parametric estimators, i.e. they can estimate any conditional distribution as long as the number of training examples is sufficient. These models are used to accurately estimate the term \\(p(x_k|y_j)\\) for each explicative variable.</p> <p>#2 : Modeling dependencies through aggregates</p> <p>The Auto Feature Engineering step can produce aggregates which model the dependencies between explicative variables. This is notably the case of the \\(Selection\\) function, which is able to delimit areas in the input space by using conditions on several variables. For native variables, it is also possible to discretize pairs of variables, and also to train decision trees based on the MODL approach (see publications). </p> <p>#3 : Fighting information redundancy</p> <p>Now, we come to the most important improvement which is developed in the rest of this section. As illustrated by the big picture above, a very large number of aggregates can be generated from the Auto Feature Engineering step, which dramatically increases the dimension of the input space. In practice, the construction of the order of \\(100,000\\) aggregates does not represent a scaling problem. These numerous aggregates are generated independently of each other, exploring the space of aggregates which are informative for predicting the target variable, but without taking any precautions to ensure their independence. </p> <p>Learning Task of Parsimonious Training</p> <p>The Parsimonious Training step consists in selecting the subset of variables (native or aggregates) that are both (i) as informative as possible, (ii) and as independent as possible. More specifically, the selected variables are weighted to reduce the effect of their remaining dependencies on the model's performance.        </p>"},{"location":"learn/learning_models/#model-parameters","title":"Model Parameters","text":"<p>Khiops implements a robust and well-performing model, called \"Selective Non-naive Bayes\" (SNB), which takes advantage of the improvements described above (1,2,3) and optimizes variable selection and weighting (4) in the following way: </p> \\[p(y_j | \\{x_k\\}_{k \\in [1, K]}) = \\frac{p(y_j) \\prod^K_{k=1} p(x_k|y_j)^{\\color{red}w_k}}{\\sum^J_{j=1} p(y_j) \\prod^K_{k=1} p(x_k|y_j)^{\\color{red}w_k}}\\] <p>where \\(\\{w_k\\}_{k \\in [1,K]}\\) is a vector of weights associated to the \\(K\\) explicative variables (native or aggregates). These weights are floating-point values such that \\(w_k \\in [0,1]\\), and a large majority of them are null \\(w_k = 0\\) corresponding to the non-selected variables. This weight vector constitutes the SNB parameters to be optimized during the training process.</p>"},{"location":"learn/learning_models/#optimization-criterion","title":"Optimization Criterion","text":"<p>As in the other steps of the Auto ML pipeline, the optimization criterion used to learn the SNB model is based on the MODL approach. A variable weighting model \\(h\\) is entirely defined by the weight vector \\(\\{w_k\\}_{k \\in [1,K]}\\). And the most probable model given the training set \\(d\\) is obtained by minimizing the following criterion:</p> \\[-\\log(p(h).p(d|h)) = \\gamma \\times  \\overbrace{ \\left( \\underbrace{ \\vphantom{\\sum\\limits_{k=1}^K} - \\log \\left(\\frac{1}{p^*(K_s)}\\right)  }_{\\textbf{level 1}} - \\underbrace{ \\vphantom{\\sum\\limits_{k=1}^K} \\log \\left ( K_s! \\prod_{k=1}^{K_s} (p_k)^{w_k}  \\right )}_{\\textbf{level 2}} \\right)   }^{\\textbf{Prior}}  - \\overbrace{  \\vphantom{\\left( \\underbrace{ \\vphantom{\\sum\\limits_{k=1}^K}}_{\\textbf{level 1}} - \\underbrace{ \\vphantom{\\sum\\limits_{k=1}^K} \\log \\left ( K_s! \\prod_{k=1}^{K_s} p(w_k \\neq 0) \\right )}_{\\textbf{level 2}} \\right)}  \\sum\\limits_{n=1}^N \\log p(y^{(n)} |  \\{x^{(n)}_k\\}_{k \\in [1, K]}) }^{\\textbf{Likelihood}}\\] <p>The first two terms correspond to the prior model probability \\(-\\log(p(h))\\), whose distribution is as previously hierarchical and uniform at each level of this hierarchy. Here is the meaning of each term:</p> <ul> <li>level 1: probability to select a certain number of variables denoted \\(K_s\\), i.e. the number of non-null weights in the vector \\(\\{w_k\\}_{k \\in [1,K]}\\). Due to the previous Auto Feature Engineering step, the number of candidate variables (or aggregates) is potentially infinite. Thus, this term represents the probability of choosing \\(K_s\\) variables from an infinite number. The term \\(p^*(K_s)\\) is given by the universal Rissanen prior which is the most uniform possible prior on the set of integers. Without going into details, the following figure shows that low values of \\(K_s\\) are more probable than high values, and the rate of decay is taken to be small as possible.  </li> </ul> <p> </p> <p>Key idea #1</p> <p>The MODL approach is pursued, as the use of the Rissanen prior allows to get as close as possible to a uniform distribution, in order to build a hierarchical and uniform prior distribution. </p> <ul> <li>level 2: probability of selecting a particular subset of \\(K_s\\) variables among all possible subsets of the same size, which is derived from a multinomial distribution modeling the selection (or not) of each variable. The term \\(p_k\\) represents the prior probability of the \\(k\\)-th selected variable, and is given by the previous Auto Feature Engineering step. Indeed, this term is defined by \\(-\\log(p_k)=L(h_a^k)+L(h_e^k)\\), representing the product of the prior probabilities of the \\(k\\)-th aggregation model \\(h_a^k\\) and the \\(k\\)-th encoding model \\(h_e^k\\).</li> </ul> <p>Key idea #2</p> <p>Since the construction cost \\(L(h_a^k)\\) is taken into account in this prior distribution, the simple aggregates are preferred to complex ones for equivalent likelihoods.</p> <p>The last term is the negative log-likelihood which measures the quality of the model \\(h\\) in estimating the target variable of the training data \\(d\\). It is the sum of the terms \\(- \\log p(y^{(n)} | \\{x^{(n)}_k\\}_{k \\in [1, K]})\\), where \\(y^{(n)}\\) represents the target value of the \\(n\\)-th training example and \\(\\{x^{(n)}_k\\}\\) its explicative variable values. Finally, the negative log-likelihood evaluates the average quality of the output probabilities estimated by the classifier, since the lower the probability of the true labels, the higher the penalty.</p> <p>The regularization parameter \\(\\gamma\\) weighs the relative importance of the prior and likelihood terms, so it has an influence on both the parsimony of the trained models and overfitting. Indeed, if \\(\\gamma = 0\\) the optimization criterion is not regularized anymore, which leads to the selection of a large number of variables and to overfitting issues. On the other hand, the choice \\(\\gamma = 1\\) corresponds to a full regularization which makes sense only when the explicative variables are independent. Finally, the use of \\(\\gamma&lt;1\\) is equivalent to relaxing the na\u00efve hypothesis by allowing the selection of additional variables which are not totally independent of the others, and which bring complementary and useful information to improve the model accuracy. </p> <p>Key idea #3</p> <p>Khiops does not involve any hyper-parameter, the parameter \\(\\gamma\\) has been adjusted using a large collection of datasets. It is set to a fixed sweet spot value which considerably limits the risks of overfitting in practice and provides parsimonious and accurate models. In addition, the adjustment of this parameter is not very sensitive and does not produce significant variation in accuracy from one data set to another.</p>"},{"location":"learn/learning_models/#optimization-algorithm","title":"Optimization Algorithm","text":"<p>Algorithmic Challenge</p> <p>This optimizing problem is very hard, since \\(\\{w_k\\}_{k \\in [1,K]}\\) is a vector of floating-point values, for which there are an infinite number of candidate solutions.</p> <p>The optimization algorithm used is based on two key principles: (i) for reasons of efficiency, the set of solutions is explored in a discrete way; (ii) in order to prioritize important interactions between variables, the grain level with which the floating-point weights are updated becomes finer and finer. </p> <p>The following figure illustrates the main steps of this algorithm:</p> <ul> <li>Step 1 initializes the weight vector \\(\\{w_k\\}\\) with zero values, the weight increment is set to \\(1\\) and the set of selected variables is initialized to the empty set. It is important to keep in mind that the set of selected variables is a working variable, which changes state during the algorithm, and which is used to update the weight vector.</li> <li>Step 2 is inspired by the Forward/Backward variable selection algorithm, which attempts to add or remove variables in random order, with the aim of improving the optimization criterion. Then, added variables are used to increment the  weight \\(w_k\\) of the corresponding variables, and similarly deleted variables are used to decrement the weights.</li> <li>Step 3 controls the complexity of the algorithm by repeating the previous step a certain number of times, calculated according to the number of available variables and the size of the dataset.</li> <li>Step 4 refines the weight vector by repeating the previous steps with decreasing weight increments. This step allows to progressively select variables whose contribution and interaction with the other variables become less and less important.</li> </ul>"},{"location":"learn/learning_models/#technical-benefits","title":"Technical Benefits","text":"<p>Model Interpretability</p> <p>The parsimony of the SNB model, which selects a small subset of variables (native or aggregates), combined with the fact that the aggregates generated in the previous Auto Feature Engineering step have explicit names, make the models produced by Khiops very easy to interpret. A visualization tool is provided for this purpose, making it possible to understand and visualize the entire Auto ML pipeline, from optimal encoding to model evaluation.    </p> <p>Fast Predict</p> <p>The parsimony of the SNB model also represents a major time saver when the model is deployed on large amounts of data. Selecting only a few variables means speed! </p>"},{"location":"learn/modl/","title":"An Original Formalism","text":"<p>The MODL approach is an essential component that gives Khiops three significant features: a natural resistance to overfitting, no need to optimize hyperparameters and impressive scalability. However, what is the MODL approach? How does it distinguish itself from standard machine learning approaches?</p> <p>This section introduces and compares MODL with conventional approaches. Exploring this scientific framework is a crucial point, as it fundamentally influences the functioning of Khiops. A deeper understanding of this original formalism will highlight the unique features that set Khiops apart in the machine learning landscape.</p>"},{"location":"learn/modl/#standard-machine-learning-approaches","title":"Standard Machine Learning Approaches","text":"<p>Most machine learning approaches are derived from statistical learning theory. This section shows how these approaches rely on optimization of hyperparameters to limit overfitting, and how Khiops contrasts with them. Indeed, Khiops is freed from the complexity of hyperparameter optimization, which offers a significant advantage over other machine learning approaches.</p>"},{"location":"learn/modl/#regularization","title":"A Closer Look at Empirical Risk Minimization","text":"<p>In the field of machine learning, the goal of learning algorithms is to identify the optimal model, represented as \\(\\hat{h}\\), from a set of possible models, denoted as \\(\\mathcal{H}\\). This identification relies on a sample of training examples, denoted by \\(\\{(x_i,y_i)\\}_{i \\in [1,n]}\\), with each example associating input variables \\(x\\) with their corresponding ground truths \\(y\\) to be predicted.</p> <p>For any given training example, a loss function, \\(\\mathcal{L}(h(x_i),y_i)\\) (or cost function), is used to evaluate how the predicted output of a model \\(h(x_i)\\) compares with the actual ground truth \\(y_i\\), i.e., the expected value. The loss function thus evaluates the relevance (or the cost) of a candidate model \\(h\\), for the example \\((x_i,y_i)\\). A popular choice of a loss function is the squared error, defined as \\((y_i - h(x_i))^2\\).</p> <p>The average value of the loss function given all the training examples \\(\\{(x_i,y_i)\\}_{i \\in [1,n]}\\) is the empirical risk. Under the assumption of independence and equal distribution among the training examples, this empirical risk provides an approximation of the actual risk the model will face when dealing with new data, corresponding to the mathematical expectation. For instance, using the squared error loss function, the associated empirical error is defined as \\(\\frac{1}{n} \\sum^n_{i=1} (y_i - h(x_i))^2\\). This is known as the method of the Least squares.</p> <p>The concept of empirical risk might suggest that a model that correctly explains the training examples and minimizes the empirical risk would be an ideal one. However, an exclusive focus on minimizing the empirical risk can lead to overfitting, where the model doesn't generalize well to new data. To address this, most machine learning approaches use a regularization term, \\(Reg(h)\\), as part of the optimization criterion:</p> \\[\\hat{h} = \\arg \\min_{h \\in \\mathcal{H}} \\left [ \\underbrace{\\frac{1}{n} \\sum^n_{i=1} \\mathcal{L}(h(x_i),y_i)}_\\textrm{Empirical risk}  + \\gamma . \\underbrace{ \\vphantom{\\sum^n_{i=1}} Reg(h)}_\\textrm{Regularization} \\right ]\\] <p>The regularization function, \\(Reg(h)\\), can take various forms. However, its purpose remains constant: to impose a penalty on excessively complex models to prevent overfitting.</p>"},{"location":"learn/modl/#the-fundamental-role-of-hyperparameters-in-scalability","title":"The Fundamental Role of Hyperparameters in Scalability","text":"<p>Looking at the equation presented above, it is clear that the empirical risk and regularization terms don't share the same mathematical nature. That's why the regularization strength \\(\\gamma\\) is introduced to balance the training objective between empirical risk minimization and regularization. However, the learning algorithm on its own cannot determine the best value of \\(\\gamma\\) as it comes under the umbrella of model selection (see Hyperparameter).</p> <p>This issue necessitates a trial-and-error process, which involves validating possible values of \\(\\gamma\\) and assessing the candidate model \\(h\\) using a dedicated validation set of examples. This process is typically repeated multiple times for robustness against sampling variation, a method known as cross-validation.</p> <p>In real-world applications, standard machine learning methods don't have just one, but numerous hyperparameters, the optimal values of which can't be figured out by the learning algorithm itself. These hyperparameters may include the depth and minimum number of examples in the leaves for decision trees, or the number of layers, the size of layers, and the transfer functions for neural networks.</p> <p>To discover the best set of hyperparameters, it's necessary to test all possible combinations using a method called grid-search. However, the number of hyperparameters directly impacts scalability since it determines how often the learning algorithm needs to be executed. For instance, if a grid search tests five candidate values for each hyperparameter over a ten-fold cross-validation process, the training algorithm will need to be repeated:</p> <p> </p> <ul> <li>250 times for 2 hyperparameters,</li> <li>1250 times for 3 hyperparameters,</li> <li>6250 times for 4 hyperparameters,</li> <li>31250 times for 5 hyperparameters, and so on.</li> </ul> <p>Standard approaches are limited in their scaling by the empirical optimization of hyperparameters, which remains necessary to fight against overfitting.</p>"},{"location":"learn/modl/#khiops","title":"Khiops","text":"<p>Khiops operates on a different paradigm from conventional machine learning approaches, which focus primarily on minimizing empirical risk. It uses the MODL approach, which is based on a Bayesian model selection principle. This approach gives Khiops an advantage, freeing it from the constraints of optimizing hyperparameters and enhancing its robustness against overfitting. The following discussion clarifies the underlying principles of the MODL approach, drawing attention to its inner workings and the benefits it brings to machine learning.</p>"},{"location":"learn/modl/#bayes","title":"A Bayesian Model Selection Approach","text":"<p>The MODL approach aims at selecting the most probable model given the training data. The Bayes formula is thus the starting point for deriving the optimization criteria used, whose general form is the following:</p> \\[\\arg \\max_{h \\in \\mathcal{H}} P(h|d) = \\arg \\max_{h \\in \\mathcal{H}} \\frac{P(h)P(d|h)}{P(d)}\\] <p>where \\(h\\) represents the model at hand which belongs to \\(\\mathcal{H}\\), the set of learnable hypotheses, and \\(d\\) is the training set. </p> <p>All the MODL optimization criteria are designed in the same way (optimal encoding, feature engineering and parsimonious training), through the following steps: </p> <ul> <li> <p>definition of the family of models \\(\\mathcal{H}\\), i.e. the model parameters, with respect to the learning task to be addressed;</p> </li> <li> <p>definition of the prior distribution \\(P(h)\\) over these parameters, which is always hierarchical and uniform;</p> </li> <li> <p>obtaining the optimization criterion from the development of the Bayes' formula by accounting for the likelihood term \\(P(d|h)\\);</p> </li> <li> <p>model training by optimizing the final criterion just once.</p> </li> </ul>"},{"location":"learn/modl/#info-theory","title":"Link with Information Theory","text":"<p>In Information Theory, the model selection problem described above can be translated into an encoding problem, whose purpose is to find the most compact way of encoding an information source for transmission over a telecommunication channel. Let's consider an information source emitting symbols [e.g. a, b, c, etc.] whose alphabet is known. In information theory, the negative logarithm of the probability that a symbol is emitted [e.g. \\(-log(P(a))\\)] represents its optimal coding length, denoted by \\(L\\) and expressed in bits. According to Shannon's intuition, the most efficient encoding strategy assigns a short coding length to the most frequent symbols.</p> <p>In the same way, the probabilities in Bayes' formula above can be replaced by negative logarithms to obtain a MODL criterion to be minimized, that can be interpreted as follows:</p> \\[-\\log(P(h).P(d|h)) = \\underbrace{L(h)}_{\\textbf{Prior}}  + \\underbrace{L(d|h)}_{\\textbf{Likelihood}} \\] <ul> <li>the prior corresponds to the coding length of the model, i.e. the number of bits needed to describe it; </li> <li>the likelihood is the coding length of the training data, with the model in hand.</li> </ul> <p>In this encoding problem, the model is transmitted over the telecommunication channel first, followed by the training data. The Minimum Description Length (MDL) principle aims to select the most compact model describing the data, and it is applied in the MODL approach by the choice of a hierarchical prior representing successive choices on the model parameters.</p>"},{"location":"learn/modl/#lets-dive-deeper-with-the-analogy-of-density-estimation","title":"Let's Dive Deeper with the Analogy of Density Estimation","text":"<p>To illustrate the model selection performed by Khiops, we draw an analogy with a very simple problem: estimating the density of a numerical variable using histograms. Here, the histogram takes on the role of the model under training. This is a discretization model (i.e. dividing the domain of the variable under study into intervals of varying size) that is instantiated by first defining the number of intervals, then the frequency and bounds of each interval. For the sake of illustration, consider a dataset drawn from a Gaussian distribution. </p> <p>The MODL approach selects the most probable model based on the data, among all possible models, i.e. histograms with a more or less important number of intervals, and varying interval sizes. As shown in the following figure, the key is to choose the right number of intervals. An insufficient number of intervals leads to oversimplification and thus to a loss of information. Conversely, an excessive number of intervals introduces unnecessary complexity and sensitivity to outliers and noisy values.</p> <p> </p> <p>In the MODL approach:</p> <ol> <li>the Prior \\(P(h)\\) specifies a preference for the simplest model; </li> <li>the Likelihood \\(P(d|h)\\) measures the model's ability to accurately describe the data sample. </li> </ol> <p>The following paragraph presents the role of each of these terms and how they are constructed:</p> <ul> <li> <p>The Prior follows a particular form - hierarchical and uniform. In our histogram example, the parameter hierarchy has two levels: (i) the choice of the number of intervals in the histogram; (ii) the definition of the bounds' positions. Model instantiation involves successive choices of parameter values by moving towards the bottom of the parameter hierarchy (first the number of bounds, then their positions). All possible choices have the same probability within each hierarchical level, preserving neutrality in model selection and allowing data to best express their information. The hierarchical form of the prior limits the complexity of the model, reducing the risk of overfitting. Indeed, increasing the number of intervals in a histogram considerably increases the number of possible positions for its bounds, making it less probable a priori.</p> </li> <li> <p>The Likelihood plays the opposite role, favoring models that describe the data sample as accurately as possible. </p> </li> </ul> <p>During training, optimizing the MODL criterion selects the most probable model by combining the Prior with the Likelihood, thus achieving a balance between underfitting and overfitting.</p> <p>What you should remember</p> <p>Unlike conventional approaches, no weighting of the regularization term (the prior in this case) is necessary, as the mathematical consistency between the prior and the likelihood follows from Bayes' formula. This explains the absence of hyperparameters in the MODL approach.</p>"},{"location":"learn/modl/#conclusion","title":"Conclusion","text":"<p>The MODL approach is rooted in both Bayesian and Information theories. What makes Khiops original is the end-to-end application of the Minimum Description Length (MDL) principle across the entire AutoML pipeline provided, with a focus on industrial-strength scalability. As a result, all the learning algorithms used by Khiops share the following special features:</p> <ol> <li> <p>Trained models consistently generate a segmentation of the data (e.g. the histograms described above discretize the variable to estimate its density). The structure of this segmentation varies according to the specific learning task at hand, within the AutoML pipeline:</p> <ul> <li>Discretization of numerical variables consists in dividing their range into several intervals;</li> <li>Grouping of categorical variables involves merging some of their modalities;</li> <li>Variable Selection during Parsimonious Training discriminates the relevant variables from the irrelevant ones. </li> </ul> <p>After this partition has been trained, the statistical models that Khiops generates take the form of a piecewise constant estimator. This means that within each segment of the partition, the model predicts a constant value (e.g. as inside a histogram interval).</p> </li> <li> <p>During training, the most probable model given the data is selected. This selection is based on MODL optimization criteria, which are derived from Bayes' formula, exploiting a unique form of prior distribution. The choice of a prior that is hierarchical and uniform at each level of the hierarchy comes from the principle of minimum description length (MDL) in information theory, and corresponds to the most compact way of describing the model.</p> </li> <li> <p>By design, the optimization criteria prevent overfitting, thanks to (i) the specific shape of the a priori distribution, which favors simpler models, and (ii) the cardinality of the model family, which increases with the number of training examples. These two mathematical features automate the balance between model accuracy and robustness. As the size of the training set increases, the likelihood term gains more weight, enabling the model to become increasingly accurate while maintaining its robustness (this intuition is detailed on the discretization section).</p> </li> <li> <p>Khiops algorithms require no hyperparameters, as all parameters defining the model are optimized by the optimization criteria. This eliminates the costly step of empirically optimizing hyperparameters. What's more, the algorithms are designed to handle large amounts of data and adapt their operations to the available hardware resources.</p> </li> </ol> <p>In essence, this is the crux of Khiops' secret, implementing the MODL approach across all stages of its AutoML pipeline (Stages A, B, and C). This enables the learning algorithms to accomplish their tasks in a single run!</p> <p></p> <p> </p>"},{"location":"learn/preprocessing/","title":"Optimal Encoding","text":"<p>Data encoding is one of the necessary steps of the machine learning process. Before a model can be trained on data, the data needs to be transformed into a format proper to learning. Proper encoding doesn't just make the data readable by algorithms; it can significantly enhance the efficiency and accuracy of the learning process.</p> <p>Khiops' Auto-ML pipeline recognizes this pivotal role of encoding and designates it as the second step (B):</p> <p> </p> <p>In this crucial phase:</p> <ul> <li>Numerical variables go through an encoding process known as discretization which converts continuous variables into discrete ones;</li> <li>Categorical variables are further refined and encoded through a grouping method, which ensures their optimal categorization.</li> </ul> <p>These transformations are not simply heuristic-based adjustments. The univariate models created at this stage become essential building blocks for the next stage of the pipeline, where they are selectively combined to form a parsimonious multivariate Bayesian classifier.</p> <p>Recommendation</p> <p>By exploring this section, readers will discover the details of encoding methodologies and grasp their technical advantages. However, to fully capture the nuances and principles of these techniques, it is recommended to first delve into the section \"An original formalism\", which introduces the MODL formalism.</p> <p>The remainder of this page focuses on two primary sections: Discretization techniques and Modality Grouping methods.</p>"},{"location":"learn/preprocessing/#discretization","title":"Discretization","text":"<p>Building on the principles of the MODL formalism discussed in the Original Formalism section, this section elaborates on its application to univariate discretization. Univariate discretization transforms continuous variables into discrete intervals, which simplifies analysis and enhances model interpretability. </p> <p>This section is divided into three main sub-sections:</p> <ul> <li>Model Parameters: Detailing the parameters and hierarchical organization of the models.</li> <li>Optimization Criterion: Understanding the Bayesian-driven criterion for selecting the most probable model.</li> <li>Optimization Algorithm: An overview of the optimization algorithm developed to train the discretization model.</li> </ul> <p>Let's explore these aspects to gain a complete understanding of the MODL-based discretization process.</p>"},{"location":"learn/preprocessing/#model-parameters","title":"Model Parameters","text":"<p>In the supervised context, discretization can be viewed as a univariate probabilistic classifier that aims to partition (or segment) numeric variable in order to describe the target variable distribution.</p> <p>As previously introduced in the Original Formalism section, the initial step is the definition of the model parameters that are appropriate for the learning task at hand. This leads us to first define the family of hypotheses \\(\\mathcal{H}\\). Within this family, each hypothesis \\(h\\) represents a unique way of segmenting or partitioning each numeric input variable into distinct intervals.</p> <p>Once a numeric variable is partitioned according to a specific hypothesis, the resulting model behaves much like a probabilistic classifier. It estimates the class probability \\(P(y|x)\\) in an intuitive manner: for a particular numeric value \\(x\\), the associated probability \\(P(y|x)\\) of a class \\(y\\) is inferred by examining the distribution of classes in the interval containing \\(x\\).</p> <p> </p> <p>In this context, a discretization model is uniquely described by the following parameters, organized into 3 hierarchical levels:</p> <ol> <li>\\(I\\): the number of intervals.</li> <li>\\({ N_i}_{i \\in [1, I]}\\): the number of training examples in each of the \\(I\\) intervals.</li> <li>\\({ N_{ij}}_{i \\in [1, I], j \\in [1, J]}\\): the number of examples within each interval and belonging to each of the \\(J\\) classes.</li> </ol> <p>It is crucial to notice two characteristics:</p> <ul> <li>Cardinality: The size of \\(\\mathcal{H}\\) is directly influenced by the number of examples, denoted by \\(N\\), in our training dataset. Specifically, our first parameter \\(I\\), which denotes the number of intervals, can take any value between 1 and \\(N\\). This unusual property,  where the model's possible configurations are tied to the size of the dataset, is atypical in a Bayesian context. As we will see later, this unique feature of the MODL approach has a significant impact on regularization (to prevent overfitting).</li> <li>Hierarchy: When defining a model, decisions are made in cascade across levels 1, 2, and 3; i.e., decisions made at an earlier level influence the number of possibilities at the subsequent level. Level 1 determines the number of intervals, which in level 2 drives the choices for positioning the \\(I-1\\) boundaries between the intervals within the \\(N\\) examples. Notably, MODL leverages rank statistics, using example counts in intervals to define interval boundaries. Lastly, level 3 summarizes the conditional distribution \\(P(y|x)\\), by specifying the class distributions within each interval that is defined by the boundaries set in Level 2. The number of available choices for this distribution is influenced by the parameters \\({ I, { N_i}}\\) from levels 1 and 2.</li> </ul>"},{"location":"learn/preprocessing/#optimization-criterion","title":"Optimization Criterion","text":"<p>MODL is a Bayesian model selection approach. The goal of the optimization criterion is to select the most probable model given the training data, denoted by \\(d\\). Based on what is introduced in the Original Formalism section and referring to information theory, the optimization criterion can be expressed as: </p> \\[-\\log(P(h).P(d|h)) = \\underbrace{L(h)}_{\\textbf{Prior}}  + \\underbrace{L(d|h)}_{\\textbf{Likelihood}} \\] <p>Given the model parameters introduced above, the optimization criterion used to select the most probable discretization model can be expressed as follows:</p> <p>The prior:</p> \\[L(h) = \\underbrace{ \\vphantom{\\sum\\limits_{k=1}^K}  \\log N}_{\\textbf{level 1}} + \\underbrace{ \\vphantom{\\sum\\limits_{k=1}^K} \\log \\binom{N+I-1}{I-1}}_{\\textbf{level 2}} + \\underbrace{\\sum\\limits_{i=1}^I \\log \\binom{N_i + J -1}{J-1}}_{\\textbf{level 3}} \\] <p>It is hierarchical and uniform at each level of this hierarchy: </p> <ul> <li>level 1: probability of a particular number of intervals, all the values \\(I \\in [1, N]\\) being equiprobable with a probability that equals \\(1/N\\);</li> <li>level 2: probability of a particular positioning of the bounds between intervals, given the value of \\(I\\), with \\(\\binom{N+I-1}{I-1}\\) the number of possible positionings;</li> <li>level 3: probability of a particular class distribution within the interval, with \\(\\binom{N_i + J -1}{J-1}\\) the number of possible such distributions.</li> </ul> <p>As a reminder, the logarithm is used in this context to transform the probability product (from the Bayes formula) into a sum. Thus, the sum of levels 1, 2, and 3 finally expressed a product of independent probabilities corresponding to successive choices at each level.</p> <p>Also, it is important to notice that the prior evaluates hypotheses' probabilities from the combinatorial space of possible discretizations, regardless the training examples.  Simple models (with fewer intervals) are promoted since an important number of intervals increases the number of possible positioning of boundaries and the possible class distribution within the intervals.</p> <p>The Likelihood:</p> \\[L(d|h) = \\sum\\limits_{i=1}^I \\log \\binom{N_i}{N_{i1},N_{i2},\\dots,N_{iJ}} \\] <p>This term estimates the probability of a particular training set consistent with the model's description (i.e., given the parameters of the model), which turns out to be a multinomial problem. For each interval, the multinomial coefficient counts the number of distinct ways to permute the \\({N_i}\\) training examples (the dataset) where the number of examples \\({N_{ij}}_{j \\in [1, J]}\\) of each of the \\(J\\) classes is known (the multiplicity). Optimizing the likelihood aims to create intervals that are as homogeneous (or pure) as possible, i.e., ideally containing only a single class.</p> <p>There's a natural balance between the prior and the likelihood in this approach, preventing overfitting</p> <p>While the particular shape of the prior distribution naturally leans towards models with fewer intervals, the likelihood tends to support more complex models that accurately describe the training data (i.e., more intervals). Given that both terms are consistent, there is no need to weigh one against the other. This inherent balance is what makes MODL hyperparameter-free.</p> <p> </p> <p>In traditional machine learning approaches (see our section about classical Empirical Risk Minimization ), the weight of the regularization term depends on the size of the training set. As the training set grows, the weight attributed to the regularization term may be reduced, allowing the model to capture more accurate patterns in the data. Similarly, in the MODL approach, the balance between the prior and likelihood is natural and automatically adjusts the model's complexity based on the size of the training set.</p> <p>As the size of the training dataset grows, the cardinality of \\(\\mathcal{H}\\) also expands, offering a more significant number of ways to discretize the data into intervals. Thus, the combinatorial space of possible permutations becomes larger, thus making the likelihood term more consequential in the overall optimization process (the probability of a particular training set diminishes in this expanding space). Consequently, the prior term, which generally promotes simpler models, becomes less influential than the likelihood. This shift in balance leads to more accurate models, while maintaining robustness.</p>"},{"location":"learn/preprocessing/#optimization-algorithm","title":"Optimization Algorithm","text":"<p>This section provides a high-level overview of the optimization algorithm utilized for discretization. For a more in-depth understanding, please consult the relevant scientific publications.</p> <p>Given the exponential growth of the number of potential models relative to the dataset size, an exhaustive optimization approach is not feasible. While dynamic programming can provide an optimal solution for discretization, it may not be efficient enough. As a result, a heuristic-based algorithm is used instead. This method is significantly faster and produces models that closely approximate the optimal solution.</p> <p>The algorithm operates in two distinct steps, as illustrated in the following figure:</p> <ul> <li> <p>Step 1 consists of a greedy algorithm which starts by evaluating the most complex model (single-value intervals), and iteratively performs the best merge (minimizing the MODL criterion) between two consecutive intervals, until the single-interval model is obtained. At the end of this run, the best model is kept for the next step. </p> </li> <li> <p>Step 2 is a neighborhood exploration used to exit a local optimum, and which considers several types of transformations: (i) merging two intervals, (ii) splitting an interval, (iii) moving a bound, (iv) merging 3 intervals and splitting the result in two intervals. The best transformation is applied as long as it improves the model (i.e., it decreases the MODL criterion).</p> </li> </ul> <p>For an efficient coding, it is necessary to keep in memory the sorted lists of all possible transformations, ordered by the improvement of the optimization criteria. These lists must be updated for each transformation that is performed on the model.   </p>"},{"location":"learn/preprocessing/#grouping","title":"Modality Grouping","text":"<p>Modality grouping operates as an essential mechanism for categorizing and simplifying categorical variables, thereby improving the efficiency and interpretability of subsequent analysis processes. This section is organized similarly to the discretization section, covering model parameters, optimization criteria, and the optimization algorithm.</p>"},{"location":"learn/preprocessing/#model-parameters_1","title":"Model Parameters","text":"<p>In modality grouping, the goal is to optimally gather different modalities of a categorical variable in a manner that best predicts the target class variable. To introduce this concept, let us consider a categorical variable that has \\(M=5\\) different modalities, denoted by \\(\\{a, b, c, d, e\\}\\). In this setup, each training example is a pair \\((x, y)\\), where \\(x\\) is a modality and \\(y\\) is a class value (e.g., \\((b,{\\color{red} \\bullet})\\)). Just as for discretization, the goal is to find the most probable grouping model using the training data. This learned model estimates the conditional probabilities of class values \\(y\\) given observed modalities \\(x\\) by counting examples within each group.   </p> <p>A grouping model is uniquely described by the following parameters:</p> <ul> <li>\\(G\\): the number of modality groups;</li> <li>\\(\\{g(x)\\}_{x \\in [1, M]}\\): the index that maps a modality \\(x\\) to its respective group;</li> <li>\\(\\{ N_g\\}_{g \\in [1, G]}\\): the count of examples in each of the \\(G\\) groups;</li> <li>\\(\\{ N_{gj}\\}_{g \\in [1, G], j \\in [1, J]}\\): the count of examples in each group for each of the \\(J\\) classes.</li> </ul>"},{"location":"learn/preprocessing/#optimization-criterion_1","title":"Optimization Criterion","text":"<p>In a very similar way to the discretization, the following criterion must be minimized in order to select the most probable grouping model: </p> <p>The prior </p> <p>It has the same hierarchical and uniform shape as for discretization, but it differs slightly:</p> \\[-\\log(P(h).P(d|h)) = \\overbrace{\\underbrace{ \\vphantom{\\sum\\limits_{g=1}^G}  \\log M}_{\\textbf{level 1}} + \\underbrace{ \\vphantom{\\sum\\limits_{g=1}^G} \\log \\left (  \\sum\\limits_{g=1}^G S(M,g) \\right )}_{\\textbf{level 2}} + \\underbrace{\\sum\\limits_{g=1}^G \\log \\binom{N_g + J -1}{J-1}}_{\\textbf{level 3}}}^{\\textbf{Prior}} \\] <ul> <li>level 1: probability of a particular number of groups, all the values \\(G \\in [1, M]\\) being equiprobable,</li> <li>level 2: probability of a particular composition of groups, given the value of \\(G\\),</li> <li>level 3: probability of a particular class distribution given the previous parameters.</li> </ul> <p>Unlike numerical variables, which can be arranged in a specific order, categorical variables have distinct values which cannot be inherently ranked. Consequently, the level 2 term in our optimization criterion enumerates the possible configurations for forming \\(G\\) groups from \\(M\\) distinct modalities. Specifically, this term quantifies the probability associated with a particular index function \\(\\{g(x)\\}\\), given a fixed number of groups \\(G\\). It assumes that all possible indexes are equally probable. The count of these possible configurations is determined by the sum of the Stirling numbers of the second kind, denoted as \\(S(M,g)\\).</p> <p>For more details, you can refer to the scientific publications.</p> <p>The likelihood </p> <p>It is defined as previously discussed, i.e.:</p> \\[L(d|h) = \\sum\\limits_{i=1}^I \\log \\binom{N_i}{N_{i1}, N_{i1}, \\dots, N_{iJ}} \\]"},{"location":"learn/preprocessing/#optimization-algorithm_1","title":"Optimization Algorithm","text":"<p>The optimization algorithm used to train the grouping models is similar to the discretization case. In the same way, the number of possible grouping models increases exponentially with the number of modalities \\(M\\).  For this reason, an exhaustive optimization algorithm is not feasible, as it would not scale. The algorithm used is a heuristic composed of two successive steps: </p> <ul> <li> <p>Step 1 consists of a greedy algorithm which starts by evaluating the most complex model, and iteratively performs the best merge between two groups, until the single-group model is obtained. At the end of this run, the best model is kept for the next step. </p> </li> <li> <p>Step 2 is a neighborhood exploration which repeats the following two steps as long as the model is improved: (i) moving the modalities between groups, (ii) merging two groups.</p> </li> </ul> <p>For an efficient coding, it is necessary to keep in memory the sorted lists of all possible moves and merges, ordered by improvement of the optimization criteria. As before, these lists must be updated for each transformation performed on the model.   </p>"},{"location":"learn/preprocessing/#benefits","title":"Summary of Technical Benefits","text":"<p>Optimal Encoding</p> <p>Discretization and grouping models offer an ideal framework for encoding both numerical and categorical variables without requiring user-defined parameters. These supervised approaches become progressively more accurate as the volume of training data expands. Furthermore, Khiops can be seamlessly integrated into existing machine learning pipelines, thereby eliminating the need for arbitrary pre-encoding decisions or their inclusion in hyperparameter optimization.</p> <p>Robust Evaluation via Compression Gain Metric</p> <p>Khiops provides a robust metric called Compression Gain, also known as Level in Khiops outputs, that can be directly applied to training data for evaluation. In information theory, MODL optimization criteria represent a coding length, i.e. the number of bits needed (i) to describe the model (prior), (ii) to describe the target variable \\(y\\) of the training data (likelihood). Because this value is dependent on the dataset's size, it is not a standalone evaluation metric. The Compression Gain \\(CG\\) corrects for this by normalizing the learned model's \\(\\hat{h}\\) coding length to a baseline model \\(h_0\\), which consists of a single group/interval:</p> \\[CG = 1 - \\frac{-\\log(P(\\hat{h}).P(d|\\hat{h}))}{-\\log(P(h_0).P(d|h_0))}\\] <p>Available in Khiops' output, the Compression Gain metric weighs the individual predictive level of each variable for the target \\(y\\), effectively measuring its importance.</p> <p>Resilience to Outliers</p> <p>Being based on rank statistics, the MODL approach ensures that outliers have no impact on the learning process for discretization models. Specifically, an extreme value is simply processed as the smallest or the largest value.</p> <p>Data Cleaning Not Required</p> <p>Khiops eliminates the need for cleaning your training data, and even discourages it to avoid introducing bias into the data. The library incorporates the missing values as predictive features rather than discarding them. In the case of discretization models, missing values are treated as an extreme value, while for modality grouping models, they are treated as a distinct modality. This enables the model to capture informative patterns the missing values might offer for classifying the target variable. During inference, any unknown modalities encountered are automatically categorized as missing values.</p> <p>Invariance to Monotonic Transformations</p> <p>Khiops\u2019 discretization is based on value ranks, meaning it is invariant to any monotonic transformation (log, square root, square, exponential, etc.). Whether a numerical feature is directly used or transformed, the resulting discretization remains unchanged. As a result, users do not need to apply transformations like log-scaling or standardization.</p>"},{"location":"learn/understand/","title":"What makes Khiops different","text":"<p>Khiops is an end-to-end solution for Automated Machine Learning (AutoML), natively and effortlessly handling intricate Data Science time-consuming tasks. These include feature engineering (A), data cleaning and encoding (B), and the training of parsimonious models (C).</p> <p> </p> <p>The Auto-ML capability allows Khiops to expertly process tabular data, regardless of whether it comes in single-table format, or in the form of relational data sets, including those with complex \"snowflake\" schemas. This becomes a distinctive asset in various situations, particularly when addressing use-cases with multiple records per statistical individual (such as calls, transactions, or production logs). Khiops handles these scenarios seamlessly and automatically, making it an invaluable tool for extracting rich insights from complex datasets.</p> <p>The distinctiveness of Khiops lies in its departure from typical AutoML solutions that often run an expensive range of complex algorithms over putative sets of parameters in a grid search manner. Instead, Khiops employs an original formalism called MODL (which is hyperparameter-free), enabling it to push boundaries by enhancing automation levels. As a result, it can build high-performance models that retain simplicity for ease of interpretation. Moreover, this approach supports impressive scalability, setting a new benchmark in machine learning.</p> <p> </p>"},{"location":"learn/understand/#advanced-automation","title":"Advanced Automation","text":"<p>Khiops significantly enhances the productivity of data scientists by seamlessly automating numerous time-intensive tasks. Key benefits include:</p> <ul> <li>Automated Data Cleaning: Khiops alleviates the need for manual data cleaning, and the training of models remains unaffected by outliers. It minimizes the need for intricate preprocessing and outlier detection, saving valuable time.</li> <li>Native Processing of Variables: Khiops adeptly manages all data types without requiring manual encoding. Categorical variables are naturally grouped, and numerical variables are cleverly divided into intervals, making it easier for the models to find patterns and relationships. </li> <li>Auto Feature Engineering: Khiops automatically calculates and selects the summarizing 'features' or 'aggregates' from temporal and relational datasets (in a unique way thanks to the MODL approach), providing a concise, informative snapshot that can be readily used for modeling. This saves considerable time and effort while ensuring that essential data patterns are not overlooked.</li> <li>Efficient Variable Selection Algorithms: Khiops employs two sophisticated variable selection techniques. Firstly, during the optimal encoding phase (B), it weeds out variables that lack correlation with the target using Compression Gain. Secondly, during the parsimonious learning step (C), it handpicks a compact subset of the most informative and mutually independent variables.</li> </ul> <p>The remainder of the section introduces the auto-features engineering step.</p> <p>This section only introduces the concept. For technical details, please refer to the  Auto features engineering section.</p> <p>Relational data structures are common in many professional environments, encompassing data on users (e.g., call or payment logs) or production data where each step generates its records. In such contexts, we deal with a primary table of statistical individuals (potentially the targets for supervised learning) and secondary tables that contain the related logs.</p> <p>Mining insights from these datasets is definitely more intricate than working with straightforward, single-table datasets often utilized in tutorials. The crux of this complexity lies in the fact that each individual can be linked to multiple records in the secondary tables, and it's not feasible to uniformly assign these records to the individual class. In other words, handling relational data for predictive modeling calls for a specialized approach, which is where Khiops comes in.</p> <p>Consider a brief example: the goal is to identify call spammers using call log data. The primary table contains the list of customers along with the target attribute (is a fraudster or not). Complementing this, secondary tables detail 'in' and 'out' calls for each customer, some having more than thousands of rows.</p> <p> </p> The wrong way to get information from additional tables <p>In that example, it's essential to acknowledge that a single call doesn't define an individual as a fraudster - it is the overall pattern of their calls that can lead to this classification (e.g., a high volume of out calls but no in calls or consistently different recipients).</p> <p>It is not feasible, or indeed correct, to map the customer's target outcome to each of their calls (rows), then attempt to train a model on all these rows in a homogeneous manner. Such an approach fundamentally alters the 'statistical individual' under consideration from the customer to the calls, thereby violating the principle of independent learning examples.</p> <p>Instead, the ideal approach is to derive features that encapsulate the call logs, effectively summarizing customer behaviors. For instance, one could consider features such as the daily count of 'in' and 'out' calls (corresponding to the number of rows linked to individuals in the secondary tables), the average duration of calls, or the number of unique contacts.</p> <p> </p> Getting aggregates from additional tables (group by) <p>The outcome of this process is a streamlined dataset - a single row for each individual, containing both the original variables from the primary table and the newly engineered features (or aggregates) that encapsulate the supplementary information from the secondary tables. </p> <p>Performing this step manually would be time-consuming, carry technical constraints, and can be suboptimal. The limitations of human perception and inherent biases can obstruct the comprehensive exploration of potential aggregates. Additionally, any shift in the target - such as a new business objective - necessitates a fresh round of feature engineering.</p> <p>One of the strengths of Khiops lies in its automation of this complex step. By leveraging the MODL approach, Khiops effectively examines numerous candidate aggregates from the secondary tables, retaining only those whose informational value outweighs their associated complexity. Consequently, Khiops stands out as a solution that automates feature engineering while circumventing the pitfalls of overfitting.</p> <p>See what Khiops-built aggregates look like using our tutorials here.</p>"},{"location":"learn/understand/#interpretability","title":"Interpretability","text":"<p>Khiops emphasizes interpretability throughout its implementation pipeline, aligning with critical industrial requirements:</p> <ul> <li> <p>During the auto feature engineering phase (Step A), the aggregates produced bear explicitly descriptive names that mirror their calculation formulas. Khiops helps users to deepen their data comprehension by suggesting valuable aggregates for their specific task - aggregates that may have eluded even the sharpest business expertise.</p> </li> <li> <p>In the optimal encoding phase (Step B), effective and simplistic models are trained for discretization and grouping. This helps users to promptly grasp the predictive class distribution for each explanatory variable.</p> </li> <li> <p>During parsimonious training (Step C), after a rigorous aggregate search in Step A, the goal is to significantly reduce the number of utilized variables. This approach simplifies model analysis for data scientists. Moreover, the chosen variables are as independent as possible, enabling easy, additive analysis of their contribution to predictions.</p> </li> </ul> <p>Khiops is coupled with an interactive visualization tool, providing direct access to extensive pipeline results from a notebook or a dedicated application. The upcoming section offers a glimpse into visualizing variable encodings.</p> <p>This section only introduces the concept. For technical details, please refer to the  optimal encoding and  visualization tool sections.</p> <p>While the final pipeline stage (C) leverages all variables to train models, preprocessing step (B) constructs univariate models for each variable. A key advantage of employing the MODL approach at this stage is that it facilitates auditable and interpretable variable encoding. Khiops practically encodes every variable into a discretized feature: splitting numerical values into intervals and grouping categorical values. The encoding aims to strike a balance between target prediction accuracy and encoding complexity (viz. the overfitting risk).</p> <p>Consider a new example: predicting whether adult revenue falls above or below $50k. During preprocessing, Khiops analyzes every variable, partitioning numerical variables, such as age, into intervals. If accuracy is prioritized, it will generate as many intervals as there are unique values. However, if complexity is minimized, only one interval is created. Leveraging the MODL approach, Khiops finds an optimal balance without requiring parameter tuning. The visualization component of Khiops depicts the age variable encoding as follows:</p> <p> </p> Discretization of the age variable on the Khiops Visualization tool <p>The number of available training samples impacts this balance. More data leads to more precise discretization or grouping. Doubling the number of individuals in our example would result in a more detailed interval structure.</p> <p> </p> Discretization of the age variable depending on the number of training examples <p>In conclusion, Khiops' encoding provides insights into our data by offering a thorough variable analysis. The entire Auto ML pipeline is intuitively interpretable. For a comprehensive overview of the features offered by the Khiops visualization component, please refer to the relevant section.</p>"},{"location":"learn/understand/#outstanding-scalability","title":"Outstanding Scalability","text":"<p>The Khiops MODL approach automatically adjusts model complexity based on available training data without needing hyperparameters. As a result, it uses algorithms that inherently avoid overfitting right from the training stage without the need for model evaluation on validation data.</p> <p>This distinctive feature brings about exceptional scalability. Since Khiops is free from hyperparameters, it eliminates the need for time-consuming and resource-intensive steps such as cross-validation and grid search.</p> <p>Additionally, thanks to efficient low-level coding, Khiops can operate across a wide range of hardware environments. Even when the hardware resources are insufficient to load the entire training set into RAM (this is also known as \"out-of-core\" training), Khiops still manages to function effectively.</p> <p>Furthermore, Khiops can seamlessly transition between out-of-core and distributed computations due to its strategy of adapting to available hardware resources. That makes it flexible and versatile, able to accommodate various operational requirements and constraints.</p> <p> </p> <p>Deep Dive into MODL: An Overview and Guide</p> <p>The remainder of this documentation's Understanding section aims to demystify the MODL approach and illustrate its applications across the Auto ML pipeline. The content is presented pedagogically, so it is crucial to maintain the prescribed reading sequence. Those seeking a more thorough scientific comprehension should browse the collection of bibliographic references, organized as a reading guide.</p>"},{"location":"setup/","title":"Index","text":""},{"location":"setup/#installation-options","title":"Installation Options","text":"<p>Khiops supports a diversified set of installation options, to meet different needs:</p> <ul> <li>Khiops Python Library:<ul> <li>Packaged via <code>conda</code> (recommended)</li> <li>Packaged via <code>pip</code></li> <li>Packaged in our khiops-notebook container</li> </ul> </li> <li>Applications:<ul> <li>Khiops Application for advanced data analytics with just a few clicks using a graphical user interface. This application is also the basis for easy integration into different systems (all programming languages, docker, servers, etc.).</li> <li>Khiops Visualization: for intuitive visualization of all analysis results (interactive demo available here)</li> <li>Khiops Native Interface (KNI): to deploy Khiops models with a lightweight shared library.</li> </ul> </li> </ul> <p>Supported Platforms</p> <p>We support  Python from 3.8 to 3.13 and the following operating systems:</p> <ul> <li>Windows 10 or later</li> <li>Ubuntu 20, 22 and 24 (LTS)</li> <li>Debian 10, 11 and 12</li> <li>Rocky Linux 8 and 9</li> <li>macOS 12 or later, only via  Conda. Full support for macOS 13 or later on ARM architectures, limited support for macOS 12 or for x86-64 architectures.</li> </ul> <p>The  Google Colaboratory environments are supported. To benefit from Khiops on these environments, users are encouraged to install the Khiops Conda package, which has been tested in these environments.</p> <p>For other platforms, please  Contact Us.</p>"},{"location":"setup/conda/","title":"Install the Khiops Library Using Conda","text":"<p>The Conda package installation guarantees optimal performance since it handles installing or upgrading all Khiops dependencies, including the MPI library, in your Conda environment. </p>"},{"location":"setup/conda/#instructions","title":"Instructions","text":"<p>Khiops supports  Python versions from 3.8 to 3.13. By default, we recommend creating a dedicated Conda environment to ensure compatibility and avoid conflicts with other packages. To create an environment (for instance with Python 3.12), use the following command:</p> <pre><code>conda create --name khiops_env python=3.12\n</code></pre> <p>After creating the environment, activate it using:</p> <pre><code>conda activate khiops_env\n</code></pre> <p>Once the environment is activated, you can install Khiops as follow:</p> <pre><code>conda install -c conda-forge -c khiops khiops\n</code></pre> <p></p>"},{"location":"setup/conda/#user-guide","title":"User Guide","text":"<ul> <li> <p>For those who aren't familiar with Conda, you can start by reading Getting started with Conda.</p> </li> <li> <p>For  Google Colaboratory Colab users, please follow usual Conda installation procedure as documented here</p> </li> <li> <p>If you use Conda behind a company proxy, you may encounter an HTTP and SSL error. In this case, you can read Using Anaconda behind a company proxy </p> </li> </ul>"},{"location":"setup/conda/#what-you-should-know","title":"What You Should Know","text":"<p>You can consult the limitations or known problems corresponding to your operating system:</p> Users on  macOSUsers on  WindowsUsers on Linux Important Note for users upgrading from the previous pyKhiops package (up to 10.1) <p>If you are upgrading from a version prior to Khiops 10.2, it is essential that the <code>pykhiops</code> package is not installed in your Python environment. This ensures that your upgrade process is smooth and that the new version of Khiops installs without conflicts.</p> <p>To uninstall pykhiops, please execute the following command in your terminal or command prompt (use admin rights if necessary):</p> <pre><code>pip uninstall pykhiops -y\n</code></pre> <p>Alternatively, you can just create a new, fresh Conda environment and install the Khiops Conda package therein.</p> Pip and Conda Khiops installations should not be mixed. <p>If the users wish to switch from a Conda-based installation to a Pip-based installation, they need to, first, deactivate the Conda environment Khiops is installed into. Or, alternatively, they need to uninstall the Khiops Conda package:</p> <p><pre><code>conda remove khiops\n</code></pre> Note:  If the standard uninstallation process fails, it may be necessary to add <code>--force</code>. This option forces the removal, potentially bypassing dependency conflicts or other issues preventing the package from being uninstalled normally. However, users should be cautious as forcing the removal can impact the stability of their Conda environment and affect other packages.</p> <p>Warning</p> <p>On the first run of Khiops, an MPI-related popup may appear due to parallel execution sockets. To avoid these popups and ensure optimal performance, please configure Khiops to not accept incoming connections. To this end, enter the following commands in your shell:</p> <pre><code>FW=/usr/libexec/ApplicationFirewall/socketfilterfw \nsudo $FW --remove $(which MODL)\nsudo $FW --add $(which MODL)\nsudo $FW --block $(which MODL)\n</code></pre> <p>Warning</p> <p>The installation of Khiops will utilize MPICH version 3.4.3 due to compatibility issues.  This is why you need to use a dedicated command: <pre><code>conda install -c conda-forge -c khiops khiops\n</code></pre></p> <p>Be aware that this may result in slower execution times compared to other platforms. This limitation is expected to be addressed in a future MPICH release.</p> Important Note for users upgrading from the previous pyKhiops package (up to 10.1) <p>If you are upgrading from a version prior to Khiops 10.2, it is essential that the <code>pykhiops</code> package is not installed in your Python environment. This ensures that your upgrade process is smooth and that the new version of Khiops installs without conflicts.</p> <p>To uninstall pykhiops, please execute the following command in your terminal or command prompt (use admin rights if necessary):</p> <pre><code>pip uninstall pykhiops -y\n</code></pre> <p>Alternatively, you can just create a new, fresh Conda environment and install the Khiops Conda package therein.</p> Pip and Conda Khiops installations should not be mixed. <p>If the users wish to switch from a Conda-based installation to a Pip-based installation, they need to, first, deactivate the Conda environment Khiops is installed into. Or, alternatively, they need to uninstall the Khiops Conda package:</p> <pre><code>conda remove khiops\n</code></pre> <p>Note:  If the standard uninstallation process fails, it may be necessary to add <code>--force</code>. This option forces the removal, potentially bypassing dependency conflicts or other issues preventing the package from being uninstalled normally. However, users should be cautious as forcing the removal can impact the stability of their Conda environment and affect other packages.</p> <p>Warning</p> <p>On the first run of Khiops, an MPI-related popup may appear due to parallel execution sockets; please allow access for optimal functionality.</p> <p>Warning</p> <p>Windows users already having MSMPI installed may see an Anaconda warning suggesting to uninstall it; please ignore this message.</p> Important Note for users upgrading from the previous pyKhiops package (up to 10.1) <p>If you are upgrading from a version prior to Khiops 10.2, it is essential that the <code>pykhiops</code> package is not installed in your Python environment. This ensures that your upgrade process is smooth and that the new version of Khiops installs without conflicts.</p> <p>To uninstall pykhiops, please execute the following command in your terminal or command prompt (use admin rights if necessary):</p> <pre><code>pip uninstall pykhiops -y\n</code></pre> <p>Alternatively, you can just create a new, fresh Conda environment and install the Khiops Conda package therein.</p> Pip and Conda Khiops installations should not be mixed. <p>If the users wish to switch from a Conda-based installation to a Pip-based installation, they need to, first, deactivate the Conda environment Khiops is installed into. Or, alternatively, they need to uninstall the Khiops Conda package:</p> <pre><code>conda remove khiops\n</code></pre> <p>Note:  If the standard uninstallation process fails, it may be necessary to add <code>--force</code>. This option forces the removal, potentially bypassing dependency conflicts or other issues preventing the package from being uninstalled normally. However, users should be cautious as forcing the removal can impact the stability of their Conda environment and affect other packages.</p> <p>Warning</p> <p>The installation of Khiops will utilize MPICH version 4.0.3 due to compatibility issues.  This is why you need to use a dedicated command: <pre><code>conda install -c conda-forge -c khiops khiops\n</code></pre></p> <p>Be aware that this may result in slower execution times compared to other platforms. This limitation is expected to be addressed in a future MPICH release.</p> <p></p>"},{"location":"setup/demovisualization/","title":"Interactive Khiops Visualization Demo","text":"<p>Explore the capabilities of the Khiops Visualization tool through three interactive analyses:</p> <ul> <li>Single Table (Adult dataset): A straightforward supervised analysis demonstrating model performance and feature importance (and encoding) in a single-table context;</li> <li>Single Table with Trees (Adult dataset): Enhances the first analysis by integrating trees;</li> <li>Multi-Table (Accident dataset): Showcases a multi-table model, illustrating how our tool handles complex data structures and reveals key insights.</li> </ul> <p>Click on the corresponding button to choose the case you want to explore:</p>"},{"location":"setup/khiops-notebook/","title":"Run Khiops with Jupyter Docker Stacks","text":"<p>For a quick and easy way to get started with Khiops, you can use our pre-built Docker image.</p> <p>To get the Khiops Docker image, run the following command: <pre><code>docker pull khiopsml/khiops-notebook\n</code></pre></p>"},{"location":"setup/khiops-notebook/#user-guide","title":"User guide","text":"<ul> <li> <p>This image is based on the official Jupyter Docker Stacks and comes pre-configured on top of <code>scipy-notebooks</code>.</p> </li> <li> <p>A more general introduction to containers can be found on the Docker user guide.</p> </li> </ul>"},{"location":"setup/khiops-notebook/#what-you-should-know","title":"What you should know","text":"<p>Some limitations regarding the target architecture:</p> Users on ARM architecture <p>Warning</p> <p>Our Jupyter Docker image is not yet built for the ARM microprocessor architecture. Running it will be extremely slow on  Raspberry or  Apple Silicon.</p>"},{"location":"setup/kni/","title":"The Khiops Native Interface","text":"<p>The purpose of the Khiops Native Interface (or KNI) is to allow a deeper integration of Khiops in information systems, by mean of the C programming language, using a shared library (.dll in Windows, .so in Linux, not available on  macOS) This relates specially to the problem of model deployment, which otherwise requires the use of input and output data files when using directly the Khiops tool in batch mode. See Khiops Guide for an introduction to dictionary files, dictionaries, database files and deployment.</p> <p>The Khiops deployment API is thus made public through a shared library. Therefore, a Khiops model can be deployed directly from any programming language, such as C, C++, Java, Python, Matlab, etc. This enables real time model deployment without the overhead of temporary data files or launching executables. This is critical for certain applications, such as marketing or targeted advertising on the web.</p> <p>All the basics to install and use the Khiops Native Interface is available at this github project.</p>"},{"location":"setup/nocode/","title":"Khiops Application","text":""},{"location":"setup/nocode/#the-khiops-application-no-code-environment","title":"The Khiops Application (No-Code Environment)","text":""},{"location":"setup/nocode/#simplifying-data-science-for-everyone","title":"Simplifying Data Science for Everyone","text":"<p>Welcome to the Khiops application download page. Our intuitive, user-friendly desktop interface is designed for those who may not be familiar with Python or scikit-learn, as well as for users who prefer the convenience of a standalone Graphical User Interface (GUI) for data manipulation. With the Khiops application, advanced data analytics is now just a few clicks away, together with easy system integration.</p>"},{"location":"setup/nocode/#download-installation","title":"Download &amp; Installation","text":"<p>To get started with the Khiops application, follow the relevant procedure for your operating system. Please click on the relevant operating system. Note that our Application is not yet available on  macOS.</p> <p>For further details, you may refer to README.txt, and WHATSNEW.txt:</p> WindowsUbuntu / DebianRocky Linux <p>The  Khiops installer automatically installs the Khiops application, all its dependencies, plus some data samples formatted as expected by Khiops, and the Khiops Visualization application.</p> <p>        Download for Windows      </p> <p>The installation of the Khiops desktop application involves two packages:</p> <ul> <li><code>khiops-core</code>: This is a lightweight package without GUI, documentation or samples. It is intended to be used in advanced settings, on servers and Docker images.</li> <li><code>khiops</code>: This package requires <code>khiops-core</code> and is the full version of Khiops containing the GUI and the documentation.</li> </ul> <p>You can install both packages as follows:</p> <pre><code>CODENAME=$(lsb_release -cs) &amp;&amp; \\\nTEMP_DEB_CORE=\"$(mktemp)\" &amp;&amp; \\\nTEMP_DEB_KHIOPS=\"$(mktemp)\" &amp;&amp; \\\nwget -O \"$TEMP_DEB_CORE\" \"https://github.com/KhiopsML/khiops/releases/download/10.3.1/khiops-core-openmpi_10.3.1-1-${CODENAME}.amd64.deb\" &amp;&amp; \\\nwget -O \"$TEMP_DEB_KHIOPS\" \"https://github.com/KhiopsML/khiops/releases/download/10.3.1/khiops_10.3.1-1-${CODENAME}.amd64.deb\" &amp;&amp; \\\nsudo dpkg -i \"$TEMP_DEB_CORE\" \"$TEMP_DEB_KHIOPS\" || sudo apt-get -f -y install &amp;&amp; \\\nrm -f $TEMP_DEB_CORE $TEMP_DEB_KHIOPS\n</code></pre> <p>If you need the Khiops samples, you can run the following commands: <pre><code>TEMP_SAMPLES=\"$(mktemp)\" &amp;&amp; \\\nwget -O \"$TEMP_SAMPLES\" \"https://github.com/KhiopsML/khiops-samples/releases/download/10.2.4/khiops-samples-10.2.4.zip\" &amp;&amp; \\\nmkdir -p ~/khiops_data/samples &amp;&amp; \\\nunzip \"$TEMP_SAMPLES\" -d ~/khiops_data/samples &amp;&amp; \\\nrm -f $TEMP_SAMPLES\n</code></pre></p> <p>Currently, our packages are released on GitHub. In the coming weeks, we will transition to official repositories.</p> <p>The installation of the Khiops desktop application involves two packages:</p> <ul> <li><code>khiops-core</code>: This is a lightweight package without GUI, documentation or samples. It is intended to be used in advanced settings, on servers and Docker images.</li> <li><code>khiops</code>: This package requires <code>khiops-core</code> and is the full version of Khiops containing the GUI and the documentation.</li> </ul> <p>You can install both packages as follows:</p> <pre><code>sudo yum update -y &amp;&amp; sudo yum install wget python3-pip -y &amp;&amp; \\\nCENTOS_VERSION=$(rpm -E %{rhel}) &amp;&amp; \\\nTEMP_RPM=\"$(mktemp).rpm\" &amp;&amp; \\\nTEMP_RPM_KHIOPS=\"$(mktemp).rpm\" &amp;&amp; \\\nwget -O \"$TEMP_RPM\" \"https://github.com/KhiopsML/khiops/releases/download/10.3.1/khiops-core-openmpi-10.3.1-1.el${CENTOS_VERSION}.x86_64.rpm\" &amp;&amp; \\\nwget -O \"$TEMP_RPM_KHIOPS\" \"https://github.com/KhiopsML/khiops/releases/download/10.3.1/khiops-10.3.1-1.el${CENTOS_VERSION}.x86_64.rpm\" &amp;&amp; \\\nsudo yum install \"$TEMP_RPM\" \"$TEMP_RPM_KHIOPS\" -y &amp;&amp; \\\nrm -f $TEMP_RPM $TEMP_RPM_KHIOPS\n</code></pre> <p>You can find the all versions on the releases page.</p>"},{"location":"setup/nocode/#documentation","title":"Documentation","text":"<p>For a comprehensive guide on how to use the Khiops application and its GUI:</p> <ul> <li>Khiops Guide, for supervised analysis</li> <li>Khiops Coclustering Guide, for unsupervised analysis</li> <li>Tutorial</li> </ul> <p>For easy system integration:</p> <ul> <li>Khiops Scenarios, based on recording and replaying Khiops scenarios in batch mode from any programming language.\"</li> </ul>"},{"location":"setup/nocode/#what-you-should-know","title":"What You Should Know","text":"<p>You can consult the limitations or known problems for your operating system:</p> Users on  Windows <p>Warning</p> <p>The Khiops installer relies on embedded installers for Java and MPI. Antivirus software may remove executable files (.exe, .jar) during installation. In this case, you should add exceptions to your antivirus software or disable it during installation.</p> <p>Warning</p> <p>On some machines, re-installing Khiops may unexpectedly result in just discarding the existing Khiops installation directory. In that case, uninstall Khiops before reinstalling it.</p> <p>Warning</p> <p>In some companies, programs are blocked by the Windows AppLocker group policy. In this case, install Khiops in a recommended directory or run it as administrator.</p>"},{"location":"setup/nocode/#screenshots","title":"Screenshots","text":""},{"location":"setup/pip/","title":"pip","text":""},{"location":"setup/pip/#install-the-khiops-library-using-pip-for-advanced-users","title":"Install the Khiops Library Using Pip   For Advanced users","text":"<p>Opting for <code>pip</code> is ideal for those with a comprehensive grasp of Python's ecosystem and an understanding of operating system specifics. This approach, while offering adaptability for custom setups, necessitates knowledge of environment setup and dependency handling.</p> <p>The Khiops binaries must be installed as a prerequisite. This also ensures the installation of the appropriate version of <code>MPICH</code> library.</p> <p>We support  Python from 3.8 to 3.13.</p> Ubuntu / DebianWindowsRocky Linux <p>You need to download and install the <code>khiops-core</code> package (via Apt) and then the Khiops library (via Pip). You can do this through the following shell commands: <pre><code>sudo apt-get update -y &amp;&amp; sudo apt-get install wget lsb-release -y &amp;&amp; \\\nCODENAME=$(lsb_release -cs) &amp;&amp; \\\nTEMP_DEB=\"$(mktemp)\" &amp;&amp; \\\nwget -O \"$TEMP_DEB\" \"https://github.com/KhiopsML/khiops/releases/download/10.3.1/khiops-core-openmpi_10.3.1-1-${CODENAME}.amd64.deb\" &amp;&amp; \\\nsudo dpkg -i \"$TEMP_DEB\" || sudo apt-get -f -y install &amp;&amp; \\\nrm -f $TEMP_DEB &amp;&amp; \\\npip install 'https://github.com/KhiopsML/khiops-python/releases/download/10.3.1.0/khiops-10.3.1.0.tar.gz'\n</code></pre></p> <p>To install the Khiops binaries, required for the Khiops Python library to operate, you must first install the Khiops application before executing the <code>pip</code> installation command:</p> <p>        Download for Windows      </p> <p>Then, you can run the following Pip command:</p> <pre><code>pip install \"https://github.com/KhiopsML/khiops-python/releases/download/10.3.1.0/khiops-10.3.1.0.tar.gz\"\n</code></pre> <p>The default Python version on Rocky Linux 8 is 3.6, which does not meet our requirements (at least Python 3.8), please ensure a compatible Python version is installed before continuing.</p> <p>Then, you need to download and install the <code>khiops-core</code> package (via Yum) and then the Khiops library (via Pip). You can do this through the following command:</p> <pre><code>sudo yum update -y &amp;&amp; sudo yum install wget python3-pip -y &amp;&amp; \\\nCENTOS_VERSION=$(rpm -E %{rhel}) &amp;&amp; \\\nTEMP_RPM=\"$(mktemp).rpm\" &amp;&amp; \\\nwget -O \"$TEMP_RPM\" \"https://github.com/KhiopsML/khiops/releases/download/10.3.1/khiops-core-openmpi-10.3.1-1.el${CENTOS_VERSION}.x86_64.rpm\" &amp;&amp; \\\nsudo yum install \"$TEMP_RPM\" -y &amp;&amp; \\\nrm -f $TEMP_RPM &amp;&amp; \\\npip install 'https://github.com/KhiopsML/khiops-python/releases/download/10.3.1.0/khiops-10.3.1.0.tar.gz'\n</code></pre>"},{"location":"setup/pip/#user-guide","title":"User Guide","text":"<ul> <li> <p>Users who want to understand how to manage their Python packages can read the  Pip user guide. It will also help those who work behind a company proxy.</p> </li> <li> <p>We also encourage our users to use virtual environments. If you are not familiar with them, you can read this Python documentation page.</p> </li> </ul>"},{"location":"setup/pip/#what-you-should-know","title":"What You Should Know","text":"<p>You can consult the limitations or known issues for your operating system:</p> Users on  LinuxUsers on  WindowsUsers on  macOS <p>Currently, our packages are released on GitHub. In the coming weeks, we will transition to official repositories.</p> Important Note for users upgrading from the pre-10.2.0 versions of the <code>pyKhiops</code> package <p>If you are upgrading from a version prior to Khiops 10.2.0, it is essential to first make sure the <code>pykhiops</code> package is not installed in your Python environment. This ensures that your upgrade process is smooth and that the new version of Khiops installs without conflicts.</p> <p>To uninstall pykhiops, please execute the following command in your terminal or command prompt, in your Python environment (use admin rights if necessary):</p> <pre><code>pip uninstall pykhiops -y\n</code></pre> Pip and Conda Khiops installations should not be mixed. <p>If the users wish to switch from a Pip-based installation to a Conda-based installation, they need to deactivate the Python virtual environment Khiops had been installed into, via Pip. Or, if no virtual environment has been used, the users need to uninstall the Khiops Python package:</p> <pre><code>pip uninstall khiops\n</code></pre> <p>Even though the Khiops binaries would remain installed on the operating system, the Conda-based installation would take precedence over them.</p> <p>Warning</p> <p>The <code>khiops-core</code> binary will install or upgrade the system-wide <code>MPICH</code> library on your system. If you depend on another version of <code>MPICH</code> for other programs, please prefer an installation using Conda.</p> <p>Warning</p> <p>The installation of Khiops will utilize MPICH version 4.0.3 due to compatibility issues. This is why you need to use a dedicated command: <pre><code>conda install -c conda-forge -c khiops khiops\n</code></pre></p> <p>Be aware that this may result in slower execution times compared to other platforms. This limitation is expected to be addressed in a future MPICH release.</p> <p>Currently, our packages are released on GitHub. In the coming weeks, we will transition to official repositories.</p> Important Note for users upgrading from the pre-10.2.0 versions of the <code>pyKhiops</code> package <p>If you are upgrading from a version prior to Khiops 10.2.0, it is essential to first make sure the <code>pykhiops</code> package is not installed in your Python environment. This ensures that your upgrade process is smooth and that the new version of Khiops installs without conflicts.</p> <p>To uninstall pykhiops, please execute the following command in your terminal or command prompt, in your Python environment (use admin rights if necessary):</p> <pre><code>pip uninstall pykhiops -y\n</code></pre> Pip and Conda Khiops installations should not be mixed. <p>If the users wish to switch from a Pip-based installation to a Conda-based installation, they need to deactivate the Python virtual environment Khiops had been installed into, via Pip. Or, if no virtual environment has been used, the users need to uninstall the Khiops Python package:</p> <pre><code>pip uninstall khiops\n</code></pre> <p>Even though the Khiops binaries would remain installed on the operating system, the Conda-based installation would take precedence over them.</p> <p>Warning</p> <p>On the first run of Khiops, an MPI-related popup may appear due to parallel execution sockets; please allow access for optimal functionality.</p> <p>Warning</p> <p>The Khiops installer relies on embedded installers for Java and MPI. Antivirus software may remove executable files (.exe, .jar) during installation. In this case, you should add exceptions to your antivirus software or disable it during installation.</p> <p>Warning</p> <p>The java installer results in a system reboot on some systems (e.g. on Windows Server 2008)</p> <p>Warning</p> <p>Native packages for the Khiops binaries are not yet available for macOS, which means that you cannot install Khiops on macOS using Pip for now. You can use Conda or run our Docker container (x86-64 only).</p> <p></p>"},{"location":"setup/visualization%20v2/","title":"Khiops Visualization","text":"<p>The Khiops visualization tool is available as a standalone desktop application. You can find all versions on the the following link or download the latest stable version below:</p> <p>            Download for Windows                     Download for Mac OS                     Download for Ubuntu                     Download for CentOS          </p>"},{"location":"setup/visualization%20v2/#introduction","title":"Introduction","text":"<p>The results of the analysis performed by the Khiops Auto-ML pipeline when fitting your data are stored in a dedicated report (with the .khj extension) in a JSON format. The user can parse this report to build custom visualizations, but the most useful insights can be easily accessed through a dedicated interactive visualization interface. The interface is invoked by double-clicking on the icon of a .khj file or by launching the Khiops visualization application and opening the file from the \"File\" menu.</p> <p>The interface is organized as a set of tabs, each showing specific information in dedicated panels. Some panels come with (hopefully) self-explanatory controls; information about a control can be obtained by pointing the mouse to it. The list of controls and their function is given at the end of this document.</p> <p>As supervised and unsupervised (hereafter descriptive) analyses have different goals and produce different kinds of results, their visualization interfaces and tab contents are slightly different.</p>"},{"location":"setup/visualization%20v2/#descriptive-analysis-visualization","title":"Descriptive Analysis Visualization","text":"<p>The report is organized in the following tabs:</p> <ul> <li>Preparation</li> <li>Preparation 2D (only in case of constructed bivariate variables)</li> <li>Project information</li> </ul> <p>Unless otherwise specified, we used the Adult database for the figures below.</p>"},{"location":"setup/visualization%20v2/#preparation-tab","title":"Preparation Tab","text":"<p>The general layout of this tab is as follows (see the two figures below): panel A shows some information related to the data and the analysis (left) and the histogram of the target modalities (right). Panel B stores the list of the variables as a table together with some descriptive statistics information. </p> <p>Clicking on a row of the table in panel B displays the histogram of the corresponding variable in panel C and information about the variable (Name and derivation rule(if any)) in panel D. Clicking on a bin of the histogram displays information about this bin in panel E. </p> <p>Categorical and numerical variables have different histograms.</p> <p>For categorical variables, the number of bins is automatically set to its optimal value according to the MDL criterion. In the case of a variable with many categories, a specific bin (\"Default group index\") may gather many modalities and be shown with a different color. This specific bin is given the name of its most populated modality, and clicking on the bin makes its content available in panel E. As it may gather many small modalities, it may appear as the dominant bin of the distribution, and the name might be misleading, whence the different color. The Coverage/Frequency toggle allows the display of either the coverage (in %, linear scale) or the populations (in log scale).</p> <p></p> <p>The figure above shows the histogram of the variable <code>native_country</code> with 41 modalities. The default group is selected, and its content is displayed in panel E.</p> <p>For numerical variables, the bounds of the bins are also automatically set to their optimal value from an MDL approach.</p> <p></p> <p>Different representations of the histogram are available. The y-axis shows the density and can be plotted in linear or log scale. The x-axis can also be plotted in linear or log scale; for the log scale representation, the variable range is split in (up to) three areas:</p> <ul> <li>the bins in the negative range, excluding any bin containing zero (even as a bound)</li> <li>the bins containing zero (one or two bins, if zero is a bound)</li> <li>the bins in the positive range, excluding any bin containing zero (even as a bound)</li> </ul> <p>The log scale for the negative range is \\(\\(\\log_{10}(-x)\\)\\), running to infinity toward the left. The log scale for the positive range is \\(\\(\\log_{10}(x)\\)\\), running to infinity towards the right. The bins containing zero are given an arbitrary width, set to 1/10 of the figure. This allows the plotting of informative histograms of challenging heavy-tailed distributions such as the Cauchy distribution. </p>"},{"location":"setup/visualization%20v2/#preparation-2d","title":"Preparation 2D","text":"<p>The general layout is similar to the Preparation tab, except for panel C, which shows the bivariate discretization of the pair selected in panel B. This discretization is the Cartesian product of the parts of each variable. By default, the information displayed on the subtab \"Matrix\" is the mutual information between both groupings, allowing to analyze the correlations between the variables of the pair. Other information, such as the frequencies in each cell, can be selected from the menu. The subtab \"Cells\" displays the information about the cell statistics as a table.</p> <p></p>"},{"location":"setup/visualization%20v2/#project-information","title":"Project Information","text":"<p>This tab shows information related to the project, particularly the content of the \"Short description\" field.</p> <p></p>"},{"location":"setup/visualization%20v2/#supervised-analysis","title":"Supervised Analysis","text":"<p>There are two different reports depending on the categorical or numerical nature of the target variable.</p>"},{"location":"setup/visualization%20v2/#classification","title":"Classification","text":"<p>The target variable is categorical.</p> <p>The report is organized in the following tabs :</p> <ul> <li>Preparation</li> <li>Tree Preparation (only in case of constructed decision tree variables)</li> <li>Preparation 2D (only in case of constructed bivariate variables)</li> <li>Modeling</li> <li>Evaluation</li> <li>Project information</li> </ul> <p>For the figures below, we used the Adult database, with \"education\" as a target variable (excluding \"education num\" from the predictors) and randomly selecting 70% of the data for the train set and the remaining 30% for the test set.</p>"},{"location":"setup/visualization%20v2/#preparation","title":"Preparation","text":"<p>Panel A shows some information related to the data and the analysis (left) and the histogram of the target modalities (right). Panel B stores the list of the variables as a table together with some descriptive statistics information.  Clicking on a line of the table in panel B displays the information about the parts obtained from the optimal supervised grouping of the selected variable in panel C and information about the variable (Name and derivation rule(if any)) in panel D. Panel C shows the histogram of the group populations (top) and the distribution of the target modalities conditional to the group (bottom) for each group.</p> <p>Group population histogram can be displayed in linear or log scale from the menu above the figure. For the target modalities conditional to the group histogram, displayed target modalities can be selected from the \"Values\" menu, and the histogram can be displayed as probabilities or lift (\"Probabilities/Lift\" toggle).</p> <p>Clicking on a bin of the population histogram or the corresponding target conditional histogram displays information about the selected group in panel E.</p> <p></p>"},{"location":"setup/visualization%20v2/#tree-preparation","title":"Tree Preparation","text":"<p>Decision trees are treated as categorical variables with as many modalities as terminal leaves. The general layout of panels A, B, and C and the information they display are similar to the Preparation tab. Panel D shows the content of a selected group of leaves as a table of the leaves in the group and their target modalities statistics. Clicking on a leaf displays information about this leaf in panel E: subtab \"Leaf infos\" shows the histogram of the target modalities conditional to this leaf, together with the leaf population and leaf purity; subtab \"Leaf rules\" gives the list of tests leading to this leaf, starting from the root node. The left panel (panel F) is a hypertree visualization of the selected decision tree. Only terminal leaves are clickable; clicking on a leaf dynamically points to the relevant leaf and group of leaves information in the other panels. The population and purity of the leaves can be visualized by their size and color density. This can be toggled on and off, and the \"values\" menu allows to select some of the target modalities only.</p> <p></p>"},{"location":"setup/visualization%20v2/#preparation-2d_1","title":"Preparation 2D","text":"<p>The general layout is similar to the Preparation tab, except for panel C, which shows the bivariate supervised discretization of the pair selected in panel B. This discretization is the Cartesian product of the parts for each variable. By default, the information displayed on the subtab \"Matrix\" is the mutual information between a cell of the Cartesian product and a modality of the target variable (selected from the \"Target\" menu), allowing analysis of the information brought by the variables as a pair to the target variable. Other kinds of information, such as the frequencies in each cell, can be selected from the menu. The subtab \"Cells\" displays the information about the cell statistics as a table. Clicking on a cell displays the target modalities histogram conditional to this cell (bottom of panel C) and the information about the parts of the selected cell in panel E.</p> <p></p>"},{"location":"setup/visualization%20v2/#modeling","title":"Modeling","text":"<p>The general layout is similar to the Preparation tab. Panel B shows information related to those variables used in the model only. The tables show each variable's level, weight (in the weighted naive Bayes model), and importance (computed as the geometric mean of the level and the weight).</p> <p></p>"},{"location":"setup/visualization%20v2/#evaluation","title":"Evaluation","text":"<p>This tab gathers information about the performance of the model. Panel A shows which datasets are concerned by the analysis.  Panel B gives some performance indicators for the model: accuracy, compression rate, the area under the lift curve (AUC), and Gini coefficient. Panel C shows the lift curves: the target modality can be chosen from the menu at the top, and the \"Filter curves\" menu allows to choose which curves are displayed. The lift curve plots the target modality coverage as a function of the population selected according to the model score. Panel D shows a confusion matrix ($modality is the predicted modality). </p> <p></p>"},{"location":"setup/visualization%20v2/#project-information_1","title":"Project Information","text":"<p>This tab shows information related to the project, particularly the content of the \"Short description\" field.</p> <p></p>"},{"location":"setup/visualization%20v2/#regression","title":"Regression","text":"<p>The target variable is numerical. The report is organized in the following tabs :</p> <ul> <li>Preparation</li> <li>Modeling</li> <li>Evaluation</li> <li>Project information</li> </ul> <p>For the figures below, we used the Adult database, with \"education\" as a target variable (excluding \"education num\" from the predictors) and randomly selecting 70% of the data for the train set and the remaining 30% for the test set.</p>"},{"location":"setup/visualization%20v2/#preparation_1","title":"Preparation","text":"<p>Panel A shows some information related to the data and the analysis. Panel B stores the list of the variables as a table together with some descriptive statistics information.  Clicking on a line of the table in panel B displays the information about the parts obtained from the optimal supervised grouping of the selected variable in panel C and information about the variable (Name and derivation rule(if any)) in panel D.</p> <p>The top of Panel C shows the histogram of the group populations obtained from the optimal supervised grouping for the predictive variable. The bottom of panel C shows the optimal grouping of the (predictive variable and target variable) pair.  Note that the discretization of the target variable depends on the predictive variable, which is helpful for interpretation purposes. By default, the mutual information brought by each cell is given (red means positive association and blue, negative association); other kinds of information can be selected from the menu.</p> <p>The toggle Standard/Frequency selects a representation mode: - in \"Standard\" mode, the span of the group is proportional to the number of modalities in the group for categorical variables or constant for numerical variables - in \"Frequency\" mode, the span of a group is proportional to its population.</p> <p>Clicking on a cell of the Cartesian product shows information about the selected group in panel E.</p> <p></p>"},{"location":"setup/visualization%20v2/#modeling_1","title":"Modeling","text":"<p>The general layout is similar to the Preparation tab. Panel B shows information related to those variables used in the model only. The table shows each variable's level, weight (in the weighted naive Bayes model), and importance (computed as the geometric mean of the level and the weight).</p> <p></p>"},{"location":"setup/visualization%20v2/#evaluation_1","title":"Evaluation","text":"<p>This tab gathers information about the performance of the model. Panel A shows which datasets are concerned by the analysis.  Panel B gives some performance indicators for the model: rmse, mae, nlpd (negative log predictive density) and their equivalents for rank regression (e.g. the rank negative log predictive density). Panel C shows the REC curves: the target modality can be chosen from the menu at the top, and the \"Filter curves\" menu allows to choose which curves are displayed.</p> <p></p>"},{"location":"setup/visualization%20v2/#project-information_2","title":"Project Information","text":"<p>This tab shows information related to the project, in particular, the content of the \"Short description\" field.</p> <p></p>"},{"location":"setup/visualization%20v2/#control-elements","title":"Control Elements","text":"<p>Control elements appear at the top right of the panels.</p> <p></p> <p>At the top right of the screen,   from left to right:</p> <ul> <li>allows setting some environment variables such as the number of significant digits or the clear/dark mode. These settings are kept across sessions;</li> <li>allows to copy the figure of a selected panel (select a panel by clicking on it; a selected panel appears with a light-blue border). This can be pasted into any other document;</li> <li>allows the copy of the data of a selected panel.</li> </ul> <p>At the top right of panel A  from left to right:</p> <ul> <li>allows to open a bar plot of the levels of the variables in decreasing order;</li> <li>allows to open a search box on the variable names (fill at least two characters);</li> <li>allows to fit the table to the panel size;</li> <li>allows to fit the columns of the table to their content;</li> <li>allows to select/unselect columns to the table;</li> <li>allows to open the panel in full screen. </li> </ul> <p>The control  at the top right of the full screen, the panel allows to go back to the multi-panel interface.</p> <p>At the top right of panel B  allows to modify the horizontal scale of the histogram (by default, all groups appear in the figure, which may be messy when there are many groups).</p> <p>Additional elements apply to bivariate Cartesian product visualizations.</p> <p></p> <p>On the left side of the panel (in full-screen mode, here), from top to bottom:</p> <ul> <li>allows to zoom in on the figure (horizontal and vertical sliders allow to move in the figure);</li> <li>allows to reset the figure to its original scale;</li> <li>allows to zoom out of the figure;</li> <li>allows to transpose the figure.</li> </ul>"},{"location":"setup/visualization/","title":"Khiops Visualization","text":""},{"location":"setup/visualization/#simplifying-data-visualization","title":"Simplifying Data Visualization","text":"<p>Welcome to the Khiops Visualization tool! It facilitates an in-depth exploration of your data. In a standalone desktop application, it provides an intuitive visualization of all analysis results, ensuring a streamlined interpretation of discovered patterns. Dive deeper into your data, leveraging the outstanding capabilities of Khiops, such as variable encoding, feature engineering, and parsimonious training.</p> <p> Try or Interactive Demo</p>"},{"location":"setup/visualization/#download-the-khiops-visualization-desktop-application","title":"Download the Khiops Visualization Desktop Application","text":"<p>You can find all versions of the Khiops Visualization tool on the the following link or download the latest stable version for your operating system from the URLs shown below:</p> <ul> <li> Windows </li> <li> macOS x86-64   (Intel-based)</li> <li> macOS ARM    ( Apple Silicon - M1/M2/M3)</li> <li> Ubuntu </li> <li> Rocky Linux </li> </ul>"},{"location":"setup/visualization/#download-the-khiops-covisualization-desktop-application","title":"Download the Khiops CoVisualization Desktop Application","text":"<p>For users familiar with Khiops co-clustering, co-clustering analysis results ca be visualized with the CoVisualization desktop application. All versions of this application are available here. The latest stable version can be downloaded from the URLs shown below:</p> <ul> <li> Windows </li> <li> macOS x86-64   (Intel-based)  </li> <li> macOS ARM    ( Apple Silicon - M1/M2/M3) </li> <li> Ubuntu </li> <li> Rocky Linux </li> </ul>"},{"location":"setup/visualization/#documentation","title":"Documentation","text":"<p>For a comprehensive guide on how to use the Khiops Visualization application:</p> <ul> <li>Khiops Visualization Guide, for supervised analysis</li> <li>Khiops Co-Visualization Guide, for unsupervised analysis</li> </ul>"},{"location":"setup/visualization/#screenshots","title":"Screenshots","text":""},{"location":"tutorials/docker/","title":"Running Khiops with Docker","text":"<p>Docker provides a convenient way to run Khiops in a consistent and documented environment, making it easier to manage dependencies and ensure clarity across different systems. This is particularly useful for running Khiops on different operating systems, ensuring consistency across development and production setups.</p> <p>With Khiops official Docker images, you can execute Khiops commands in isolated environments while benefiting from containerization's flexibility and portability. </p>"},{"location":"tutorials/docker/#why-use-docker-for-khiops","title":"Why Use Docker for Khiops?","text":"<p>Using Docker to run Khiops offers several advantages:</p> <ul> <li>No installation required: No need to install dependencies manually\u2014simply pull the Khiops Docker image and start using it;</li> <li>Cross-platform compatibility: Run Khiops consistently on Windows, macOS, and Linux without worrying about system dependencies;</li> <li>Reproducibility: Ensure the same execution environment across different machines and teams;</li> <li>Easy integration into workflows: Docker allows seamless integration into CI/CD pipelines and automation scripts.</li> </ul>"},{"location":"tutorials/docker/#getting-started","title":"Getting Started","text":""},{"location":"tutorials/docker/#base-images","title":"Base Images","text":"<p>You will find two images on dockerhub:</p> <ul> <li>khiopsml/khiops-ubuntu: A minimal installation of Khiops on Ubuntu.</li> <li>khiopsml/khiops-python: The same base image with Khiops-Python preinstalled.</li> </ul>"},{"location":"tutorials/docker/#basic-usage","title":"Basic Usage","text":"<p>By default, the Docker image is configured to launch a Khiops service. You can also run specific commands or use interactive mode. Here\u2019s how to get started:</p> <p>Running a Scenario File</p> <p>To run a specific scenario file located in your local directory, use the following command:</p> <pre><code>docker run -v $PWD:/my_data \\\n  -it khiopsml/khiops-ubuntu\n  khiops -b -i /my_data/my_scenario._kh\n</code></pre> <p>This command mounts your current directory (<code>$PWD</code>) to <code>/my_data</code> in the container and runs the specified scenario file using Khiops.</p> <p>Running a Python Script</p> <p>Similarly, you can use the Python image to run a Khiops-Python script:</p> <pre><code>docker run -v $PWD:/my_volume \\\n  -it khiopsml/khiops-python\n  python /my_volume/script.py\n</code></pre> <p>This command mounts your current directory to <code>/my_volume</code> in the container and executes the specified Python script.</p>"},{"location":"tutorials/docker/#service-usage","title":"Service Usage","text":"<p>You can run the container as a Khiops service, allowing it to process multiple scenarios without restarting the container. Here's how to set it up:</p> <pre><code>docker run -v $PWD:/my_data -p 11000:11000 \\\n  -it khiopsml/khiops-ubuntu\n</code></pre> <p>This command maps port 11000 on your host to the container, enabling you to submit requests to the Khiops service.</p> <p>Submitting a Request</p> <p>To process a scenario, send a POST request to the REST endpoint using standard HTTP tools (cURL, wget, postman...):</p> <pre><code>curl -k -X POST -d \"{\\\"scenario\\\": \\\"/my_data/my_scenario._kh\\\"}\" \"https://localhost:11000/v1/batch\" \\\n -H \"accept: application/json\"\n</code></pre> <p>This command submits a job to the Khiops service to process the specified scenario file. The API definition is available at https://localhost:11000.</p>"},{"location":"tutorials/docker/#conclusion","title":"Conclusion","text":"<p>Using Docker with Khiops simplifies deployment and ensures a consistent environment across different systems. Whether you're running specific commands or using it as a service, Docker provides the flexibility to integrate Khiops seamlessly into your workflow. </p>"},{"location":"tutorials/introduction/","title":"Getting Started with Khiops","text":"<p>Welcome to the practical guide to using Khiops. Whether you are exploring its capabilities for the first time or preparing for industrial-scale deployments, this section will help you understand how Khiops streamlines and enhances your data science workflows.</p> <p>Unlike traditional machine learning libraries, Khiops is built on a unique formalism and advanced automation capabilities that fundamentally reshape the data science process. By automating tedious, repetitive and technically complex steps, Khiops allows users to focus on the core objectives of data science: understanding their data and solving meaningful business problems.</p> <p>At the same time, this singular approach may feel unfamiliar to those accustomed to standard tools and libraries. This section provides a comprehensive introduction to Khiops, enabling you to make the most of its strengths and integrate it into your projects with confidence.</p> <p>Here's what you'll find in this page:</p> <ul> <li>How Khiops accelerates your workflow: Learn how Khiops transforms the traditional data science pipeline by simplifying complex processes and letting you focus on high-value tasks.</li> <li>Choosing the right API: The Scikit-learn-like API is ideal for rapid experimentation and prototyping, while the core API excels in production-scale applications. Determine which best suits your needs.</li> </ul> <p>This introduction will guide you through the foundational points and help you navigate the tutorials that follow. Here's an overview of the sections:</p> <ul> <li>Scikit-learn-like API tutorials: Learn the basics of Khiops with examples on quickstarts, single- and multi-table examples, and hands-on notebooks that showcase Khiops' technical advantages, such as automated data preparation and multi-table processing.</li> <li>Core API &amp; dictionaries: Dive deeper into advanced capabilities, including dictionary usage for scalable, production-ready workflows.</li> <li>Deployment &amp; Integration: Learn how to deploy Khiops models efficiently in real-world environments, with support for cloud storage, containerized execution, and real-time inference.</li> </ul> <p>Whether you\u2019re exploring Khiops for rapid prototyping or integrating it into large-scale industrial workflows, this section will provide you with the necessary guidance to leverage its full potential.</p> <p>Info</p> <p>Questions about deploying Khiops in specific environments (e.g. Hadoop, Openshift, K8s) can be addressed in our Q&amp;A section or through our contact form.</p>"},{"location":"tutorials/introduction/#how-khiops-fits-into-the-data-science-workflow","title":"How Khiops Fits into the Data Science Workflow","text":"<p>Khiops introduces a streamlined and effective approach to data science, simplifying every stage of the process while providing advanced automation and a robust formalism. Unlike traditional tools, Khiops enables you to focus on what truly matters: understanding your data, interpreting insights (the story your data tells), solving business problems, and deploying reliable models. Here's how you can leverage Khiops' unique features step by step:</p> <ul> <li> <p>Skip Data Cleaning and Preparation: Forget about spending hours on cleaning and formatting your data. Khiops reads raw data directly and handles common issues like missing values, inconsistent formats, or noisy inputs. For example, if your dataset contains missing values, Khiops automatically treats them as meaningful signals when training models. It also removes the need for transformations like log scaling or standardization, as its value rank-based encoding is inherently invariant to monotonic transformations.</p> <p>Follow the No Need for Data Preparation tutorial to see this in action.</p> </li> <li> <p>Skip Variable Encoding: Before using variables in a machine learning model, they often need to be transformed into a format the algorithm can process (e.g. categorical variables must be converted into numerical representations). Khiops eliminates this complexity with its MODL formalism, which automatically encodes categorical and numerical variables into statistically optimal groups or intervals.</p> <p>For example, instead of manually binning a variable like <code>age</code>, Khiops will determine ranges like [0, 18], ]18, 35], ]35, 50], etc. These intervals are not arbitrary but are optimally chosen according to the target variable, indeed building a univariate classifier.</p> <p>Explore the Optimal Encoding tutorial and learn more about the concept on the Optimal Encoding foundations page.</p> </li> <li> <p>Skip Feature Engineering: When working with multi-table datasets (arguably the most common scenario in real-world business use cases), feature engineering often becomes one of the most labor-intensive stages. Traditionally, it requires significant domain expertise and trial-and-error to create meaningful features. Khiops automates this process entirely, saving you time and delivering optimal results.</p> <p>Khiops performs feature engineering in a supervised manner, ensuring that new features are relevant to the target variable, with quasilinear complexity that enables scaling efficiently to large datasets. By balancing model complexity with statistical significance, Khiops avoids overfitting while generating informative aggregates.</p> <p>For example, Khiops can automatically calculate metrics like \"total purchases per customer\" or \"average transaction amount per week\" when working with a sales dataset. </p> <p>Explore the Auto Feature Engineering tutorial and learn about the methodology in the dedicated foundations section.</p> </li> <li> <p>Skip Hyperparameter Tuning and Questioning About Overfitting: Traditional machine learning libraries often require time-consuming hyperparameter tuning (learning rates or regularization coefficients) to optimize model performance, and careful regularization to prevent overfitting. Khiops eliminates both concerns thanks to its unique MODL formalism rooted in information theory. It operates without hyperparameters and naturally balances model complexity and information gain (only significant patterns are captured).</p> <p>Explore the MODL formalism and its parsimonious training principles.</p> </li> <li> <p>Evaluate Models with Confidence: Khiops enables you to assess your models with clarity and trust. Thanks to its unique formalism, every transformation (variable encoding or feature engineering) is explicit and interpretable by design. The resulting models are parsimonious, allowing you to understand the precise contribution of each feature to the predictions.</p> <p>Khiops is also robust by nature. If your data lacks meaningful information for the target variable, Khiops won't generate a model, ensuring you can trust the output. This gives you confidence that issues in performance stem from the data, not the modeling process itself.</p> <p>For easy model evaluation, Khiops includes a native visualization tool that helps you interpret your results. With this tool, you can explore lift curves, confusion matrices, and variable importance, gaining clear insights into your model's behavior and reliability.</p> <p>Try the Visualization Tool demo and set it up with these instructions.</p> </li> </ul> <p>With Khiops simplifying every stage of the data science workflow, the next step is choosing the right API for your needs. Whether you're exploring datasets or preparing for industrial-scale deployments, Khiops offers two powerful options: the Scikit-learn-like API and the core API. Let's dive into their differences and find the best fit for your projects.</p>"},{"location":"tutorials/introduction/#two-apis-for-different-needs","title":"Two APIs for different needs","text":"<p>Khiops offers two APIs tailored to different use cases: the Scikit-learn-like API and the core API. While both leverage Khiops' unique strengths, they are optimized for distinct stages of the data science workflow and scaling requirements.</p>"},{"location":"tutorials/introduction/#scikit-learn-like-api-for-quick-prototyping-and-integration","title":"Scikit-learn-like API: For quick prototyping and integration","text":"<p>The scikit-learn-like API is ideal for data scientists familiar with Python and the Scikit-learn ecosystem. It provides an accessible entry point for experimenting with Khiops' key features, including multi-table support and automated feature engineering.</p> Advantages Limitations Familiar syntax: Designed for immediate use with standard Scikit-learn workflows, making onboarding effortless. High I/O requirements: Data loading and processing rely on Python and Pandas, which can be memory-intensive. Ecosystem integration: Acts as standard Scikit-learn estimators, enabling easy integration with other tools (e.g. pyCaret for benchmarking). Scalability constraints: Not optimized for large-scale datasets as it does not support Khiops out-of-core processing. Feature testing: Lets you explore Khiops' multi-table capabilities and auto feature engineering, supporting star or snowflake schemas (with some limitations). Limited support for key Khiops features: limited expressiveness of multi-table schemas and data management capabilities."},{"location":"tutorials/introduction/#core-api-production-ready-and-scalable","title":"Core API: Production-ready and scalable","text":"<p>The core API unleashes the full power of Khiops, offering unmatched scalability and flexibility for industrial-scale projects. Its rich dictionary-based formalism supports complex multi-table databases and facilitates streamlined data management for production use.</p> Advantages Limitations Rich data description: The dictionary formalism provides a structured, detailed way to describe data and especially multi-table relationships, enabling efficient data processing on the fly. Learning Curve: The dictionary formalism introduces new concepts, requiring users to invest time in learning its syntax and structure. Advanced data management: Automates business-level transformations such as aggregate creation, variable selection, and example filtering, all within the API. It can also act as a highly efficient ETL tool. Learning Curve: The core API is tailored to Khiops and thus provides a different experience as compared to Scikit-learn APIs. Facilitated versioning: Dictionaries serve as centralized, versionable configurations for data transformations and model definitions, ensuring traceability. Seamless production deployment: Models trained with the core API are ready for deployment (via the output dictionary file), ensuring robust integration into production workflows. Out-of-core processing: Optimized for hardware resource usage, handling datasets that exceed memory limits efficiently."},{"location":"tutorials/introduction/#conclusion","title":"Conclusion","text":"<p>Choosing the Right API:</p> <ul> <li>Start with the Scikit-learn-like API if you're exploring Khiops' capabilities on small datasets or need a quick, familiar way to test models within the Python ecosystem.</li> <li>Move to the core API for production-grade scalability, complex data management needs, or when working with large, multi-table datasets.</li> </ul>"},{"location":"tutorials/k8s/","title":"Distributed Execution with k8s","text":"<p>Kubernetes is a powerful platform for managing containerized applications, and it's an excellent choice for deploying Khiops in a scalable and distributed manner. One of the most remarkable features of Khiops is its ability to run seamlessly across multiple machines, thanks to its usage of MPI (Message Passing Interface). This means you can effortlessly scale your Khiops deployments using the same docker image across a Kubernetes cluster, making it incredibly straightforward to leverage distributed computing resources.</p> <p>This guide will walk you through the process of setting up Khiops on a Kubernetes cluster using the MPI Operator to handle distributed execution. The setup leverages the official Khiops Docker image, ensuring consistency and simplicity across environments. For more details on using Khiops with Docker, refer to the dedicated page.</p>"},{"location":"tutorials/k8s/#prerequisites","title":"Prerequisites","text":"<p>To run Khiops on Kubernetes, you need to have a Kubernetes cluster up and running. Additionally, you'll need to install the MPI Operator, which simplifies the deployment of MPI (Message Passing Interface) jobs on Kubernetes and manages the provisioning of worker nodes for distributed computing tasks. Follow the official MPI Operator documentation  to install it on your cluster.</p>"},{"location":"tutorials/k8s/#job-definition","title":"Job Definition","text":"<p>Once the MPI Operator is installed, you can define an MPI Job to run Khiops with the desired configuration and resources. Below is an example of a Kubernetes manifest for an MPI Job that runs the <code>khiops -s</code> command which displays the allocated resources as seen by the Khiops program:</p> <pre><code>apiVersion: kubeflow.org/v2beta1\nkind: MPIJob\nmetadata:\n  name: khiops\nspec:\n  slotsPerWorker: 4\n  runPolicy:\n    cleanPodPolicy: Running\n    ttlSecondsAfterFinished: 3600\n  sshAuthMountPath: /home/ubuntu/.ssh\n  mpiReplicaSpecs:\n    Launcher:\n      replicas: 1\n      template:\n        spec:\n          containers:\n          - image: khiopsml/khiops-ubuntu\n            name: mpi-launcher\n            securityContext:\n              runAsUser: 1000\n            args:\n            - khiops\n            - -s\n            resources:\n              limits:\n                cpu: 1\n                memory: 512Mi\n    Worker:\n      replicas: 2\n      template:\n        spec:\n          containers:\n          - image: khiopsml/khiops-ubuntu\n            name: mpi-worker\n            securityContext:\n              runAsUser: 1000\n            args:\n            - /usr/sbin/sshd\n            - -De\n            - -f\n            - /home/ubuntu/.sshd_config\n            env:\n            - name: KHIOPS_MEMORY_LIMIT\n              valueFrom:\n                resourceFieldRef:\n                  divisor: \"1Mi\"\n                  resource: requests.memory\n            readinessProbe:\n              tcpSocket:\n                port: 2222\n              initialDelaySeconds: 4\n            resources:\n              requests\n                cpu: 4\n                memory: 4Gi\n</code></pre>"},{"location":"tutorials/k8s/#explanation","title":"Explanation","text":"<ul> <li>slotsPerWorker: Defines the number of processes per worker pod (must be set to at least 2 for Khiops to function correctly). It's recommended to set this equal to the number of CPU cores requested for optimal performance. In the example, each of the 2 worker pods runs 4 virtual cpus, totaling 8 processes.</li> <li>runPolicy: Specifies policies for cleaning up pods and setting a time-to-live for finished jobs.</li> <li>mpiReplicaSpecs: Defines the specifications for the launcher and worker pods. The launcher initiates the job, while workers perform the computations.</li> </ul>"},{"location":"tutorials/k8s/#customizing-the-job","title":"Customizing the Job","text":"<p>To tailor the Khiops job to your specific needs, you can customize the MPI Job definition.</p> <p>If you have built a custom Docker image that includes a Python script or other custom logic, replace the container image of the launcher with your custom image. The workers will continue to use the standard Khiops image (<code>khiopsml/khiops-ubuntu</code>), as they function as pure Khiops slaves.</p> <p>Example:</p> <pre><code>mpiReplicaSpecs:\n  Launcher:\n    template:\n      spec:\n        containers:\n        - image: my-custom-khiops-python-image\n          name: mpi-launcher\n          args:\n          - python\n          - my_script.py\n</code></pre> <p>If your job needs to access data stored in cloud storage solutions like Amazon S3, you can provide the necessary credentials in two ways: </p> <ul> <li>Environment Variables: Pass the credentials as environment variables in the job definition.</li> <li>Mounted Volume: Mount a volume containing the standard AWS configuration files (config and credentials) to the pods.</li> </ul>"},{"location":"tutorials/k8s/#launching-the-job","title":"Launching the Job","text":"<p>Save the manifest to a file (e.g., <code>khiops_job.yaml</code>) and apply it to your Kubernetes cluster:</p> <pre><code>kubectl apply -f khiops_job.yaml\n</code></pre> <p>The MPI Operator will handle launching the pods and interconnecting them. Once ready, the launcher will start the execution, and you can monitor the job's progress using Kubernetes tools like <code>kubectl get pods</code> or <code>kubectl logs</code>.</p>"},{"location":"tutorials/k8s/#conclusion","title":"Conclusion","text":"<p>Running Khiops on Kubernetes with the MPI Operator allows you to leverage the power of distributed computing for large-scale data processing tasks. This setup ensures that your Khiops jobs are scalable, efficient, and integrated seamlessly into your Kubernetes workflows.</p>"},{"location":"tutorials/kdic_intro/","title":"Start Using Dictionaries","text":""},{"location":"tutorials/kdic_intro/#benefits-of-using-khiops-dictionaries","title":"Benefits of Using Khiops Dictionaries","text":"<p>Using the Khiops Core API, particularly through the manipulation of Khiops dictionaries, significantly accelerates the deployment of machine learning models into production. By enabling users to easily describe their data and incorporate business knowledge during the data preparation step, dictionaries streamline workflows and enhance efficiency. Moreover, they serve as a comprehensive record of all manual and automated processing steps required for model training, and are executed seamlessly by Khiops during inference.</p> <p>The main user benefits include:</p> <ul> <li> <p>Meaningful data description: In its simplest form (a single training data table), the dictionary describes the variables, with their names and types. For more complex data spread across multiple tables, the dictionary also encodes relationships by linking tables, treating a secondary table as a \"variable\" of the table it is linked to. This approach allows users to easily define and structure data into a comprehensible relational schema, even for advanced setups such as snowflake schemas (see examples in the multi-table learning page).</p> </li> <li> <p>Meaningful data preparation: The user simply expresses their business knowledge by selecting the relevant information for the problem at hand and enriching data with relevant user-defined variables. These enrichment rules are similar to SQL expressions, allowing users to define new variables with intuitive logic (e.g. using the mathematical rules described here). </p> </li> </ul> <p>During training, the dictionary provided by the user is automatically enriched to implement the entire data transformation flow of the machine learning pipeline, from mapping available data sources to predicting the target variable. For instance, during the automated feature engineering phase, new aggregates are generated and added to the dictionary as variables (the predefined functions used to create these aggregates are listed here).</p> <p>The main technical benefits include:</p> <ul> <li> <p>Agile model release: A single dictionary file encodes an entire data transformation flow, enabling predictions to be made directly from the raw data. Furthermore, updating a model simply involves replacing a file, what ensures straightforward versioning, traceability, and easy rollbacks using standard tools like <code>git</code>.</p> </li> <li> <p>On-the-fly processing: Data transformation is implemented dynamically, which means that variables are computed only when required during execution, rather than being precomputed and stored. For instance, the aggregates defined during the Auto Feature Engineering step are instantiated just-in-time for predictions. This minimizes RAM usage, avoids storage overhead, and ensures scalability for large-scale datasets. </p> </li> <li> <p>Distributed computing and Out-of-core: Khiops' low-level implementation ensures data transformations are executed with exceptional efficiency, dynamically adapting to available hardware resources. The initial learning task is divided into sub-tasks, either due to limited RAM (out-of-core processing sequentially loads sub-parts of the data) or to distribute processing across a cluster of computers. This makes Khiops not only a powerful modeling tool but also a lightweight solution for scalable data transformation.</p> </li> </ul>"},{"location":"tutorials/kdic_intro/#what-is-a-khiops-dictionary","title":"What Is a Khiops Dictionary?","text":"<p>A dictionary is a program that describes and transforms data</p> <p>Dictionaries are written using a declarative programming language dedicated to both data definition and manipulation. Thus, a dictionary includes a specification of the data itself (i.e. definition), as well as the transformations to be applied (i.e. manipulation).  </p>"},{"location":"tutorials/kdic_intro/#how-are-dictionaries-used","title":"How Are Dictionaries Used?","text":"<p>This section illustrates how dictionaries are used throughout the machine learning pipeline: (i) who writes the dictionaries; (ii) what are they used for.   </p>"},{"location":"tutorials/kdic_intro/#data-description-and-preparation","title":"Data Description and Preparation","text":"<p>In the standard case of machine learning, where training data is encoded in a single-table format, the dictionary file provided by the user takes its simplest form, just describing the variables with their names and types.</p> <p>Example of a simple dictionary describing a single training table</p> <pre><code>Dictionary  iris\n{\n    Numerical   SepalLength ;\n    Numerical   SepalWidth      ;\n    Numerical   PetalLength     ;\n    Numerical   PetalWidth      ;\n    Categorical Class   ;\n};\n</code></pre> <p>However, in most industrial applications, raw data is scattered across multiple, loosely organized sources. In these cases, dictionaries play a crucial role in organizing the data. To be read by Khiops, these sources must provide formatted data, typically CSV, log or text files. At this stage, the goal is to produce structured and enriched data that reflects business knowledge and is comprehensible to stakeholders.    </p> <p> </p> <p>What the user writes \u270d\ufe0f</p> <p>The user writes an initial dictionary to describe and prepare the data:</p> <ul> <li> <p>Description: (i) Mapping available data sources to tables, (ii) selecting relevant tables and variables, (iii) defining relationships between tables, and (iv) specifying variable types.</p> </li> <li> <p>Manipulation: (i) Selecting training examples (e.g., filtering by time period), and (ii) coding new user-defined variables to integrate business knowledge.</p> </li> </ul>"},{"location":"tutorials/kdic_intro/#training-stage","title":"Training Stage","text":"<p>During training, the machine learning pipeline is executed in two stages. First, pre-processing is performed, including both auto feature engineering and optimal encoding. Then, the parsimonious training stage combines the univariate pre-processing steps to obtain a multivariate predictive model.   </p> <p>1) Pre-processing</p> <p> </p> <p>What Khiops writes automatically \u270d\ufe0f</p> <p>The dictionary initially provided by the user is automatically enriched in order to (i) compute a large number of informative aggregate variables, and (ii) to define the rules used to encode each variable into intervals or groups.</p> <p>2) Parsimonious training</p> <p> </p> <p>What Khiops continues to write \u270d\ufe0f</p> <p>Finally, this dictionary is modified (i) to select a subset of informative and independent variables, and (ii) to compute target predictions based on the encoded variables. </p>"},{"location":"tutorials/kdic_intro/#model-release-in-production-stage","title":"Model Release in Production Stage","text":"<p>At the end of training, predictions can be made directly from raw data, which facilitates model productionalization in two aspects. </p> <ul> <li> <p>Updating a model involves simply replacing the dictionary file, as long as the raw data format remains unchanged, without changing anything else in the project. </p> </li> <li> <p>The trained model's archive includes the complete data transformation flow, eliminating the need to maintain external pre-processing code and mitigating risks from versioning conflicts. </p> </li> </ul> <p> </p> <p>What the user and Khiops wrote together</p> <p>The final dictionary integrates both the data preparation provided by the user and the data transformation flow applied by Khiops during training.</p>"},{"location":"tutorials/kdic_multi_table/","title":"Multi-Table Concepts","text":"<p>This section introduces the use of Khiops dictionaries for managing data preparation with multi-table datasets, a frequent scenario in real-world business applications. Khiops eliminates the need for labor-intensive pre-processing and manual handling of relationships between tables, offering a scalable and automated solution for relational data. For full documentation, please refer to the dictionaries reference page.</p>"},{"location":"tutorials/kdic_multi_table/#relational-data-description","title":"Relational Data Description","text":"<p>When working with relational (multi-table) data, a key step is defining relationships between tables and their respective variables. Traditional libraries require manual table joins, ad-hoc data management, and domain expertise to prepare the data for analysis. This is time-consuming, error-prone, and becomes infeasible for large-scale datasets (which occurs more rapidly with such multi-table datasets).</p> <p>Khiops uses a dictionary-based language that naturally describes the relational schema without flattening the data. Relationships between tables are encoded directly in the dictionary, enabling efficient and interpretable processing. Key benefits include:</p> <ul> <li>Scalability: Process large datasets without loading tables into memory, leveraging I/O optimizations, out-of-core and distributed processing.</li> <li>Streamlined Versioning: End-to-end data transformations are recorded in the dictionary, eliminating the need for versioning large intermediary datasets.</li> </ul>"},{"location":"tutorials/kdic_multi_table/#simple-star-relational-schemas","title":"Simple Star Relational Schemas","text":"<p>Let's start with an example of a star schema describing the customers of a company, their addresses, and the services they use:</p> <p><pre><code>    Customer\n    |\n    +-- Address\n    |\n    +-- Services\n</code></pre> <code>Customer</code> designates the data type of the main statistical units under study, <code>Address</code> and <code>Services</code> correspond to the types of their secondary records.</p> <p>The corresponding Khiops dictionary is:</p> <p>Example: Dictionary for a star schema</p> <pre><code>Dictionary Customer (customer_id)\n{\n    Categorical customer_id;\n    Numerical age;\n    Categorical sex;\n    Categorical marketingSegment;\n    Entity(Address) customerAddress;  // 0-1 relationship\n    Table(Services) customerServices; // 0-n relationship\n};\n\nDictionary Address (customer_id)\n{\n    Categorical customer_id;\n    Categorical streetNumber;\n    Categorical streetName;\n    Categorical city;\n    Categorical zipcode;\n    Categorical State;\n};\n\nDictionary Services (customer_id)\n{\n    Categorical customer_id;\n    Categorical name;\n    Numerical cost;\n    Date purchaseDate;\n};\n</code></pre> <p>In this example:</p> <ul> <li>Each <code>Dictionary</code> corresponds to a user-defined data structure (<code>Customer</code>, <code>Address</code>, <code>Services</code>), effectively serving as a data type;</li> <li>The categorical variable <code>customer_id</code> acts as the key, uniquely identifying the statistical units in the main table (i.e., <code>Customer</code>) and linking their associated records in the secondary tables (i.e., <code>Services</code> and <code>Address</code>).</li> <li><code>Entity(Address)</code> designates a 0-1 relationship (e.g., each customer has one address or none).</li> <li><code>Table(Services)</code> designates a 0-N relationship (e.g., each customer can have multiple services). Each statistical unit in the main table <code>Customer</code> refers to a set of records in the secondary table <code>Services</code>.</li> </ul>"},{"location":"tutorials/kdic_multi_table/#snowflake-relational-schemas","title":"Snowflake Relational Schemas","text":"<p>This example extends the previous star schema by introducing a snowflake relational schema, where secondary tables are expanded with additional hierarchical levels. In this case, the schema includes a new <code>Usages</code> table, which describes how customers use specific <code>Services</code>.</p> <pre><code>    Customer\n    |\n    +-- Address\n    |\n    +-- Services\n        |\n        +-- Usages\n</code></pre> <p>Note that a customer can use each service several times (relation 0-N). This additional complexity reflects real-world scenarios, such as tracking multiple transactions or interactions linked to a single entity.</p> <p>Here's the dictionary file describing this relational data:</p> <p>Example: Dictionary file of a snowflake relational schema</p> <pre><code>Dictionary Customer (customer_id)\n{\n    Categorical customer_id;\n    Numerical age;\n    Categorical sex;\n    Categorical marketingSegment;\n    Entity(Address) customerAddress;  // 0-1 relationship\n    Table(Services) customerServices; // 0-n relationship\n};\n\nDictionary Address (customer_id)\n{\n    Categorical customer_id;\n    Categorical streetNumber;\n    Categorical streetName;\n    Categorical city;\n    Categorical zipcode;\n    Categorical State;\n};\n\nDictionary Services (customer_id, service_id)\n{\n    Categorical customer_id;\n    Categorical service_id;\n    Categorical name;\n    Numerical cost;\n    Date purchaseDate;\n    Table(Usages) serviceUsages; // 0-n relationship\n};\n\nDictionary Usages (customer_id, service_id)\n{\n    Categorical customer_id;\n    Categorical service_id;\n    Categorical usageType;\n    Date date;\n    Time time;\n    Numerical duration;\n};\n</code></pre> <p>The only new syntax feature in this example is the use of a multiple-field key in the <code>Services</code> and <code>Usages</code> dictionaries. These keys allow Khiops to efficiently associate multiple levels of data, such as linking usage records to specific services used by specific customers. As the number of table levels increases in a snowflake schema, the identification key becomes longer, consisting of a concatenation of multiple identifier variables (e.g., <code>customer_id</code> and <code>service_id</code>).</p> <p>This structure enables hierarchical feature engineering. For example, Khiops can compute aggregates like \"Total usage duration per customer\" by traversing multiple levels.</p>"},{"location":"tutorials/kdic_multi_table/#snowflake-schema-with-external-tables","title":"Snowflake Schema with External Tables","text":"<p>This example introduces the concept of an external table. External tables are used to enrich descriptive variables in other tables without duplicating information. Unlike standard secondary tables, external tables are not directly linked to the main statistical units (e.g., <code>customer_id</code>) but instead provide additional descriptive information for a specific variable.</p> <p>For instance, the <code>City</code> table in the following schema adds information such as the city name, country, and time zone to the <code>Address</code> table, without repeating the same city details across multiple rows. Instead, the <code>Address</code> table references the <code>City</code> table using the <code>zipcode</code> key. This approach ensures efficiency and consistency, especially in large datasets.</p> <p>The relational schema is structured as follows: <pre><code>    Customer\n    |\n    +-- Address\n    |   |\n    |   +-- City\n    |\n    +-- Services\n        |\n        +-- Usages\n</code></pre></p> <p>Here's the dictionary file that defines this relational schema:</p> <p>Example: Dictionary file for a snowflake schema with an external table</p> <pre><code>Root Dictionary Customer (customer_id)\n{\n    Categorical customer_id;\n    Numerical age;\n    Categorical sex;\n    Categorical marketingSegment;\n    Entity(Address) customerAddress; // 0-1 relationship\n    Table(Services) services;        // 0-n relationship\n};\n\nDictionary Address (customer_id)\n{\n    Categorical customer_id;\n    Categorical streetNumber;\n    Categorical streetName;\n    Categorical zipcode;\n    Entity(City) city[zipcode];\n};\n\nRoot Dictionary City (zipcode)\n{\n   Categorical zipcode;\n   Categorical name;\n   Categorical country;\n   Categorical timeZone;\n};\n\nDictionary Services (customer_id, service_id)\n{\n    Categorical customer_id;\n    Categorical service_id;\n    Categorical name;\n    Numerical cost;\n    Date purchaseDate;\n    Table(Usage) Usages; // 0-n relationship\n};\n\nDictionary Usage (customer_id, service_id)\n{\n    Categorical customer_id;\n    Categorical service_id;\n    Categorical usageType;\n    Date date;\n    Time time;\n    Numerical duration;\n};\n</code></pre> <p>An external table behaves like an Entity, creating a 0-1 relationship. The syntax starts with <code>Entity(City)</code>. And because it provides additional information for a specific descriptive variable rather than statistical units, square brackets <code>[zipcode]</code> are used to specify the linking key.</p> <p>The <code>Root</code> keyword is required for external tables, as it indicates that the key variable (e.g., zip code) uniquely identifies the records in that table. For the <code>Customer</code> table, the <code>Root</code> keyword is optional and simply tags the main dictionary representing the statistical units under study.</p>"},{"location":"tutorials/kdic_multi_table/#filtering-out-of-scope-tables-and-variables","title":"Filtering Out-of-Scope Tables and Variables","text":"<p>In data management, defining the scope of an analysis is crucial to focus on the most relevant pieces of information while avoiding the overhead of processing unnecessary data. For multi-table datasets, this involves filtering out tables and variables that do not contribute to the analysis.</p> <p>With conventional tools, data scientists often load entire datasets into memory before manually discarding irrelevant variables or tables. This approach is manageable for small datasets but becomes inefficient and costly for larger, multi-table datasets. It can also lead to excessive trial-and-error runs, creating versioning challenges and increasing storage costs, especially in cloud environments.</p> <p>Khiops dictionaries provide a more efficient solution by enabling on-the-fly filtering. Only the relevant columns and tables are loaded into memory during processing. This eliminates unnecessary overhead and makes dictionaries lightweight and easy to version, offering a scalable alternative for industrial use cases.</p> <p>The <code>Unused</code> keyword in Khiops dictionaries allows you to specify variables and tables that should be excluded from analysis. Here's an example:</p> <p>Example: Filtering with the Unused keyword</p> <pre><code>Dictionary Customer (customer_id)\n{\n    Categorical customer_id;\n    Numerical age;\n    Categorical sex;\n    Categorical marketingSegment;\n    Unused Entity(Address) customerAddress; // Unused entity\n    Table(Services) customerServices;\n};\n\nDictionary Address (customer_id)\n{\n    Categorical customer_id;\n    Categorical streetNumber;\n    Categorical streetName;\n    Categorical city;\n    Categorical zipcode;\n    Categorical State;\n};\n\nDictionary Services (customer_id)\n{\n    Categorical customer_id;\n    Categorical name;\n    Unused Numerical cost; // Unused variable\n    Date purchaseDate;\n};\n</code></pre>"},{"location":"tutorials/kdic_multi_table/#user-defined-relational-schema","title":"User-Defined Relational Schema","text":"<p>In many cases, databases are designed with technical constraints in mind (e.g., optimizing query speed or storage efficiency), often at the expense of usability for business experts. As a result, the stored data can be difficult to interpret or align with the experts' knowledge of the problem. Reorganizing this data to better reflect the business context is a critical step in data management.</p> <p>Traditionally, this process involves manually coding project-specific data management workflows - a time-consuming and error-prone task, compounded by the challenges of versioning multiple scripts and iterations.</p> <p>Khiops dictionaries offer a powerful alternative. By using their built-in data manipulation language (see the reference page), you can streamline this process and encode business knowledge directly into a reusable, versionable format. This not only simplifies data management but also ensures alignment between technical implementation and business understanding.</p> <p>The next three subsections illustrate how dictionaries can facilitate data manipulation through:</p> <ul> <li>Redefining the scope of statistical units;</li> <li>Concatenating tables;</li> <li>Advanced selection of training examples.</li> </ul>"},{"location":"tutorials/kdic_multi_table/#redefining-the-scope-of-statistical-units","title":"Redefining the Scope of Statistical Units","text":"<p>In many real-world applications, the statistical units of interest are not directly stored in the database and need to be derived from raw data. For instance, it might be necessary to eliminate noisy or insignificant records to focus on meaningful information. This process is essential for reducing complexity, saving computational resources, and ensuring the analysis is precise.</p> <p>Khiops makes this process seamless using the <code>TableSelection</code> function, which allows users to filter secondary records based on specified criteria, directly within the dictionary. This eliminates the need for pre-processing steps in Python or SQL, thus enabling scalable and efficient data management.</p> <p>Example 1: Filter by Duration of Use</p> <p>The following example redefines the scope of statistical units by selecting only the records in the <code>Usage</code> table where the duration exceeds ten minutes <code>(GE(duration,10))</code>:</p> <p>The TableSelection function applied to secondary records</p> <pre><code>Root Dictionary Customer (customer_id)\n{\n    Categorical customer_id;\n    Numerical age;\n    Categorical sex;\n    Categorical marketingSegment;\n    Table(Services) services;\n};\n\nDictionary Services (customer_id, service_id)\n{\n    Categorical customer_id;\n    Categorical service_id;\n    Categorical name;\n    Numerical cost;\n    Date purchaseDate;\n    Unused Table(Usage) allServiceUsages;\n\n    // selection of usages &gt; 10 minutes\n    Table(Usage) studiedUsages = TableSelection(allServiceUsages, GE(duration,10));\n};\n\nDictionary Usage (customer_id, service_id)\n{\n    Categorical customer_id;\n    Categorical service_id;\n    Categorical usageType;\n    Date date;\n    Time time;\n    Numerical duration;\n};\n</code></pre> <p>This selection rule function identifies and retains only the relevant records from the <code>Usage</code> table, streamlining the analysis process by discarding unnecessary data.</p> <p>Notice that in the rule <code>TableSelection(.,.)</code>, the first operand designates the name of a table and the second operand is a selection rule applied in the scope of this table.</p> <p>Example 2: Filter by Time Relative to a Parent Variable</p> <p>In more advanced scenarios, filtering can require conditions that depend on variables from a parent table. For instance, the following example filters records from the <code>Usage</code> table to include only those occurring within 30 days of the <code>purchaseDate</code> from the <code>Services</code> table:</p> <p>The . scope operator</p> <pre><code>Root Dictionary Customer (customer_id)\n{\n    Categorical customer_id;\n    Numerical age;\n    Categorical sex;\n    Categorical marketingSegment;\n    Table(Services) services;\n};\n\nDictionary Services (customer_id, service_id)\n{\n    Categorical customer_id;\n    Categorical service_id;\n    Categorical name;\n    Numerical cost;\n    Date purchaseDate;\n    Unused Table(Usage) allServiceUsages;\n\n    // selection of usages within 30 days after subscription\n    Table(Usage) studiedUsages = TableSelection(\n        allServiceUsages, LE(DiffDate(date, .purchaseDate),30)\n    );\n};\n\nDictionary Usage (customer_id, service_id)\n{\n    Categorical customer_id;\n    Categorical service_id;\n    Categorical usageType;\n    Date date;\n    Time time;\n    Numerical duration;\n};\n</code></pre> <p>In this example, the second operand in the rule <code>LE(DiffDate(date, .purchaseDate), 30)</code> operates within the scope of the <code>Usage</code> dictionary, where it directly uses the <code>date</code> variable. It also accesses the <code>purchaseDate</code> variable from the parent <code>Services</code> dictionary using the <code>.</code> scope operator to establish the relationship between the two tables.</p>"},{"location":"tutorials/kdic_multi_table/#table-concatenation","title":"Table Concatenation","text":"<p>In many production environments, large data tables are often divided into multiple chunks for easier storage and management. This is especially common for secondary tables with a high volume of records, such as log data or transactional details. While this practice improves storage efficiency and system performance, it introduces the challenge of reconstructing the original table for analysis or model training.</p> <p>Khiops provides an efficient solution for this scenario through the <code>TableUnion</code> rule, which allows you to seamlessly concatenate these chunks. In the example below, the <code>Usage</code> table is divided into quarterly chunks (<code>usagesQuarter1</code>, <code>usagesQuarter2</code>, etc.). These chunks are unified into a single table using <code>TableUnion</code>. To prevent duplicates during analysis, the <code>Unused</code> keyword is applied to each chunk sub-table, as the concatenated table already contains all the records.</p> <p>The TableUnion function</p> <pre><code>Root Dictionary Customer (customer_id)\n{\n    Categorical customer_id;\n    Numerical age;\n    Categorical sex;\n    Categorical marketingSegment;\n    Table(Services) services;\n};\n\nDictionary Services (customer_id, service_id)\n{\n    Categorical customer_id;\n    Categorical service_id;\n    Categorical name;\n    Numerical cost;\n    Date purchaseDate;\n    Unused Table(Usage) usagesQuarter1;\n    Unused Table(Usage) usagesQuarter2;\n    Unused Table(Usage) usagesQuarter3;\n    Unused Table(Usage) usagesQuarter4;\n\n    // Concatenation of 4 files, divided for volume purposes\n    Unused Table(Usage) allUsages =\n      TableUnion(usagesQuarter1, usagesQuarter2, usagesQuarter3, usagesQuarter4);\n\n    // Selection of usages &gt; 10 minutes\n    Table(Usage) studiedUsages = TableSelection(allUsages, GE(duration,10));\n};\n\nDictionary Usage (customer_id, service_id)\n{\n    Categorical customer_id;\n    Categorical service_id;\n    Categorical usageType;\n    Date date;\n    Time time;\n    Numerical duration;\n};\n</code></pre>"},{"location":"tutorials/kdic_multi_table/#advanced-example-selection","title":"Advanced Example Selection","text":"<p>The selection of training examples is an important step in data management, which allows users to define the scope of the analysis. In the case of multi-table data, this step can become complex, requiring laborious manual work and coding. Here again, dictionaries offer an effective alternative, and greatly facilitate versioning.</p> <p>The following example shows the selection of a particular marketing segment consisting of housewives under 50 who have used the VOD service at least 10 times. As this selection criterion is complex, it is written on several lines:</p> <ul> <li>condition 1: selection of customers &lt; 50 years old</li> <li>condition 2: selection of women</li> <li>condition 3: selection of services that are both (i) VOD and (ii) with more than 10 uses</li> </ul> <p>For full documentation on the dictionary language, please refer to the reference page.</p> <p>Complex criteria for selecting examples</p> <pre><code>Root Dictionary Customer (customer_id)\n{\n    Categorical customer_id;\n    Numerical age;\n    Categorical sex;\n    Categorical marketingSegment;\n    Table(Services) services;\n\n    // Selection of women under 50 that have used VOD at least 10 times.\n    // This complex selection criterion is written on three conditions:\n    // - condition 1: selection of customers &lt; 50 years old, AND\n    // - condition 2: selection of women, AND\n    // - condition 3: selection of services that:\n    //   - sub-condition 3-1: service is VOD, AND\n    //   - sub-condition 3-2: have at least 10 uses\n    Unused Numerical selectionVariable = And(\n        L(age, 50),     // condition 1\n        EQc(sex, \"F\"),  // condition 2\n        // condition 3\n        EQ(\n            TableCount(TableSelection(\n                services,\n                And(\n                    EQc(name, \"VOD\"),                    // sub-condition 3-1\n                    GE(TableCount(allServiceUsages), 10) // sub-condition 3-2\n                )\n            )),\n            1\n        )\n        // end of condition 3\n    );\n};\n\nDictionary Services (customer_id, service_id)\n{\n    Categorical customer_id;\n    Categorical service_id;\n    Categorical name;\n    Numerical cost;\n    Date purchaseDate;\n    Table(Usage) allServiceUsages;\n};\n\nDictionary Usage (customer_id, service_id)\n{\n    Categorical customer_id;\n    Categorical service_id;\n    Categorical usageType;\n    Date date;\n    Time time;\n    Numerical duration;\n};\n</code></pre>"},{"location":"tutorials/kdic_single_table/","title":"Single-Table Concepts","text":"<p>This section introduces the use of dictionaries to easily implement the data management steps of a typical data science project, where the training data consists of a single table. For full documentation, please refer to the dictionaries reference page.</p>"},{"location":"tutorials/kdic_single_table/#data-description","title":"Data Description","text":"<p>One of the very first steps in data management is to specify the types of each variable in the training table, to ensure that they will be processed correctly in the rest of the pipeline.</p> <p>In prototyping, the usual practice of data scientists is to first load data into memory (e.g. as a Pandas Dataframe) and check the types automatically inferred. While this approach works for small datasets, it becomes inefficient for large-scale datasets in production contexts. This is where Khiops dictionaries offer a much more efficient alternative.</p> <p>A dictionary is a standalone file that specifies variable types and is read alongside the data during processing. This eliminates the need to load data into memory upfront and enables advanced features such as:</p> <ul> <li>I/O optimization;</li> <li>Out-of-core processing for handling datasets that exceed memory limits;</li> <li>Distributed processing for scaling across multiple machines.</li> </ul> <p>Additionally, dictionaries enable robust error handling. By providing a data specification beforehand, Khiops can:</p> <ul> <li>Replace on the fly unreadable values with missing values (which Khiops can leverage effectively to extract meaningful insights, as shown in this tutorial);</li> <li>Ignore records with an incorrect number of fields, avoiding column shifts and ensuring consistent data.</li> </ul> <p>This approach allows Khiops to process even poor-quality data sources reliably, while preserving the integrity and value of subsequent computations.</p> <p>Here's an example of a dictionary for the Iris dataset:</p> <p>Example: A simple dictionary for the Iris dataset</p> <pre><code>Dictionary Iris\n{\n    Numerical SepalLength ;\n    Numerical SepalWidth  ;\n    Numerical PetalLength  ;\n    Numerical PetalWidth  ;\n    Categorical Class ;\n};\n</code></pre> <ul> <li><code>Iris</code> indicates the name of the dictionary;</li> <li><code>SepalLength</code>, <code>SepalWidth</code> ... <code>Class</code> indicate variable names;</li> <li>The keywords <code>Numerical</code> and <code>Categorical</code> define the corresponding variable types.</li> </ul> <p>Dictionaries also enable advanced and flexible management of time variables, with support for numerous types (i.e. <code>Date</code>, <code>Time</code>, <code>Timestamp</code>, <code>TimestampTZ</code>, and custom time formats). Detailed descriptions of these types are provided on the dictionary reference page.</p>"},{"location":"tutorials/kdic_single_table/#programmatic-manipulation-with-the-core-api","title":"Programmatic Manipulation with the Core API","text":"<p>Khiops dictionaries can be manipulated programmatically using the Khiops Python library, via the Core API. This includes checking the consistency of a dataset given a dictionary.</p> <p>Example: Checking a database using the Core API</p> <pre><code># Imports\nimport os\nfrom khiops import core as kh\n\n# Set the file paths\ndictionary_file_path = os.path.join(kh.get_samples_dir(), \"Iris\", \"Iris.kdic\")\ndata_table_path = os.path.join(kh.get_samples_dir(), \"Iris\", \"Iris.txt\")\nlog_file = os.path.join(\"kh_samples\", \"check_database\", \"check_database.log\")\n\n# Check the database\nkh.check_database(\n    dictionary_file_path,\n    \"Iris\",\n    data_table_path,\n    log_file_path=log_file,\n    max_messages=50,\n)\n</code></pre> <p>Khiops also offers automatic type extraction from the training data file, as shown in the following example:</p> <p>Example: Building a dictionary from a data table</p> <pre><code># Imports\nimport os\nfrom khiops import core as kh\n\n# Set the file paths\ndata_table_path = os.path.join(kh.get_samples_dir(), \"Iris\", \"Iris.txt\")\ndictionary_name = \"AutoIris\"\ndictionary_file_path = os.path.join(\n    \"kh_samples\", \"build_dictionary_from_data_table\", \"AutoIris.kdic\"\n)\n\n# Create the dictionary from the data table\nkh.build_dictionary_from_data_table(\n    data_table_path, dictionary_name, dictionary_file_path\n)\n</code></pre>"},{"location":"tutorials/kdic_single_table/#filtering-out-of-scope-variables","title":"Filtering Out-of-Scope Variables","text":"<p>Another important step in data management is to define the scope of the analysis. Often, the available data contains variables that are irrelevant to the task at hand and need to be filtered out. In the case of single-table training data, this involves selecting only the variables that are relevant to the analysis, while ignoring the rest.</p> <p>Rather than loading the entire dataset into memory and manually dropping unnecessary columns, as it is common in small-scale prototyping, dictionaries enable a more efficient approach. Khiops directly filters out unwanted variables during the data reading phase. By specifying these variables as <code>Unused</code> in the dictionary, only the relevant columns are loaded into memory, regardless of the dataset size. This avoids excessive RAM usage and simplifies workflows, especially when trial-and-error is needed during data preparation.</p> <p>Using dictionaries also minimizes storage overhead. Instead of versioning large datasets for each modification, you can simply version the lightweight dictionary file, which encodes all filtering logic. This is particularly cost-effective in cloud environments where storage can be expensive.</p> <p>The following example shows how the <code>Unused</code> keyword can be used in dictionaries to filter out irrelevant variables:</p> <p>Example: Using the <code>Unused</code> keyword in a dictionary</p> <pre><code>Dictionary Iris\n{\n    Unused Numerical SepalLength ;\n    Unused Numerical SepalWidth  ;\n    Numerical PetalLength  ;\n    Numerical PetalWidth  ;\n    Categorical Class ;\n};\n</code></pre> <p>In this example, the variables <code>SepalLength</code> and <code>SepalWidth</code> are marked as <code>Unused</code>, which means that they will not be loaded into memory. Only the columns <code>PetalLength</code>, <code>PetalWidth</code>, and <code>Class</code> will be processed.</p>"},{"location":"tutorials/kdic_single_table/#user-defined-variables","title":"User-Defined Variables","text":"<p>In many cases, raw data stored in databases does not align with business experts' understanding. This occurs because the data often lacks the transformations needed to reflect domain knowledge. A crucial step in data management is translating this knowledge into manually-defined variables calculated from the raw data. For example, in a medical application, a variable such as the body mass index (BMI) can be derived from a patient\u2019s height and weight.</p> <p>Typically, data scientists handle this by loading the entire dataset into memory and manually calculating these variables by using libraries like Pandas. While effective for small-scale prototyping, this approach is resource-intensive, requiring substantial RAM and storage, especially when numerous trial-and-error iterations are performed during feature engineering. Data versioning also becomes cumbersome, as changes must be tracked across multiple files and scripts.</p> <p>For industrial-scale projects, dictionaries provide an efficient and scalable alternative. User-defined variables are calculated on the fly when the raw data is read, reducing memory overhead and eliminating the need for precomputed transformations. Additionally, since the entire data transformation flow is encoded within the dictionary, versioning is limited to a single text file, which can be managed by a version control system, such as Git.</p> <p>The following dictionary example shows the calculation of a user-defined variable representing the area of sepals, in the Iris dataset:</p> <p>Example: Calculating a user-defined variable</p> <pre><code>Dictionary Iris\n{\n    Numerical SepalLength ;\n    Numerical SepalWidth  ;\n    Numerical PetalLength  ;\n    Numerical PetalWidth  ;\n    Numerical SepalArea = Product(SepalLength,SepalWidth);\n    Categorical Class ;\n};\n</code></pre> <ul> <li>As previously, the field <code>Iris</code> indicates the name of the dictionary;</li> <li>And the fields <code>SepalLength</code>, <code>SepalWidth</code> ... <code>Class</code> indicate variable names;</li> <li>The primitive <code>Product</code> is used to calculate the user-defined variable;</li> <li>Finally, <code>SepalLength</code>, <code>SepalWidth</code> correspond to the operands of the primitive, which can either be variable names, constant values, or results of other primitives.</li> </ul> <p>Khiops offers a highly-expressive data transformation language, making it easy to define user-defined variables. An exhaustive list of available primitives is available on the reference page.</p> <p>For situations that require numerous user-defined variables, the Khiops Python library Core API allows users to programmatically add these variables to a dictionary, as shown in the following example:</p> <p>Add user-defined variables programmatically using the core API</p> <pre><code>import os\nfrom khiops import core as kh\n\n# Set path to the dictionary\ndictionary_file_path = os.path.join(kh.get_samples_dir(), \"Iris\", \"Iris.kdic\")\n\n# Load the learning dictionary object\ndomain = kh.read_dictionary_file(dictionary_file_path)\ndictionary = domain.get_dictionary(\"Iris\")\n\n# Add 10 unused supplemental numerical variables to the learning dictionary\nnumber_of_supplemental_variables = 10\nfor variable_index in range(1, number_of_supplemental_variables + 1):\n    supplemental_variable = kh.Variable()\n    supplemental_variable.name = \"SupplementalVariable\" + str(variable_index)\n    supplemental_variable.type = \"Numerical\"\nsupplemental_variable.used = False\ndictionary.add_variable(supplemental_variable)\n</code></pre>"},{"location":"tutorials/kdic_single_table/#example-selection","title":"Example Selection","text":"<p>Another possibility for defining the scope of an analysis involves selecting a subset of training examples (i.e. the rows of the dataset). For instance, when building a model to predict unemployment risk, the training dataset should exclude retirees and minors, focusing only on the working population.</p> <p>Khiops makes example selection efficient and scalable, even for large datasets. By defining a selection criterion directly in the dictionary, filtering is done on the fly during data processing, avoiding the need to load the entire dataset into memory.</p> <p>For example, in the Iris dataset, rows where the Class is \"Iris-setosa\" can be excluded by adding a user-defined selection variable to the dictionary:</p> <p>Example: Using a selection variable in a dictionary</p> <pre><code>Dictionary iris\n{\n    Numerical SepalLength;\n    Numerical SepalWidth;\n    Numerical PetalLength;\n    Numerical PetalWidth;\n    Unused  Numerical Selection = NEQc(Class, \"Iris-setosa\"); // exclude the \"Iris-setosa\" class from the training set\n    Categorical Class;\n};\n</code></pre> <ul> <li><code>Selection</code> is a user-defined variable calculated with the <code>NEQc</code> primitive (<code>NEQc</code> stands for a not-equal-to primitive, with categorical operands);</li> <li>The <code>Unused</code> keyword ensures the variable is not part of the analysis but is used only for filtering the examples.</li> </ul> <p>Once defined, this selection variable, containing 0s and 1s, can be exploited programmatically during training with the Khiops Python Core API. The following example demonstrates how to filter examples by retaining only rows where the selection variable equals 1 while training a predictive model:</p> <p>Train a predictive model using the core API</p> <pre><code># Imports\nimport os\nfrom khiops import core as kh\n\n# Set the file paths\ndictionary_file_path = os.path.join(kh.get_samples_dir(), \"Iris\", \"Iris.kdic\")\ndata_table_path = os.path.join(kh.get_samples_dir(), \"Iris\", \"Iris.txt\")\nresults_dir = os.path.join(\"kh_samples\", \"train_predictor\")\n\n# Train the predictor\nkh.train_predictor(\n    dictionary_file_path,\n    \"iris\",\n    data_table_path,\n    \"Class\",\n    results_dir,\n    max_trees=0,\n    selection_variable=\"Selection\",\n    selection_value=1\n)\n</code></pre>"},{"location":"tutorials/kni/","title":"Real-Time Deployment","text":"<p>The Khiops Native Interface (KNI) is a lightweight dynamically-linked library (DLL) that provides an ANSI C interface which enables the seamless integration of Khiops models directly into your applications. Unlike the standalone Khiops executable, KNI facilitates direct in-memory model inference, eliminating the need for temporary files or external calls.</p> <p>Its key features include:</p> <ul> <li>Real-Time Scoring: KNI allows you to apply trained Khiops models directly within your application, providing low-latency predictions ideal for real-time applications.</li> <li>Batch Processing: While optimized for real-time use, KNI also efficiently handles batch processing, offering versatility for various deployment scenarios.</li> <li>Lightweight and Fast: Written in optimized C++, KNI is designed to run with minimal computational overhead, ensuring speed and efficiency.</li> <li>Multi-Language Support: KNI can be integrated into applications written in C, C++, Java, Python, and even Matlab, making it adaptable to diverse development environments.</li> </ul>"},{"location":"tutorials/kni/#getting-started-with-kni","title":"Getting Started with KNI","text":"<p>The KNI is targeted primarily at system integrators which aim to deploy the scoring capabilities of Khiops in native Windows and Linux environments. KNI can also be used in Conda environments on Windows, Linux and MacOS.</p>"},{"location":"tutorials/kni/#installation","title":"Installation","text":"WindowsUbuntu / DebianRocky Linuxconda <p>There are two steps for installing KNI on Windows:</p> <ol> <li> <p>Download KNI: Download the KNI package <code>KNI-10.3.1.zip</code>.</p> </li> <li> <p>Set Environment Variable: Set the environment variable <code>KNI_HOME</code> to the extracted directory. </p> </li> </ol> <p>You can install KNI as follows:</p> <pre><code>CODENAME=$(lsb_release -cs) &amp;&amp; \\\nTEMP_DEB_KNI=\"$(mktemp)\" &amp;&amp; \\\nwget -O \"$TEMP_DEB_KNI\" \"https://github.com/KhiopsML/khiops/releases/download/10.3.1/kni_10.3.1-1-${CODENAME}.amd64.deb\" &amp;&amp; \\\nsudo dpkg -i \"$TEMP_DEB_KNI\" || sudo apt-get -f -y install &amp;&amp; \\\nrm -f $TEMP_DEB_KNI \n</code></pre> <p>You can install KNI as follow::</p> <pre><code>sudo yum update -y &amp;&amp; sudo yum install wget python3-pip -y &amp;&amp; \\\nCENTOS_VERSION=$(rpm -E %{rhel}) &amp;&amp; \\\nTEMP_RPM_KNI=\"$(mktemp).rpm\" &amp;&amp; \\\nwget -O \"$TEMP_RPM_KNI\" \"https://github.com/KhiopsML/khiops/releases/download/10.3.1/kni-{{ ROCKY_KHIOPS_VERSION }}-1.el${CENTOS_VERSION}.x86_64.rpm\" &amp;&amp; \\\nsudo yum install \"$TEMP_RPM_KNI\" -y &amp;&amp; \\\nrm -f $TEMP_RPM_KNI\n</code></pre> <pre><code>conda install -c khiops kni\n</code></pre> <p>You can find the all versions on the releases page.</p>"},{"location":"tutorials/kni/#example-application-in-c","title":"Example Application in C","text":"<p>This example demonstrates how to compile and run a C program (<code>KNIRecodeFile</code>) that applies Khiops native recoding process using the shared library. The <code>KNIRecodeFile</code> binary is a standalone executable that directly processes input data using a Khiops dictionary, transforming raw features into recoded (scored) outputs.</p> <p>The following example shows how to use KNI in C, on Linux, for a single-table case. If you need to apply KNI in a multi-table scenario or on a different operating system, refer to the KNI-tutorial repository for additional implementations in Python, Java, and Matlab.</p>"},{"location":"tutorials/kni/#building-the-example","title":"Building the Example","text":"<ol> <li>Clone our tutorial repositoy: Run the following <code>git clone</code> command:     <pre><code>git clone https://github.com/KhiopsML/KNI-tutorial.git\n</code></pre></li> <li>Navigate to the Example Directory: Change to the directory containing the example files:     <pre><code>cd KNI-tutorial/cpp/\n</code></pre></li> <li>Compile the Example: Run the following command to compile the <code>KNIRecodeFile</code> application:    <pre><code>gcc -o KNIRecodeFile cpp/KNIRecodeFile.c -I /usr/include/ -lKhiopsNativeInterface -ldl\n</code></pre></li> </ol>"},{"location":"tutorials/kni/#running-the-example","title":"Running the Example","text":"<p>Once compiled, you can run the <code>KNIRecodeFile</code> application to recode a dataset. Here's how:</p> <ol> <li> <p>Prepare Your Files: Ensure you have the following files:</p> <ul> <li>A Khiops dictionary file (e.g., <code>ModelingIris.kdic</code>);</li> <li>An input data file (e.g., <code>Iris.txt</code>).</li> </ul> </li> <li> <p>Run the Application: Use the following command to recode the input file:     <pre><code>KNIRecodeFile data/ModelingIris.kdic SNB_Iris data/Iris.txt R_Iris.txt\n</code></pre>     using the following inputs:</p> <ul> <li><code>data/ModelingIris.kdic</code>: The dictionary file that describes the model;</li> <li><code>SNB_Iris</code>: The name of the dictionary within the dictionary file;</li> <li><code>data/Iris.txt</code>: The input data file containing the data to be recoded;</li> <li><code>R_Iris.txt</code>: The output file where the recoded (scored) data will be saved.</li> </ul> </li> </ol>"},{"location":"tutorials/kni/#understanding-the-output","title":"Understanding the Output","text":"<p>The output file (<code>R_Iris.txt</code>) will contain the recoded data, which includes the scores or predictions generated by the Khiops model as described in the dictionary file. This process allows you to integrate Khiops' powerful data transformation capabilities directly into your applications, enabling real-time scoring and efficient batch processing.</p>"},{"location":"tutorials/kni/#conclusion","title":"Conclusion","text":"<p>KNI provides a powerful and flexible solution for real-time deployment of Khiops models. Its efficiency, versatility, and ease of integration make it an ideal choice for developers and integrators looking to enhance their data science workflows with real-time scoring capabilities. By leveraging KNI, you can ensure that your applications are responsive, efficient, and capable of handling complex data processing tasks in real-time. </p>"},{"location":"tutorials/quickstart/","title":"Quickstart Guide","text":"<p>The user-friendly Khiops Python library provides a unique Auto-ML solution. Khiops offers significant practical advantages, based on an original formalism: </p> <ul> <li>Advanced Automation</li> <li>Model Interpretability</li> <li>Outstanding Scalability</li> </ul>"},{"location":"tutorials/quickstart/#auto-ml-as-simple-as-a-regular-classifier","title":"Auto-ML as Simple as a Regular Classifier","text":"<p>The Khiops Python library allows users to set-up Auto-ML pipelines which automate supervised Machine Learning, e.g. for classifying input examples into predefined groups, each identified by a label. Common applications include predicting customer churn (Yes or No), the severity of a failure (Minor, Major, Critical) etc. </p> <p>Featuring unique learning algorithms, Khiops automates many steps seamlessly for the user. For example, there's no longer any need to prepare training data, as missing values, noise, outliers and unbalanced classes are handled for you. Encoding categorical variables is also no longer a problem. </p> <p>Ultimately, all you need to do is use the standard Scikit-Learn syntax, and Khiops takes care of handling poor-quality raw data, producing competitive, robust and interpretable models.  </p>"},{"location":"tutorials/quickstart/#setup","title":"\ud83d\udd27 Setup","text":"<pre><code># Straightforward installation using Conda.\n#!conda install -c conda-forge -c khiops khiops\n</code></pre> <pre><code># Import relevant packages\nimport pandas as pd\n\nfrom khiops.sklearn import KhiopsClassifier\nfrom sklearn.model_selection import train_test_split\n</code></pre>"},{"location":"tutorials/quickstart/#load-sample-dataset","title":"\ud83d\udcca Load Sample Dataset","text":"<pre><code># Load and read the data file into a Pandas DataFrame\nurl = \"https://raw.githubusercontent.com/KhiopsML/khiops-samples/10.2.4/Adult/Adult.txt\"\ndf = pd.read_csv(url, delimiter='\\t',index_col=\"Label\")\n</code></pre> <pre><code># Drop the \"class\" column to create the feature set (X)\nX = df.drop(\"class\", axis=1)\n</code></pre> <pre><code># Extract the \"class\" column to create the target labels y (useful for performance analysis of the models)\ny = df[\"class\"].map({'less': 0, 'more': 1})\n</code></pre> <pre><code># Randomly split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y)\n</code></pre>"},{"location":"tutorials/quickstart/#just-fit-it","title":"\ud83d\ude80 Just fit it","text":"<pre><code># Declare Khiops Auto-ML classifier object and fit it to the training dataset\nclf = KhiopsClassifier()\nclf.fit(X_train, y_train)\n</code></pre>"},{"location":"tutorials/quickstart/#predict","title":"\ud83d\udd2e Predict","text":"<pre><code># Predict labels on the testing dataset\npred = clf.predict(X_test)\n</code></pre>"},{"location":"tutorials/quickstart/#auto-feature-engineering-as-simple-as-creating-a-dictionary","title":"Auto Feature Engineering as Simple as Creating a Dictionary","text":"<p>Feature Engineering aims to build a training dataset from multi-table data, by summarizing useful information from the secondary tables. For example, let's consider multi-table data where the root table describes the customers of a telecommunications operator (with one record per customer) and the secondary tables each describe call details, services used, contracts (with a varying number of records per customer). In this case, the ''call rate to foreign countries'' could be a useful aggregate for predicting customer churn.</p> <p>In practice, when undertaken manually, Feature Engineering is an extremely time-consuming task which is not efficient and risks over-fitting (i.e. when using over-complex aggregates). Just feed multi-table data into the Khiops Auto-ML solution to trigger an ultra-efficient automatic Feature Engineering algorithm. All you need to do is describe the structure of the input multi-table data with a dictionary.</p>"},{"location":"tutorials/quickstart/#simply-describe-your-multi-table-data","title":"\ud83d\udd8b\ufe0f Simply Describe Your Multi-Table Data","text":"<p>Below, we show the relevant <code>syntax</code> for describing multi-table data. </p> <p>We continue with the previous example, where the multi-table data describes the customers of a telecommunications operator and where the goal is to predict customer churn:</p> <p></p> <p>Khiops allows users to describe multi-table data as a Python dictionary (a running sample is available here). Here's an example: </p> <pre><code>X_train = {\n    \"main_table\": \"Customer\",\n    \"tables\": {\n        \"Customer\": (customer_main_df, \"CustomerId\"),\n        \"Call\": (call_df, [\"CustomerId\", \"CallId\"]),\n        \"Service\": (service_df, [\"CustomerId\", \"ServiceId\"]),\n        \"Contract\": (contract_df, [\"CustomerId\", \"ContractId\"]),\n    },\n    \"relations\": [\n        (\"Customer\", \"Call\"),\n        (\"Customer\", \"Service\"),\n        (\"Customer\", \"Contract\"),\n    ],\n}\n</code></pre> <p>This dictionary includes three attributes: </p> <ul> <li><code>main_table</code> indicating the name of the main table,</li> <li><code>tables</code> describing all tables, </li> <li><code>relations</code> specifying the links between tables. </li> </ul> <p><code>tables</code> is itself a dictionary, composed of one record per table. For each record, the key corresponds to the table name and the value is a tuple associating a Pandas Dataframe and a list of keys (first the main key, then the secondary keys). And <code>relations</code> is a list of tuples, which contain pairs of names of the linked tables (one pair per link).</p>"},{"location":"tutorials/quickstart/#just-fit-it-as-usual","title":"\ud83d\ude80 Just Fit It ... as Usual","text":"<p>Once you have described the multi-table input data, no further effort is required.</p> <pre><code># Declare the classifier and train it\nclf = KhiopsClassifier()\nclf.fit(X_train, y_train)\n</code></pre>"},{"location":"tutorials/quickstart/#visualizing-the-analysis-reports","title":"Visualizing the Analysis Reports","text":"<p>The analysis report contains details on the features obtained by the Khiops Auto Feature Engineering algorithm, the encoding of variables, the selection of variables and their importance for predictions. </p> <p>You can open and visualize the report using the standalone Khiops visualization desktop application on several operating systems. More details on this visualization desktop application can be found on the dedicated page. </p> <p>To generate the analysis report, you need to specify the <code>output_dir</code> parameter when creating the <code>KhiopsClassifier</code> estimator object instance: </p> <pre><code># Declare the classifier with a specified output directory and train it\nclf = KhiopsClassifier(output_dir=\"User/Documents/test_khiops\")\nclf.fit(X_train, y_train)\n</code></pre> <p>Open the <code>AllReports.khj</code> file on the visualization desktop application:</p> <p></p>"},{"location":"tutorials/storage/","title":"Cloud Storage","text":"<p>Khiops seamlessly integrates with cloud storage services, enabling direct reading and writing of datasets stored in AWS S3 and Google Cloud Storage (GCS) buckets. By using Khiops dedicated cloud storage drivers, you can process large-scale datasets without having to manually download or transfer files, significantly improving efficiency and scalability in cloud-based workflows.</p> <p>With these drivers, Khiops treats cloud storage just like a local filesystem, meaning that all Khiops commands and workflows remain unchanged\u2014only the dataset paths need to be adjusted.</p> <p>Driver Installation Support</p> <p>On Windows and macOS, Khiops drivers are only supported through Conda. If you are using another installation method on these operating systems, consider switching to a Conda environment to enable driver support. On Linux, Khiops drivers are supported through both Conda (Python only) and by using the binary installation method (compatible with the Khiops Application and Python via pip). </p>"},{"location":"tutorials/storage/#using-khiops-with-google-cloud-storage-gcs","title":"Using Khiops with Google Cloud Storage (GCS)","text":"<p>Khiops can read and write datasets stored in GCS buckets using the <code>khiopsdriver-gcs</code> package. Once configured, you can reference GCS paths directly in Khiops commands, scenarios and the GUI (where applicable) using the format <code>gs://&lt;bucket-name&gt;/path/to/file.csv</code>.</p>"},{"location":"tutorials/storage/#installation","title":"Installation","text":"<p>If you installed Khiops through Conda as recommended, you can install the driver as follows:</p> <pre><code>conda install -c khiops khiops-driver-gcs\n</code></pre> If you installed Khiops using <code>pip</code> on Linux...  Ubuntu / DebianRocky Linux <pre><code>CODENAME=$(lsb_release -cs) &amp;&amp; \\\nTEMP_DEB=\"$(mktemp)\" &amp;&amp; \\\nwget -O \"$TEMP_DEB\" \"https://github.com/KhiopsML/khiopsdriver-gcs/releases/download/0.0.11/khiops-driver-gcs_0.0.11-1-${CODENAME}.amd64.deb\" &amp;&amp; \\\nsudo dpkg -i \"$TEMP_DEB\" &amp;&amp; \\\nrm -f $TEMP_DEB\n</code></pre> <pre><code>sudo yum update -y &amp;&amp; sudo yum install wget -y &amp;&amp; \\\nROCKY_VERSION=$(rpm -E %{rhel}) &amp;&amp; \\\nTEMP_RPM=\"$(mktemp).rpm\" &amp;&amp; \\\nwget -O \"$TEMP_RPM\" \"https://github.com/KhiopsML/khiopsdriver-gcs/releases/download/0.0.11/khiops-driver-gcs_0.0.11-1.el${ROCKY_VERSION}.x86_64.rpm\" &amp;&amp; \\\nsudo yum install \"$TEMP_RPM\" -y &amp;&amp; \\\nrm -f $TEMP_RPM\n</code></pre> <p>To verify the installation, run:</p> <pre><code>khiops -s\n</code></pre> <p>You should see an output indicating that the GCS driver is loaded and ready to use for data files following the URI <code>gs</code> scheme, as follows:</p> <pre><code>Khiops 10.3.1\n\nDrivers:\n    'GCS driver' for URI scheme 'gs'\nEnvironment variables:\n    None\nInternal environment variables:\n    None\n</code></pre>"},{"location":"tutorials/storage/#authentication","title":"Authentication","text":"<p>To access data stored in GCS buckets, you need valid authentication credentials. Khiops uses the standard Application Default Credentials for authentication. Set up your local environment with these credentials using the <code>gcloud</code> CLI:</p> <pre><code>gcloud init\ngcloud auth application-default login\n</code></pre> <p>With these credentials in place, Khiops can access your GCS data seamlessly.</p>"},{"location":"tutorials/storage/#using-gcs-uris-in-khiops","title":"Using GCS URIs in Khiops","text":"<p>Once installed, Khiops can directly read and write GCS paths in the format <code>gs://&lt;bucket-name&gt;/path/to/file.csv</code> from the desktop application (GUI), Python scripts, or within Khiops scenarios. For example:</p> <p>Low-Level Khiops Usage: <pre><code>khiops -b -i gs://mydatabucket/khiops_samples/scenario.kh\n</code></pre></p> <p>Python Sample:</p> <pre><code># Imports\nimport os\nfrom khiops import core as kh\n\n# Set the file paths\ndictionary_file_path = \"gs://mydatabucket/khiops_samples/Adult/Adult.kdic\"\ndata_table_path = \"gs://mydatabucket/khiops_samples/Adult/Adult.kdic\"\nresults_dir = \"khiops_output\"\n\n# Train the predictor\nkh.train_predictor(\n    dictionary_file_path,\n    \"Adult\",\n    data_table_path,\n    \"class\",\n    results_dir,\n    max_trees=0,\n)\n</code></pre>"},{"location":"tutorials/storage/#using-khiops-with-aws-s3-storage","title":"Using Khiops with AWS S3 Storage","text":"<p>To start using Khiops with your data on S3, install the S3 driver package alongside Khiops. If you installed Khiops through Conda as recommended, you can install the driver as follows:</p> <pre><code>conda install -c khiops khiops-driver-s3\n</code></pre> If you installed Khiops using <code>pip</code> on Linux... Ubuntu / DebianRocky Linux <pre><code>CODENAME=$(lsb_release -cs) &amp;&amp; \\\nTEMP_DEB=\"$(mktemp)\" &amp;&amp; \\\nwget -O \"$TEMP_DEB\" \"https://github.com/KhiopsML/khiopsdriver-s3/releases/download/0.0.13/khiops-driver-s3_0.0.13-1-${CODENAME}.amd64.deb\" &amp;&amp; \\\nsudo dpkg -i \"$TEMP_DEB\" &amp;&amp; \\\nrm -f $TEMP_DEB\n</code></pre> <pre><code>sudo yum update -y &amp;&amp; sudo yum install wget -y &amp;&amp; \\\nROCKY_VERSION=$(rpm -E %{rhel}) &amp;&amp; \\\nTEMP_RPM=\"$(mktemp).rpm\" &amp;&amp; \\\nwget -O \"$TEMP_RPM\" \"https://github.com/KhiopsML/khiopsdriver-s3/releases/download/0.0.13/khiops-driver-s3_0.0.13-1.el${ROCKY_VERSION}.x86_64.rpm\" &amp;&amp; \\\nsudo yum install \"$TEMP_RPM\" -y &amp;&amp; \\\nrm -f $TEMP_RPM\n</code></pre> <p>To verify the installation, run:</p> <pre><code>khiops -s\n</code></pre> <p>You should see an output indicating that the S3 driver is loaded and ready to use for data files following the URI <code>s3</code> scheme, as follows:</p> <pre><code>Khiops 10.3.1\n\nDrivers:\n    'S3 driver' for URI scheme 's3'\nEnvironment variables:\n    None\nInternal environment variables:\n    None\n</code></pre>"},{"location":"tutorials/storage/#authentication_1","title":"Authentication","text":"<p>To access data stored in S3 buckets, you need valid authentication credentials. Khiops supports the same configuration options as the AWS CLI, accepting credentials and configuration options provided via configuration files or environment variables.</p> <p>File-Based Configuration:</p> <p>Create a <code>config</code> file in the $HOME/.aws folder:</p> <pre><code>[default]\nregion=us-east-1\nendpoint_url = https://my-server.cloudprovider.com\n</code></pre> <p>Create a <code>credentials</code> file in the same folder:</p> <pre><code>[default]\naws_access_key_id = AKIAIOSFODNN7EXAMPLE\naws_secret_access_key = wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\n</code></pre> <p>Environment Variables:</p> <p>Alternatively, you can set the configuration options and credentials via environment variables:</p> <pre><code>export AWS_ACCESS_KEY_ID=AKIAIOSFODNN7EXAMPLE\nexport AWS_SECRET_ACCESS_KEY=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\nexport AWS_DEFAULT_REGION=us-east-1\nexport AWS_ENDPOINT_URL=https://my-server.cloudprovider.com\n</code></pre>"},{"location":"tutorials/storage/#using-s3-uris-in-khiops","title":"Using S3 URIs in Khiops","text":"<p>Once installed, Khiops can directly read and write S3 paths in the format <code>s3://&lt;bucket-name&gt;/path/to/file.csv</code> from the desktop application (GUI), Python scripts, or within Khiops scenarios. For example:</p> <p>Low-Level Khiops Usage: <pre><code>khiops -b -i s3://mydatabucket/khiops_samples/scenario.kh\n</code></pre></p> <p>Python Sample:</p> <pre><code># Imports\nimport os\nfrom khiops import core as kh\n\n# Set the file URIs\ndictionary_file_path = \"s3://mydatabucket/khiops_samples/Adult/Adult.kdic\"\ndata_table_path = \"s3://mydatabucket/khiops_samples/Adult/Adult.kdic\"\nresults_dir = \"khiops_output\"\n\n# Train the predictor\nkh.train_predictor(\n    dictionary_file_path,\n    \"Adult\",\n    data_table_path,\n    \"class\",\n    results_dir,\n    max_trees=0,\n)\n</code></pre>"},{"location":"tutorials/Notebooks/No_data_Cleaning/","title":"No Need for Data Preparation","text":"In\u00a0[1]: Copied! <pre>#!conda install -y -c conda-forge -c khiops khiops\n</pre> #!conda install -y -c conda-forge -c khiops khiops <p>For the experiments, you also need some external libraries you can install via <code>pip</code>:</p> In\u00a0[2]: Copied! <pre># Installation of external libraries\n#!pip install matplotlib seaborn\n\n# Note: Installing PyCaret can sometimes be complex due to its dependencies. \n# If you encounter any issues, please refer to the PyCaret documentation for detailed installation instructions:\n# https://pycaret.gitbook.io/docs/get-started/installation\n#!pip install pycaret\n</pre> # Installation of external libraries #!pip install matplotlib seaborn  # Note: Installing PyCaret can sometimes be complex due to its dependencies.  # If you encounter any issues, please refer to the PyCaret documentation for detailed installation instructions: # https://pycaret.gitbook.io/docs/get-started/installation #!pip install pycaret <p>We now import all the dependencies here:</p> In\u00a0[3]: Copied! <pre>import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\nfrom khiops.sklearn import KhiopsClassifier\n\nfrom pycaret.classification import *\n\nplt.rcParams['font.family'] = ['DejaVu Sans']\n</pre> import pandas as pd import matplotlib.pyplot as plt import numpy as np import seaborn as sns  from khiops.sklearn import KhiopsClassifier  from pycaret.classification import *  plt.rcParams['font.family'] = ['DejaVu Sans'] In\u00a0[4]: Copied! <pre># Method 1: Load data directly from GitHub (recommended for quick tests or small datasets)\nurl = \"https://raw.githubusercontent.com/KhiopsML/khiops-samples/10.2.4/Adult/Adult.txt\"\ndf = pd.read_csv(url, delimiter='\\t',index_col=\"Label\")\n\n# Method 2: Load data locally after downloading all Khiops samples (best for offline use or large datasets)\n#If the samples have not been downloaded yet:\n#from khiops.tools import download_datasets\n#download_datasets() \n#\n#from os import path\n#from khiops import core as kh\n#adult_path = path.join(kh.get_samples_dir(), \"Adult\", \"Adult.txt\")\n#df = pd.read_csv(adult_path, sep=\"\\t\",index_col=\"Label\")\n\n# Display the first 10 records from the dataset\ndf.head(10)\n</pre> # Method 1: Load data directly from GitHub (recommended for quick tests or small datasets) url = \"https://raw.githubusercontent.com/KhiopsML/khiops-samples/10.2.4/Adult/Adult.txt\" df = pd.read_csv(url, delimiter='\\t',index_col=\"Label\")  # Method 2: Load data locally after downloading all Khiops samples (best for offline use or large datasets) #If the samples have not been downloaded yet: #from khiops.tools import download_datasets #download_datasets()  # #from os import path #from khiops import core as kh #adult_path = path.join(kh.get_samples_dir(), \"Adult\", \"Adult.txt\") #df = pd.read_csv(adult_path, sep=\"\\t\",index_col=\"Label\")  # Display the first 10 records from the dataset df.head(10) Out[4]: age workclass fnlwgt education education_num marital_status occupation relationship race sex capital_gain capital_loss hours_per_week native_country class Label 1 39 State-gov 77516 Bachelors 13 Never-married Adm-clerical Not-in-family White Male 2174 0 40 United-States less 2 50 Self-emp-not-inc 83311 Bachelors 13 Married-civ-spouse Exec-managerial Husband White Male 0 0 13 United-States less 3 38 Private 215646 HS-grad 9 Divorced Handlers-cleaners Not-in-family White Male 0 0 40 United-States less 4 53 Private 234721 11th 7 Married-civ-spouse Handlers-cleaners Husband Black Male 0 0 40 United-States less 5 28 Private 338409 Bachelors 13 Married-civ-spouse Prof-specialty Wife Black Female 0 0 40 Cuba less 6 37 Private 284582 Masters 14 Married-civ-spouse Exec-managerial Wife White Female 0 0 40 United-States less 7 49 Private 160187 9th 5 Married-spouse-absent Other-service Not-in-family Black Female 0 0 16 Jamaica less 8 52 Self-emp-not-inc 209642 HS-grad 9 Married-civ-spouse Exec-managerial Husband White Male 0 0 45 United-States more 9 31 Private 45781 Masters 14 Never-married Prof-specialty Not-in-family White Female 14084 0 50 United-States more 10 42 Private 159449 Bachelors 13 Married-civ-spouse Exec-managerial Husband White Male 5178 0 40 United-States more <p>We use bar chart to compare all the models, so we create a dedicate function:</p> In\u00a0[5]: Copied! <pre>def plot_model_comparison(df, title):\n    \"\"\"\n    Plot a bar chart comparing various metrics across different models.\n    \n    Parameters:\n    - df: DataFrame containing the model metrics. Must have a 'Model' column and metric columns like 'Accuracy', 'AUC', etc.\n    - title: The title for the plot.\n    \"\"\"\n    # Filter the models based on a criterion if necessary (e.g., F1-score &gt; 0.5)\n    # df = df[df['F1'] &gt; 0.5]\n\n    # To ease visualization, sort and reshape the DataFrame\n    df_plot = df.sort_values(by=\"Model\").melt(id_vars=['Model'], var_name='Metric', value_name='Value')\n\n    plt.figure(figsize=(14, 6))\n\n    # Create a bar plot with Seaborn\n    sns.barplot(x='Metric', y='Value', hue='Model', data=df_plot, palette=\"Set3\")\n\n    plt.title(title)\n    plt.ylabel('Value')\n    plt.xlabel('Metric')\n\n    plt.legend(title='Model', bbox_to_anchor=(1.05, 1), loc='upper left')\n\n    plt.tight_layout()\n    plt.show()\n</pre> def plot_model_comparison(df, title):     \"\"\"     Plot a bar chart comparing various metrics across different models.          Parameters:     - df: DataFrame containing the model metrics. Must have a 'Model' column and metric columns like 'Accuracy', 'AUC', etc.     - title: The title for the plot.     \"\"\"     # Filter the models based on a criterion if necessary (e.g., F1-score &gt; 0.5)     # df = df[df['F1'] &gt; 0.5]      # To ease visualization, sort and reshape the DataFrame     df_plot = df.sort_values(by=\"Model\").melt(id_vars=['Model'], var_name='Metric', value_name='Value')      plt.figure(figsize=(14, 6))      # Create a bar plot with Seaborn     sns.barplot(x='Metric', y='Value', hue='Model', data=df_plot, palette=\"Set3\")      plt.title(title)     plt.ylabel('Value')     plt.xlabel('Metric')      plt.legend(title='Model', bbox_to_anchor=(1.05, 1), loc='upper left')      plt.tight_layout()     plt.show() In\u00a0[6]: Copied! <pre># Drop the \"class\" column to create the feature set (X).\nX = df.drop(\"class\", axis=1)\n</pre> # Drop the \"class\" column to create the feature set (X). X = df.drop(\"class\", axis=1) In\u00a0[7]: Copied! <pre># Extract the \"class\" column to create the target labels (y).\ny = df[\"class\"].map({'less': 0, 'more': 1})\n</pre> # Extract the \"class\" column to create the target labels (y). y = df[\"class\"].map({'less': 0, 'more': 1}) In\u00a0[8]: Copied! <pre># the pyCaret setup for the standard models:\nsetup(pd.concat([X, y], axis=1), target = 'class', session_id=123, verbose=False)\n#compare_models(include=[\"lr\",\"gbc\",\"ada\",\"rf\",\"lda\",\"ridge\",\"et\",\"dt\"])\ncompare_models(include=[\"lr\",\"lightgbm\",\"gbc\",\"ada\",\"rf\",\"lda\",\"ridge\",\"et\",\"dt\"])\n</pre> # the pyCaret setup for the standard models: setup(pd.concat([X, y], axis=1), target = 'class', session_id=123, verbose=False) #compare_models(include=[\"lr\",\"gbc\",\"ada\",\"rf\",\"lda\",\"ridge\",\"et\",\"dt\"]) compare_models(include=[\"lr\",\"lightgbm\",\"gbc\",\"ada\",\"rf\",\"lda\",\"ridge\",\"et\",\"dt\"]) Model Accuracy AUC Recall Prec. F1 Kappa MCC TT (Sec) lightgbm Light Gradient Boosting Machine 0.8743 0.9287 0.6557 0.7840 0.7139 0.6342 0.6385 11.8950 gbc Gradient Boosting Classifier 0.8659 0.9210 0.5954 0.7927 0.6798 0.5972 0.6071 3.3380 ada Ada Boost Classifier 0.8616 0.9159 0.6082 0.7657 0.6777 0.5911 0.5976 1.3070 rf Random Forest Classifier 0.8551 0.9030 0.6186 0.7345 0.6713 0.5793 0.5830 2.5590 lda Linear Discriminant Analysis 0.8416 0.8921 0.5582 0.7177 0.6278 0.5293 0.5361 0.5860 ridge Ridge Classifier 0.8407 0.0000 0.5009 0.7510 0.6008 0.5063 0.5226 0.3420 et Extra Trees Classifier 0.8354 0.8793 0.6038 0.6746 0.6371 0.5311 0.5326 2.7370 dt Decision Tree Classifier 0.8135 0.7465 0.6183 0.6087 0.6133 0.4904 0.4906 0.4450 lr Logistic Regression 0.7981 0.5749 0.2619 0.7131 0.3827 0.2918 0.3445 0.7100 Out[8]: <pre>LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n               importance_type='split', learning_rate=0.1, max_depth=-1,\n               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,\n               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,\n               subsample_for_bin=200000, subsample_freq=0)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LGBMClassifier<pre>LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n               importance_type='split', learning_rate=0.1, max_depth=-1,\n               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,\n               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,\n               subsample_for_bin=200000, subsample_freq=0)</pre> In\u00a0[9]: Copied! <pre># we store the pycaret results on a dedicated DataFrame\nresults_on_raw_data= pull()\n</pre> # we store the pycaret results on a dedicated DataFrame results_on_raw_data= pull() In\u00a0[10]: Copied! <pre># the pyCaret setup for Khiops:\nsetup(pd.concat([X, y], axis=1), target = 'class', session_id=123, verbose=False, preprocess=False)\ncompare_models(include=[KhiopsClassifier()])\n</pre> # the pyCaret setup for Khiops: setup(pd.concat([X, y], axis=1), target = 'class', session_id=123, verbose=False, preprocess=False) compare_models(include=[KhiopsClassifier()]) Model Accuracy AUC Recall Prec. F1 Kappa MCC TT (Sec) 0 KhiopsClassifier 0.8701 0.9260 0.6208 0.7918 0.6957 0.6146 0.6222 4.9880 Out[10]: <pre>KhiopsClassifier(auto_sort=True, internal_sort=None, key=None, n_features=100,\n                 n_pairs=0, n_trees=10, output_dir=None, verbose=False)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KhiopsClassifier<pre>KhiopsClassifier(auto_sort=True, internal_sort=None, key=None, n_features=100,\n                 n_pairs=0, n_trees=10, output_dir=None, verbose=False)</pre> In\u00a0[11]: Copied! <pre># we now store the Khiops results on our DataFrame \nresults_on_raw_data = pd.concat([results_on_raw_data, pull()], ignore_index=True)\nresults_on_raw_data.sort_values(by=\"Accuracy\",ascending=False)\n</pre> # we now store the Khiops results on our DataFrame  results_on_raw_data = pd.concat([results_on_raw_data, pull()], ignore_index=True) results_on_raw_data.sort_values(by=\"Accuracy\",ascending=False) Out[11]: Model Accuracy AUC Recall Prec. F1 Kappa MCC TT (Sec) 0 Light Gradient Boosting Machine 0.8743 0.9287 0.6557 0.7840 0.7139 0.6342 0.6385 11.895 9 KhiopsClassifier 0.8701 0.9260 0.6208 0.7918 0.6957 0.6146 0.6222 4.988 1 Gradient Boosting Classifier 0.8659 0.9210 0.5954 0.7927 0.6798 0.5972 0.6071 3.338 2 Ada Boost Classifier 0.8616 0.9159 0.6082 0.7657 0.6777 0.5911 0.5976 1.307 3 Random Forest Classifier 0.8551 0.9030 0.6186 0.7345 0.6713 0.5793 0.5830 2.559 4 Linear Discriminant Analysis 0.8416 0.8921 0.5582 0.7177 0.6278 0.5293 0.5361 0.586 5 Ridge Classifier 0.8407 0.0000 0.5009 0.7510 0.6008 0.5063 0.5226 0.342 6 Extra Trees Classifier 0.8354 0.8793 0.6038 0.6746 0.6371 0.5311 0.5326 2.737 7 Decision Tree Classifier 0.8135 0.7465 0.6183 0.6087 0.6133 0.4904 0.4906 0.445 8 Logistic Regression 0.7981 0.5749 0.2619 0.7131 0.3827 0.2918 0.3445 0.710 <p>We can finally show those results on a bar chart to have an overview of the all results. Notice that we intentionally exclude the computation time (TT in seconds) from the overview, as its significance is minimal on small datasets, and because the different scale (compared to scores range) does not ease the reading. We remind as well this computation time is given for a small <code>n2-standard-2 (2 vCPUs, 8 GB RAM)</code> machine.</p> In\u00a0[12]: Copied! <pre># We'll plot the results. \nplot_model_comparison(results_on_raw_data.drop('TT (Sec)',axis=1),'Comparison of Models on Raw Data')\n</pre> # We'll plot the results.  plot_model_comparison(results_on_raw_data.drop('TT (Sec)',axis=1),'Comparison of Models on Raw Data') In\u00a0[13]: Copied! <pre>X_class_based_missing_values = X.copy()\n\n# Percentage of missing values depending on the class\nmissing_rate_class_0 = 0.2  # 20% missing when class is 0\nmissing_rate_class_1 = 0.05  # 5% missing when class is 1\n\n# Identify indices where class is 0\nindices_class_0 = X_class_based_missing_values.index[y.loc[X_class_based_missing_values.index] == 0]\n\n# Identify indices where class is 1\nindices_class_1 = X_class_based_missing_values.index[y.loc[X_class_based_missing_values.index] == 1]\n\n# Number of missing values for each class\nN_0 = int(len(indices_class_0) * missing_rate_class_0)\nN_1 = int(len(indices_class_1) * missing_rate_class_1)\n\n# Iterate over feature columns\nfor col in X_class_based_missing_values.columns:\n\n        # the column index (for the seed)\n        col_index = X_class_based_missing_values.columns.get_loc(col)\n    \n        # Varying the seed by column index to distribute NaNs across different rows.\n        np.random.seed(42 + col_index) ; random_indices_0 = np.random.choice(indices_class_0, N_0, replace=False)\n        np.random.seed(42 + col_index) ; random_indices_1 = np.random.choice(indices_class_1, N_1, replace=False)\n        \n        # Introduce missing values\n        X_class_based_missing_values.loc[random_indices_0, col] = np.nan\n        X_class_based_missing_values.loc[random_indices_1, col] = np.nan\n\nX_class_based_missing_values.head(10)\n</pre> X_class_based_missing_values = X.copy()  # Percentage of missing values depending on the class missing_rate_class_0 = 0.2  # 20% missing when class is 0 missing_rate_class_1 = 0.05  # 5% missing when class is 1  # Identify indices where class is 0 indices_class_0 = X_class_based_missing_values.index[y.loc[X_class_based_missing_values.index] == 0]  # Identify indices where class is 1 indices_class_1 = X_class_based_missing_values.index[y.loc[X_class_based_missing_values.index] == 1]  # Number of missing values for each class N_0 = int(len(indices_class_0) * missing_rate_class_0) N_1 = int(len(indices_class_1) * missing_rate_class_1)  # Iterate over feature columns for col in X_class_based_missing_values.columns:          # the column index (for the seed)         col_index = X_class_based_missing_values.columns.get_loc(col)              # Varying the seed by column index to distribute NaNs across different rows.         np.random.seed(42 + col_index) ; random_indices_0 = np.random.choice(indices_class_0, N_0, replace=False)         np.random.seed(42 + col_index) ; random_indices_1 = np.random.choice(indices_class_1, N_1, replace=False)                  # Introduce missing values         X_class_based_missing_values.loc[random_indices_0, col] = np.nan         X_class_based_missing_values.loc[random_indices_1, col] = np.nan  X_class_based_missing_values.head(10) Out[13]: age workclass fnlwgt education education_num marital_status occupation relationship race sex capital_gain capital_loss hours_per_week native_country Label 1 39.0 NaN 77516.0 Bachelors NaN Never-married Adm-clerical NaN White Male 2174.0 0.0 40.0 United-States 2 50.0 NaN 83311.0 Bachelors 13.0 Married-civ-spouse NaN NaN White Male 0.0 NaN 13.0 NaN 3 38.0 Private 215646.0 HS-grad NaN Divorced Handlers-cleaners Not-in-family NaN NaN 0.0 0.0 40.0 United-States 4 53.0 Private 234721.0 NaN NaN Married-civ-spouse Handlers-cleaners Husband Black Male 0.0 0.0 NaN NaN 5 NaN Private NaN Bachelors NaN Married-civ-spouse NaN Wife Black Female 0.0 0.0 40.0 Cuba 6 37.0 NaN 284582.0 NaN 14.0 Married-civ-spouse Exec-managerial Wife White Female 0.0 0.0 40.0 United-States 7 NaN NaN 160187.0 9th NaN Married-spouse-absent NaN Not-in-family Black Female NaN 0.0 NaN Jamaica 8 52.0 Self-emp-not-inc 209642.0 HS-grad 9.0 Married-civ-spouse Exec-managerial Husband NaN Male 0.0 0.0 45.0 United-States 9 31.0 Private 45781.0 Masters 14.0 Never-married Prof-specialty Not-in-family White Female 14084.0 0.0 50.0 United-States 10 42.0 Private 159449.0 Bachelors 13.0 Married-civ-spouse Exec-managerial Husband White Male 5178.0 0.0 40.0 United-States In\u00a0[14]: Copied! <pre>setup(pd.concat([X_class_based_missing_values, y], axis=1), target = 'class', session_id=123, verbose = False)\ncompare_models(include=[\"lr\",\"lightgbm\",\"gbc\",\"ada\",\"rf\",\"lda\",\"ridge\",\"et\",\"dt\"])\n</pre> setup(pd.concat([X_class_based_missing_values, y], axis=1), target = 'class', session_id=123, verbose = False) compare_models(include=[\"lr\",\"lightgbm\",\"gbc\",\"ada\",\"rf\",\"lda\",\"ridge\",\"et\",\"dt\"]) Model Accuracy AUC Recall Prec. F1 Kappa MCC TT (Sec) lightgbm Light Gradient Boosting Machine 0.9021 0.9526 0.7471 0.8274 0.7850 0.7219 0.7236 18.9730 gbc Gradient Boosting Classifier 0.8890 0.9443 0.6577 0.8445 0.7393 0.6702 0.6787 3.3480 rf Random Forest Classifier 0.8819 0.9318 0.6775 0.7989 0.7330 0.6579 0.6617 2.4670 ada Ada Boost Classifier 0.8809 0.9349 0.6590 0.8082 0.7258 0.6507 0.6564 1.1730 et Extra Trees Classifier 0.8593 0.9056 0.6486 0.7330 0.6880 0.5976 0.5996 2.8120 dt Decision Tree Classifier 0.8390 0.7830 0.6751 0.6602 0.6674 0.5612 0.5614 0.4550 lda Linear Discriminant Analysis 0.8387 0.8838 0.5232 0.7267 0.6082 0.5100 0.5210 0.5700 ridge Ridge Classifier 0.8373 0.0000 0.4680 0.7602 0.5792 0.4854 0.5075 0.3260 lr Logistic Regression 0.7999 0.5844 0.2523 0.7530 0.3728 0.2884 0.3520 0.4500 Out[14]: <pre>LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n               importance_type='split', learning_rate=0.1, max_depth=-1,\n               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,\n               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,\n               subsample_for_bin=200000, subsample_freq=0)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LGBMClassifier<pre>LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n               importance_type='split', learning_rate=0.1, max_depth=-1,\n               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,\n               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,\n               subsample_for_bin=200000, subsample_freq=0)</pre> In\u00a0[15]: Copied! <pre># we store the pycaret results on a dedicated DataFrame\nresults_class_based_missing_values= pull()\n</pre> # we store the pycaret results on a dedicated DataFrame results_class_based_missing_values= pull() In\u00a0[16]: Copied! <pre># the pyCaret setup for Khiops:\nsetup(pd.concat([X_class_based_missing_values, y], axis=1), target = 'class', session_id=123, verbose=False, preprocess=False)\ncompare_models(include=[KhiopsClassifier()])\n</pre> # the pyCaret setup for Khiops: setup(pd.concat([X_class_based_missing_values, y], axis=1), target = 'class', session_id=123, verbose=False, preprocess=False) compare_models(include=[KhiopsClassifier()]) Model Accuracy AUC Recall Prec. F1 Kappa MCC TT (Sec) 0 KhiopsClassifier 0.9222 0.9688 0.8059 0.8602 0.8321 0.7815 0.7823 4.0110 Out[16]: <pre>KhiopsClassifier(auto_sort=True, internal_sort=None, key=None, n_features=100,\n                 n_pairs=0, n_trees=10, output_dir=None, verbose=False)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KhiopsClassifier<pre>KhiopsClassifier(auto_sort=True, internal_sort=None, key=None, n_features=100,\n                 n_pairs=0, n_trees=10, output_dir=None, verbose=False)</pre> In\u00a0[17]: Copied! <pre># we now store the Khiops results on our DataFrame \nresults_class_based_missing_values = pd.concat([results_class_based_missing_values, pull()], ignore_index=True)\nresults_class_based_missing_values.sort_values(by=\"Accuracy\",ascending=False)\n</pre> # we now store the Khiops results on our DataFrame  results_class_based_missing_values = pd.concat([results_class_based_missing_values, pull()], ignore_index=True) results_class_based_missing_values.sort_values(by=\"Accuracy\",ascending=False) Out[17]: Model Accuracy AUC Recall Prec. F1 Kappa MCC TT (Sec) 9 KhiopsClassifier 0.9222 0.9688 0.8059 0.8602 0.8321 0.7815 0.7823 4.011 0 Light Gradient Boosting Machine 0.9021 0.9526 0.7471 0.8274 0.7850 0.7219 0.7236 18.973 1 Gradient Boosting Classifier 0.8890 0.9443 0.6577 0.8445 0.7393 0.6702 0.6787 3.348 2 Random Forest Classifier 0.8819 0.9318 0.6775 0.7989 0.7330 0.6579 0.6617 2.467 3 Ada Boost Classifier 0.8809 0.9349 0.6590 0.8082 0.7258 0.6507 0.6564 1.173 4 Extra Trees Classifier 0.8593 0.9056 0.6486 0.7330 0.6880 0.5976 0.5996 2.812 5 Decision Tree Classifier 0.8390 0.7830 0.6751 0.6602 0.6674 0.5612 0.5614 0.455 6 Linear Discriminant Analysis 0.8387 0.8838 0.5232 0.7267 0.6082 0.5100 0.5210 0.570 7 Ridge Classifier 0.8373 0.0000 0.4680 0.7602 0.5792 0.4854 0.5075 0.326 8 Logistic Regression 0.7999 0.5844 0.2523 0.7530 0.3728 0.2884 0.3520 0.450 In\u00a0[18]: Copied! <pre># We'll plot the results. \n# Notice that we intentionally **exclude the computation time (TT in seconds)** from the overview, as its significance is minimal on small datasets,\n# and because the different scale (compared to scores range) does not ease the reading. \nplot_model_comparison(results_class_based_missing_values.drop('TT (Sec)',axis=1),'Comparison of Models with class-based missing values')\n</pre> # We'll plot the results.  # Notice that we intentionally **exclude the computation time (TT in seconds)** from the overview, as its significance is minimal on small datasets, # and because the different scale (compared to scores range) does not ease the reading.  plot_model_comparison(results_class_based_missing_values.drop('TT (Sec)',axis=1),'Comparison of Models with class-based missing values') <p>While the specific rates of missing values used in these experiments (20% for class '0' and 5% for class '1') may not be directly representative of real-world datasets, they serve to illustrate a crucial point:</p> <p>Khiops outperformed other models in this experiment when the missing data is not random and carries informative weight about the target variable. This shows the effectiveness of Khiops in leveraging the informational value of missing data, which is often not missing at random in real-world applications. Plus, Khiops automatically takes advantage of the information without requiring additional manual effort.</p> In\u00a0[19]: Copied! <pre>X_noise_and_outliers = X_class_based_missing_values.copy()\n\n# Introduce noise to 'age'\nnp.random.seed(42)\nnoise_age = np.random.normal(0, 3, len(X_noise_and_outliers))\nX_noise_and_outliers['age'] = X_noise_and_outliers['age'] + np.round(noise_age).astype(int)\n\n# Introduce noise to 'hours_per_week'\nnp.random.seed(43)\nnoise_hours_per_week = np.random.normal(0, 3, len(X_noise_and_outliers))\nX_noise_and_outliers['hours_per_week'] = X_noise_and_outliers['hours_per_week'] + np.round(noise_hours_per_week).astype(int)\n\n# Introduce outliers to 'age'\nnp.random.seed(42)\noutlier_indices_age = np.random.choice(X_noise_and_outliers.index, int(0.1 * len(X_noise_and_outliers)), replace=False)\nX_noise_and_outliers.loc[outlier_indices_age, 'age'] = X_noise_and_outliers['age'].max() * 30  # Setting to 30 times the max value\n\n# Introduce outliers to 'hours_per_week'\nnp.random.seed(43)\noutlier_indices_hours_per_week = np.random.choice(X_noise_and_outliers.index, int(0.1 * len(X_noise_and_outliers)), replace=False)\nX_noise_and_outliers.loc[outlier_indices_hours_per_week, 'hours_per_week'] = X_noise_and_outliers['hours_per_week'].max() * 30  # Setting to 30 times the max value\n\n# Introduce outliers to 'capital_gain'\nnp.random.seed(44)\noutlier_indices_capital_gain = np.random.choice(X_noise_and_outliers.index, int(0.1 * len(X_noise_and_outliers)), replace=False)\nX_noise_and_outliers.loc[outlier_indices_capital_gain, 'capital_gain'] = X_noise_and_outliers['capital_gain'].max() * 30  # Setting to 30 times the max value\n\nX_noise_and_outliers.head(10)\n</pre> X_noise_and_outliers = X_class_based_missing_values.copy()  # Introduce noise to 'age' np.random.seed(42) noise_age = np.random.normal(0, 3, len(X_noise_and_outliers)) X_noise_and_outliers['age'] = X_noise_and_outliers['age'] + np.round(noise_age).astype(int)  # Introduce noise to 'hours_per_week' np.random.seed(43) noise_hours_per_week = np.random.normal(0, 3, len(X_noise_and_outliers)) X_noise_and_outliers['hours_per_week'] = X_noise_and_outliers['hours_per_week'] + np.round(noise_hours_per_week).astype(int)  # Introduce outliers to 'age' np.random.seed(42) outlier_indices_age = np.random.choice(X_noise_and_outliers.index, int(0.1 * len(X_noise_and_outliers)), replace=False) X_noise_and_outliers.loc[outlier_indices_age, 'age'] = X_noise_and_outliers['age'].max() * 30  # Setting to 30 times the max value  # Introduce outliers to 'hours_per_week' np.random.seed(43) outlier_indices_hours_per_week = np.random.choice(X_noise_and_outliers.index, int(0.1 * len(X_noise_and_outliers)), replace=False) X_noise_and_outliers.loc[outlier_indices_hours_per_week, 'hours_per_week'] = X_noise_and_outliers['hours_per_week'].max() * 30  # Setting to 30 times the max value  # Introduce outliers to 'capital_gain' np.random.seed(44) outlier_indices_capital_gain = np.random.choice(X_noise_and_outliers.index, int(0.1 * len(X_noise_and_outliers)), replace=False) X_noise_and_outliers.loc[outlier_indices_capital_gain, 'capital_gain'] = X_noise_and_outliers['capital_gain'].max() * 30  # Setting to 30 times the max value  X_noise_and_outliers.head(10) Out[19]: age workclass fnlwgt education education_num marital_status occupation relationship race sex capital_gain capital_loss hours_per_week native_country Label 1 40.0 NaN 77516.0 Bachelors NaN Never-married Adm-clerical NaN White Male 2999970.0 0.0 41.0 United-States 2 50.0 NaN 83311.0 Bachelors 13.0 Married-civ-spouse NaN NaN White Male 0.0 NaN 3210.0 NaN 3 40.0 Private 215646.0 HS-grad NaN Divorced Handlers-cleaners Not-in-family NaN NaN 0.0 0.0 39.0 United-States 4 58.0 Private 234721.0 NaN NaN Married-civ-spouse Handlers-cleaners Husband Black Male 0.0 0.0 NaN NaN 5 2910.0 Private NaN Bachelors NaN Married-civ-spouse NaN Wife Black Female 0.0 0.0 43.0 Cuba 6 36.0 NaN 284582.0 NaN 14.0 Married-civ-spouse Exec-managerial Wife White Female 0.0 0.0 39.0 United-States 7 NaN NaN 160187.0 9th NaN Married-spouse-absent NaN Not-in-family Black Female NaN 0.0 3210.0 Jamaica 8 54.0 Self-emp-not-inc 209642.0 HS-grad 9.0 Married-civ-spouse Exec-managerial Husband NaN Male 0.0 0.0 51.0 United-States 9 30.0 Private 45781.0 Masters 14.0 Never-married Prof-specialty Not-in-family White Female 14084.0 0.0 54.0 United-States 10 44.0 Private 159449.0 Bachelors 13.0 Married-civ-spouse Exec-managerial Husband White Male 5178.0 0.0 39.0 United-States In\u00a0[20]: Copied! <pre>setup(pd.concat([X_noise_and_outliers, y], axis=1), target = 'class', session_id=123, verbose = False)\ncompare_models(include=[\"lr\",\"lightgbm\",\"gbc\",\"ada\",\"rf\",\"lda\",\"ridge\",\"et\",\"dt\"])\n</pre> setup(pd.concat([X_noise_and_outliers, y], axis=1), target = 'class', session_id=123, verbose = False) compare_models(include=[\"lr\",\"lightgbm\",\"gbc\",\"ada\",\"rf\",\"lda\",\"ridge\",\"et\",\"dt\"]) Model Accuracy AUC Recall Prec. F1 Kappa MCC TT (Sec) lightgbm Light Gradient Boosting Machine 0.8947 0.9470 0.7267 0.8138 0.7676 0.6999 0.7019 16.0500 gbc Gradient Boosting Classifier 0.8836 0.9387 0.6389 0.8364 0.7241 0.6520 0.6617 3.3910 rf Random Forest Classifier 0.8760 0.9236 0.6533 0.7924 0.7159 0.6375 0.6425 2.5520 ada Ada Boost Classifier 0.8710 0.9246 0.6313 0.7879 0.7007 0.6198 0.6261 1.2410 et Extra Trees Classifier 0.8525 0.8971 0.6411 0.7139 0.6753 0.5803 0.5818 2.8390 lda Linear Discriminant Analysis 0.8295 0.8644 0.4943 0.7050 0.5810 0.4781 0.4900 0.5740 ridge Ridge Classifier 0.8266 0.0000 0.4359 0.7312 0.5460 0.4473 0.4702 0.3310 dt Decision Tree Classifier 0.8174 0.7531 0.6299 0.6162 0.6228 0.5024 0.5026 0.4810 lr Logistic Regression 0.7656 0.5461 0.0633 0.5967 0.1144 0.0719 0.1351 0.5450 Out[20]: <pre>LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n               importance_type='split', learning_rate=0.1, max_depth=-1,\n               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,\n               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,\n               subsample_for_bin=200000, subsample_freq=0)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LGBMClassifier<pre>LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n               importance_type='split', learning_rate=0.1, max_depth=-1,\n               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,\n               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,\n               subsample_for_bin=200000, subsample_freq=0)</pre> In\u00a0[21]: Copied! <pre># we store the pycaret results on a dedicated DataFrame\nresults_noise_and_outliers = pull()\n</pre> # we store the pycaret results on a dedicated DataFrame results_noise_and_outliers = pull() In\u00a0[22]: Copied! <pre># the pyCaret setup for Khiops:\nsetup(pd.concat([X_noise_and_outliers, y], axis=1), target = 'class', session_id=123, verbose=False, preprocess=False)\ncompare_models(include=[KhiopsClassifier()])\n</pre> # the pyCaret setup for Khiops: setup(pd.concat([X_noise_and_outliers, y], axis=1), target = 'class', session_id=123, verbose=False, preprocess=False) compare_models(include=[KhiopsClassifier()]) Model Accuracy AUC Recall Prec. F1 Kappa MCC TT (Sec) 0 KhiopsClassifier 0.9182 0.9660 0.7928 0.8551 0.8227 0.7696 0.7706 4.1990 Out[22]: <pre>KhiopsClassifier(auto_sort=True, internal_sort=None, key=None, n_features=100,\n                 n_pairs=0, n_trees=10, output_dir=None, verbose=False)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KhiopsClassifier<pre>KhiopsClassifier(auto_sort=True, internal_sort=None, key=None, n_features=100,\n                 n_pairs=0, n_trees=10, output_dir=None, verbose=False)</pre> In\u00a0[23]: Copied! <pre># we now store the Khiops results on our DataFrame \nresults_noise_and_outliers = pd.concat([results_noise_and_outliers, pull()], ignore_index=True)\nresults_noise_and_outliers.sort_values(by=\"Accuracy\",ascending=False)\n</pre> # we now store the Khiops results on our DataFrame  results_noise_and_outliers = pd.concat([results_noise_and_outliers, pull()], ignore_index=True) results_noise_and_outliers.sort_values(by=\"Accuracy\",ascending=False) Out[23]: Model Accuracy AUC Recall Prec. F1 Kappa MCC TT (Sec) 9 KhiopsClassifier 0.9182 0.9660 0.7928 0.8551 0.8227 0.7696 0.7706 4.199 0 Light Gradient Boosting Machine 0.8947 0.9470 0.7267 0.8138 0.7676 0.6999 0.7019 16.050 1 Gradient Boosting Classifier 0.8836 0.9387 0.6389 0.8364 0.7241 0.6520 0.6617 3.391 2 Random Forest Classifier 0.8760 0.9236 0.6533 0.7924 0.7159 0.6375 0.6425 2.552 3 Ada Boost Classifier 0.8710 0.9246 0.6313 0.7879 0.7007 0.6198 0.6261 1.241 4 Extra Trees Classifier 0.8525 0.8971 0.6411 0.7139 0.6753 0.5803 0.5818 2.839 5 Linear Discriminant Analysis 0.8295 0.8644 0.4943 0.7050 0.5810 0.4781 0.4900 0.574 6 Ridge Classifier 0.8266 0.0000 0.4359 0.7312 0.5460 0.4473 0.4702 0.331 7 Decision Tree Classifier 0.8174 0.7531 0.6299 0.6162 0.6228 0.5024 0.5026 0.481 8 Logistic Regression 0.7656 0.5461 0.0633 0.5967 0.1144 0.0719 0.1351 0.545 In\u00a0[24]: Copied! <pre># We'll plot the results. \n# Notice that we intentionally **exclude the computation time (TT in seconds)** from the overview, as its significance is minimal on small datasets,\n# and because the different scale (compared to scores range) does not ease the reading. \nplot_model_comparison(results_class_based_missing_values.drop('TT (Sec)',axis=1),'Comparison of Models with missing values, noise and outliers')\n</pre> # We'll plot the results.  # Notice that we intentionally **exclude the computation time (TT in seconds)** from the overview, as its significance is minimal on small datasets, # and because the different scale (compared to scores range) does not ease the reading.  plot_model_comparison(results_class_based_missing_values.drop('TT (Sec)',axis=1),'Comparison of Models with missing values, noise and outliers') In\u00a0[25]: Copied! <pre># Identify indices of 0 and 1 classes\nindices_majority = np.where(y == 0)[0]\nindices_minority = np.where(y == 1)[0]\n\n# Randomly sample 3% of the 1 class\nnp.random.seed(42)\nrandom_indices_minority = np.random.choice(indices_minority, \n                                            int(0.03 * len(indices_minority)),\n                                            replace=False)\n\n# Concatenate the indices of the 0 class with the downsampled indices of the 1 class\nfinal_indices = np.concatenate([indices_majority, random_indices_minority])\n\n# Create the imbalanced feature matrix and target vector\nX_unbalanced = X_noise_and_outliers.iloc[final_indices, :]\ny_unbalanced = y.iloc[final_indices]\n</pre> # Identify indices of 0 and 1 classes indices_majority = np.where(y == 0)[0] indices_minority = np.where(y == 1)[0]  # Randomly sample 3% of the 1 class np.random.seed(42) random_indices_minority = np.random.choice(indices_minority,                                              int(0.03 * len(indices_minority)),                                             replace=False)  # Concatenate the indices of the 0 class with the downsampled indices of the 1 class final_indices = np.concatenate([indices_majority, random_indices_minority])  # Create the imbalanced feature matrix and target vector X_unbalanced = X_noise_and_outliers.iloc[final_indices, :] y_unbalanced = y.iloc[final_indices] In\u00a0[26]: Copied! <pre># we print the number of instances per class, showing the high imbalance\ny_unbalanced.value_counts()\n</pre> # we print the number of instances per class, showing the high imbalance y_unbalanced.value_counts() Out[26]: <pre>0    37155\n1      350\nName: class, dtype: int64</pre> In\u00a0[27]: Copied! <pre>setup(pd.concat([X_unbalanced, y_unbalanced], axis=1), target = 'class', session_id=123, verbose = False)\ncompare_models(include=[\"lr\",\"lightgbm\",\"gbc\",\"ada\",\"rf\",\"lda\",\"ridge\",\"et\",\"dt\"])\n</pre> setup(pd.concat([X_unbalanced, y_unbalanced], axis=1), target = 'class', session_id=123, verbose = False) compare_models(include=[\"lr\",\"lightgbm\",\"gbc\",\"ada\",\"rf\",\"lda\",\"ridge\",\"et\",\"dt\"]) Model Accuracy AUC Recall Prec. F1 Kappa MCC TT (Sec) ada Ada Boost Classifier 0.9920 0.9702 0.2787 0.6759 0.3882 0.3850 0.4266 1.0170 gbc Gradient Boosting Classifier 0.9919 0.9786 0.2542 0.6532 0.3631 0.3599 0.4023 2.6730 lightgbm Light Gradient Boosting Machine 0.9915 0.9737 0.2910 0.5869 0.3847 0.3811 0.4070 17.6940 rf Random Forest Classifier 0.9911 0.9252 0.0822 0.6500 0.1451 0.1434 0.2274 1.1530 lr Logistic Regression 0.9907 0.4910 0.0000 0.0000 0.0000 0.0000 0.0000 0.4510 ridge Ridge Classifier 0.9907 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.2940 et Extra Trees Classifier 0.9904 0.8492 0.1188 0.4320 0.1826 0.1795 0.2197 1.4220 dt Decision Tree Classifier 0.9856 0.6352 0.2782 0.2639 0.2679 0.2607 0.2623 0.3180 lda Linear Discriminant Analysis 0.9824 0.8734 0.1887 0.1528 0.1681 0.1594 0.1606 0.4810 Out[27]: <pre>AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',\n                   estimator=None, learning_rate=1.0, n_estimators=50,\n                   random_state=123)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.AdaBoostClassifier<pre>AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',\n                   estimator=None, learning_rate=1.0, n_estimators=50,\n                   random_state=123)</pre> In\u00a0[28]: Copied! <pre># we store the pycaret results on a dedicated DataFrame\nresults_unbalanced = pull()\n</pre> # we store the pycaret results on a dedicated DataFrame results_unbalanced = pull() In\u00a0[29]: Copied! <pre># the pyCaret setup for Khiops:\nsetup(pd.concat([X_unbalanced, y_unbalanced], axis=1), target = 'class', session_id=123, verbose=False, preprocess=False)\ncompare_models(include=[KhiopsClassifier()])\n</pre> # the pyCaret setup for Khiops: setup(pd.concat([X_unbalanced, y_unbalanced], axis=1), target = 'class', session_id=123, verbose=False, preprocess=False) compare_models(include=[KhiopsClassifier()]) Model Accuracy AUC Recall Prec. F1 Kappa MCC TT (Sec) 0 KhiopsClassifier 0.9931 0.9849 0.3395 0.8267 0.4722 0.4695 0.5210 1.8190 Out[29]: <pre>KhiopsClassifier(auto_sort=True, internal_sort=None, key=None, n_features=100,\n                 n_pairs=0, n_trees=10, output_dir=None, verbose=False)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KhiopsClassifier<pre>KhiopsClassifier(auto_sort=True, internal_sort=None, key=None, n_features=100,\n                 n_pairs=0, n_trees=10, output_dir=None, verbose=False)</pre> In\u00a0[30]: Copied! <pre># we now store the Khiops results on our DataFrame \nresults_unbalanced = pd.concat([results_unbalanced, pull()], ignore_index=True)\nresults_unbalanced.sort_values(by=\"Accuracy\",ascending=False)\n</pre> # we now store the Khiops results on our DataFrame  results_unbalanced = pd.concat([results_unbalanced, pull()], ignore_index=True) results_unbalanced.sort_values(by=\"Accuracy\",ascending=False) Out[30]: Model Accuracy AUC Recall Prec. F1 Kappa MCC TT (Sec) 9 KhiopsClassifier 0.9931 0.9849 0.3395 0.8267 0.4722 0.4695 0.5210 1.819 0 Ada Boost Classifier 0.9920 0.9702 0.2787 0.6759 0.3882 0.3850 0.4266 1.017 1 Gradient Boosting Classifier 0.9919 0.9786 0.2542 0.6532 0.3631 0.3599 0.4023 2.673 2 Light Gradient Boosting Machine 0.9915 0.9737 0.2910 0.5869 0.3847 0.3811 0.4070 17.694 3 Random Forest Classifier 0.9911 0.9252 0.0822 0.6500 0.1451 0.1434 0.2274 1.153 4 Logistic Regression 0.9907 0.4910 0.0000 0.0000 0.0000 0.0000 0.0000 0.451 5 Ridge Classifier 0.9907 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.294 6 Extra Trees Classifier 0.9904 0.8492 0.1188 0.4320 0.1826 0.1795 0.2197 1.422 7 Decision Tree Classifier 0.9856 0.6352 0.2782 0.2639 0.2679 0.2607 0.2623 0.318 8 Linear Discriminant Analysis 0.9824 0.8734 0.1887 0.1528 0.1681 0.1594 0.1606 0.481 In\u00a0[31]: Copied! <pre># We'll plot the results. \n# Notice that we intentionally **exclude the computation time (TT in seconds)** from the overview, as its significance is minimal on small datasets,\n# and because the different scale (compared to scores range) does not ease the reading. \nplot_model_comparison(results_unbalanced.drop('TT (Sec)',axis=1),'Comparison of Models with imbalance, missing values, noise and outliers')\n</pre> # We'll plot the results.  # Notice that we intentionally **exclude the computation time (TT in seconds)** from the overview, as its significance is minimal on small datasets, # and because the different scale (compared to scores range) does not ease the reading.  plot_model_comparison(results_unbalanced.drop('TT (Sec)',axis=1),'Comparison of Models with imbalance, missing values, noise and outliers') <p>Khiops' ability to efficiently process raw, uncleaned data saves you valuable time, for a better understanding :</p> <ul> <li>business needs</li> <li>the data collected and its biases</li> <li>the methodology to be implemented</li> <li>trained models and their interpretation...</li> </ul> <p>In summary, the efficiency of Khiops in dealing with raw data frees you to concentrate on the core issues, enabling a more effective and targeted approach to problem-solving.</p> In\u00a0[32]: Copied! <pre>setup(pd.concat([X_class_based_missing_values, y], axis=1), target = 'class', session_id=123, verbose=False)\ncompare_models(include=[KhiopsClassifier()])\n</pre> setup(pd.concat([X_class_based_missing_values, y], axis=1), target = 'class', session_id=123, verbose=False) compare_models(include=[KhiopsClassifier()]) Model Accuracy AUC Recall Prec. F1 Kappa MCC TT (Sec) 0 KhiopsClassifier 0.8974 0.9517 0.7286 0.8225 0.7726 0.7066 0.7089 10.5330 Out[32]: <pre>KhiopsClassifier(auto_sort=True, internal_sort=None, key=None, n_features=100,\n                 n_pairs=0, n_trees=10, output_dir=None, verbose=False)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KhiopsClassifier<pre>KhiopsClassifier(auto_sort=True, internal_sort=None, key=None, n_features=100,\n                 n_pairs=0, n_trees=10, output_dir=None, verbose=False)</pre> In\u00a0[33]: Copied! <pre>results_Khiops_raw_vs_data_prep = pull()\nresults_Khiops_raw_vs_data_prep['Model'].replace({\n    'KhiopsClassifier': 'Khiops on prepared Data',\n    }, inplace=True)\n</pre> results_Khiops_raw_vs_data_prep = pull() results_Khiops_raw_vs_data_prep['Model'].replace({     'KhiopsClassifier': 'Khiops on prepared Data',     }, inplace=True) In\u00a0[34]: Copied! <pre>setup(pd.concat([X_class_based_missing_values, y], axis=1), target = 'class', session_id=123, preprocess=False, verbose=False)\ncompare_models(include=[KhiopsClassifier()])\n</pre> setup(pd.concat([X_class_based_missing_values, y], axis=1), target = 'class', session_id=123, preprocess=False, verbose=False) compare_models(include=[KhiopsClassifier()]) Model Accuracy AUC Recall Prec. F1 Kappa MCC TT (Sec) 0 KhiopsClassifier 0.9222 0.9688 0.8059 0.8602 0.8321 0.7815 0.7823 3.8590 Out[34]: <pre>KhiopsClassifier(auto_sort=True, internal_sort=None, key=None, n_features=100,\n                 n_pairs=0, n_trees=10, output_dir=None, verbose=False)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KhiopsClassifier<pre>KhiopsClassifier(auto_sort=True, internal_sort=None, key=None, n_features=100,\n                 n_pairs=0, n_trees=10, output_dir=None, verbose=False)</pre> In\u00a0[35]: Copied! <pre>results_Khiops_raw_vs_data_prep = pd.concat([results_Khiops_raw_vs_data_prep, pull()], ignore_index=True)\nresults_Khiops_raw_vs_data_prep['Model'].replace({\n    'KhiopsClassifier': 'Khiops on Raw Data',\n    }, inplace=True)\nresults_Khiops_raw_vs_data_prep.sort_values(by=\"Accuracy\",ascending=False)\n</pre> results_Khiops_raw_vs_data_prep = pd.concat([results_Khiops_raw_vs_data_prep, pull()], ignore_index=True) results_Khiops_raw_vs_data_prep['Model'].replace({     'KhiopsClassifier': 'Khiops on Raw Data',     }, inplace=True) results_Khiops_raw_vs_data_prep.sort_values(by=\"Accuracy\",ascending=False) Out[35]: Model Accuracy AUC Recall Prec. F1 Kappa MCC TT (Sec) 1 Khiops on Raw Data 0.9222 0.9688 0.8059 0.8602 0.8321 0.7815 0.7823 3.859 0 Khiops on prepared Data 0.8974 0.9517 0.7286 0.8225 0.7726 0.7066 0.7089 10.533 In\u00a0[36]: Copied! <pre>plot_model_comparison(results_Khiops_raw_vs_data_prep.drop('TT (Sec)',axis=1),'Raw vs. Prepared: How Data Preparation Affects Khiops')\n</pre> plot_model_comparison(results_Khiops_raw_vs_data_prep.drop('TT (Sec)',axis=1),'Raw vs. Prepared: How Data Preparation Affects Khiops')"},{"location":"tutorials/Notebooks/No_data_Cleaning/#no-need-for-data-preparation","title":"No Need for Data Preparation\u00b6","text":""},{"location":"tutorials/Notebooks/No_data_Cleaning/#introduction","title":"Introduction\u00b6","text":"<p>In machine learning, optimizing model performance often involves complex data preparation, manual feature engineering and hyper-parameter tuning. In real-world applications where data quality is imperfect and uncontrolled, the cost and effort of bringing data to a clean state can outweigh the commercial value derived. While standard AutoML solutions aim to simplify these processes, they often operate as global orchestrators, exploring various learning algorithms and data preparation techniques. While powerful, they are limited in their scalability as they always involve multiple iterations, and often the inner workings of the selected model remain a black box.</p> <p>Khiops is not simply another tool or orchestrator of existing methods in the machine learning arsenal. It is built around an original formalism and dedicated optimization algorithms. Trained models are intrinsically interpretable, robust and free of hyperparameters. This means you don't need to run multiple iterations with different configurations (no grid search), nor struggle with data preparation through elaborate pipelines. With Khiops, you can process raw data directly and seamlessly, even if it comes from multiple tables (relational or multi-table data).</p> <p>In this notebook, we focus on Khiops' ability to process data in its rawest possible form, highlighting its strengths in automatic data preparation; other notebooks will look at other benefits such as interpretability, automated feature engineering (when dealing with multi-tables) and computational efficiency in large datasets.</p> <p>Purpose of the experiment:</p> <p>Our experiment starts with an \"ideal\" data set with no quality problems. We then add various types of defects common in real data: class-dependent missing values, labeling noise, outliers and class imbalance. This approach illustrates the strength of Khiops: its robustness and adaptability to changes in data quality, whereas other competing methods may require additional pre-processing steps to maintain their performance.</p> <p>Experimental background:</p> <p>This study uses the Adult UCI dataset to compare Khiops to standard machine learning models, all evaluated using the same methodology. Our experiments cover the following steps:</p> <ul> <li>Working with the \"ideal\" dataset,</li> <li>Adding class-dependent missing values (reflecting real-world scenarios where missing values are not randomly and uniformly distributed across classes),</li> <li>Adding noise and outliers,</li> <li>Adding class imbalance.</li> </ul> <p>To keep the notebook concise, readable and educational, the pre-processing steps required for other methods are handled by pyCaret, a machine learning library that handles tasks such as missing value imputation and label encoding. Our experimental protocol guarantees a fair comparison.</p> <p>This notebook is executed on a <code>n2-standard-2 (2 vCPUs, 8 GB RAM)</code> machine (Vertex AI).</p>"},{"location":"tutorials/Notebooks/No_data_Cleaning/#installation-and-set-up","title":"Installation and set up\u00b6","text":"<p>If you do not use our official <code>khiops-notebook</code> Jupyter Docker image, you may have to install khiops locally using <code>conda</code>:</p>"},{"location":"tutorials/Notebooks/No_data_Cleaning/#import-and-preparation-of-data","title":"Import and Preparation of Data\u00b6","text":"<p>For this notebook, we use the \"Adult\" UCI standard Dataset. More details available here.</p> <p>This dataset is also available on our khiops-samples repository on Github.</p>"},{"location":"tutorials/Notebooks/No_data_Cleaning/#benchmark","title":"Benchmark\u00b6","text":"<p>pyCaret drives the benchmark, in order to maintain a consistent methodology across all models and simplifying the notebook. There is two different pyCaret setup:</p> <ul> <li>Khiops is trained using <code>preprocess=False</code> . Indeed, Khiops is able to work efficiently with objects like <code>string</code>, eliminating the need for preliminary data preparation such as label encoding or imputation. It contrasts sharply with traditional machine learning models, which require to make choices for data preprocessing.</li> <li>All other standard models are train with <code>preprocess=True</code>. We use pyCaret to prepare the data for these other models, leveling the playing field.</li> </ul> <p>It's important to note that integrating Khiops into a pyCaret pipeline including data preparation would be counterproductive. The pyCaret data preparation steps that benefit other models can actually degrade Khiops' performance, as it's designed to leverage the full detailed raw data (e.g. by exploiting the missing values distribution). We will demonstrate this in the final section of this notebook.</p> <p>By default, pyCaret employes a stratified cross-validation. It's worth noting that one of Khiops' key advantages is its ability to perform robustly without the need for cross-validation (as it is hyperparameter-free and does not require data preprocessing). In real-world applications, a simple train-test split suffices for Khiops to learn an effective model, provided there is enough data.  The 10-folds cross-validation is thus more of a formality to average the performance metrics but is unlikely to change the model.</p>"},{"location":"tutorials/Notebooks/No_data_Cleaning/#training-on-the-clean-adult-dataset","title":"Training on the clean Adult dataset\u00b6","text":"<p>We can start by comparing Khiops Classifier with others using the clean adult data set (i.e., before introducing missing values and other defects).</p> <p>We prepare the data for modeling. To do this, we split the data into (X, y) where X contains the features (sepal length, width, etc.) and y contains the labels (the Class column). This is not necessary for pyCaret, but it will facilitate the continuation of our experiments.</p>"},{"location":"tutorials/Notebooks/No_data_Cleaning/#introducing-missing-values","title":"Introducing missing values\u00b6","text":"<p>In real-world applications, missing values often do not occur entirely at random. Instead, the absence of value can be indicative of underlying patterns or conditions. For example, particular sub-categories may not be available for specific product categories, illustrating missingness dependent on other observed data. Alternatively, some targeted-class users may be reluctant to disclose specific information, reflecting missingness dependent on unobserved data. In these cases, the missing data itself becomes an additional source of information, carrying potentially significant implications for the problem at hand.</p> <p>To emulate such complex patterns in realistic conditions, we will introduce Class-Based Missing Values into the dataset. The probability of a value being missing will be determined by its corresponding target class. We have chosen differentiating missing rates to accentuate the significance of the missing value pattern: a 20% missing data rate for instances belonging to class '0' and a 5% rate for those belonging to class '1'.</p> <p>By focusing solely on Class-Based Missing Values, we aim to highlight the adaptability and power of Khiops in leveraging missing data patterns, as compared to other machine learning models that often require data imputation or other preparation steps to handle missing information effectively.</p>"},{"location":"tutorials/Notebooks/No_data_Cleaning/#adding-noise-and-outliers","title":"Adding Noise and Outliers\u00b6","text":"<p>Noise and outliers are almost inevitable and can significantly impact the performance of machine learning models. Noise can be defined as random variations that obscure patterns, while outliers are extreme values that deviate considerably from the other observations. In this section, we introduce noise and outliers to key columns to test how well Khiops and other standard models cope with such challenges.</p> <p>For this experiment, we'll use the following arbitrary values:</p> <ul> <li>Noise is introduced as a normal distribution with a standard deviation set to 3.</li> <li>Outliers will be introduced in 10% of the instances and will be 30 times larger than the actual maximum value in the column.</li> </ul> <p>It's important to note that while these values are not likely to mirror real-world distributions, they evaluate the robustness of the tested models. And when the pyCaret Data Preparation should prevent impact for standard models, this experiment focuses mainly on showing that Khiops behaves correctly when dealing with poor quality raw data.</p>"},{"location":"tutorials/Notebooks/No_data_Cleaning/#adding-high-class-imbalance","title":"Adding High Class Imbalance\u00b6","text":"<p>The class distribution is often highly unbalanced in many domains, like fraud detection, healthcare, and anomaly detection. In these situations, one class is significantly underrepresented. This imbalance can be challenging for standard machine learning algorithms, which often assume an approximately equal distribution of classes. If not properly managed, the class imbalance can result in misleadingly high accuracy rates and poor identification of the minority class, which is often the class of most interest. This section will explore the impact of highly imbalanced class distribution on model performance (with only 1% instances of the class '1').</p>"},{"location":"tutorials/Notebooks/No_data_Cleaning/#raw-vs-prepared-how-khiops-challenges-conventional-data-preparation","title":"Raw vs. Prepared: How Khiops Challenges Conventional Data Preparation\u00b6","text":"<p>In machine learning, data preparation is often seen as an essential step to improve the performance of models. This common wisdom, however, doesn't always hold true, especially regarding Khiops. Unlike other models requiring intricate data preprocessing, Khiops excels when fed raw, unaltered data. In this section, we'll test this unique characteristic of Khiops. We'll use PyCaret for its data preparation pipelines as a contrasting framework. Our experiment compares the performance of Khiops on raw data against its performance within a data preparation pipeline. The aim is not to disparage PyCaret (it's a great library actually!) but to highlight that what is generally beneficial for most models can be counterproductive for Khiops.</p>"},{"location":"tutorials/Notebooks/Optimal_Encoding/","title":"Interpretable Variable Encoding","text":"In\u00a0[41]: Copied! <pre>#!conda install -y -c conda-forge -c khiops khiops\n</pre> #!conda install -y -c conda-forge -c khiops khiops <p>For the experiments, you also need some external libraries you can install via <code>pip</code>:</p> In\u00a0[42]: Copied! <pre># Installation of external libraries\n#!pip install matplotlib seaborn lightgbm\n\n# Note: Installing PyCaret can sometimes be complex due to its dependencies. \n# If you encounter any issues, please refer to the PyCaret documentation for detailed installation instructions:\n# https://pycaret.gitbook.io/docs/get-started/installation\n#!pip install pycaret\n</pre> # Installation of external libraries #!pip install matplotlib seaborn lightgbm  # Note: Installing PyCaret can sometimes be complex due to its dependencies.  # If you encounter any issues, please refer to the PyCaret documentation for detailed installation instructions: # https://pycaret.gitbook.io/docs/get-started/installation #!pip install pycaret <p>We now import all the dependencies here:</p> In\u00a0[43]: Copied! <pre>import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport time\nfrom random import randint\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom khiops.sklearn.estimators import KhiopsEncoder\nfrom khiops.sklearn import KhiopsClassifier\n\nfrom pycaret.classification import *\n</pre> import pandas as pd import matplotlib.pyplot as plt import numpy as np import time from random import randint import seaborn as sns  from sklearn.model_selection import train_test_split from khiops.sklearn.estimators import KhiopsEncoder from khiops.sklearn import KhiopsClassifier  from pycaret.classification import * In\u00a0[44]: Copied! <pre># Method 1: Load data directly from GitHub (recommended for quick tests or small datasets)\nurl = \"https://raw.githubusercontent.com/KhiopsML/khiops-samples/10.2.4/Adult/Adult.txt\"\ndf = pd.read_csv(url, delimiter='\\t',index_col=\"Label\")\n\n# Method 2: Load data locally after downloading all Khiops samples (best for offline use or large datasets)\n#If the samples have not been downloaded yet:\n#from khiops.tools import download_datasets\n#download_datasets() \n#\n#from os import path\n#from khiops import core as kh\n#adult_path = path.join(kh.get_samples_dir(), \"Adult\", \"Adult.txt\")\n#df = pd.read_csv(adult_path, sep=\"\\t\",index_col=\"Label\")\n\n# Display the first 10 records from the dataset\ndf.head(10)\n</pre> # Method 1: Load data directly from GitHub (recommended for quick tests or small datasets) url = \"https://raw.githubusercontent.com/KhiopsML/khiops-samples/10.2.4/Adult/Adult.txt\" df = pd.read_csv(url, delimiter='\\t',index_col=\"Label\")  # Method 2: Load data locally after downloading all Khiops samples (best for offline use or large datasets) #If the samples have not been downloaded yet: #from khiops.tools import download_datasets #download_datasets()  # #from os import path #from khiops import core as kh #adult_path = path.join(kh.get_samples_dir(), \"Adult\", \"Adult.txt\") #df = pd.read_csv(adult_path, sep=\"\\t\",index_col=\"Label\")  # Display the first 10 records from the dataset df.head(10) Out[44]: age workclass fnlwgt education education_num marital_status occupation relationship race sex capital_gain capital_loss hours_per_week native_country class Label 1 39 State-gov 77516 Bachelors 13 Never-married Adm-clerical Not-in-family White Male 2174 0 40 United-States less 2 50 Self-emp-not-inc 83311 Bachelors 13 Married-civ-spouse Exec-managerial Husband White Male 0 0 13 United-States less 3 38 Private 215646 HS-grad 9 Divorced Handlers-cleaners Not-in-family White Male 0 0 40 United-States less 4 53 Private 234721 11th 7 Married-civ-spouse Handlers-cleaners Husband Black Male 0 0 40 United-States less 5 28 Private 338409 Bachelors 13 Married-civ-spouse Prof-specialty Wife Black Female 0 0 40 Cuba less 6 37 Private 284582 Masters 14 Married-civ-spouse Exec-managerial Wife White Female 0 0 40 United-States less 7 49 Private 160187 9th 5 Married-spouse-absent Other-service Not-in-family Black Female 0 0 16 Jamaica less 8 52 Self-emp-not-inc 209642 HS-grad 9 Married-civ-spouse Exec-managerial Husband White Male 0 0 45 United-States more 9 31 Private 45781 Masters 14 Never-married Prof-specialty Not-in-family White Female 14084 0 50 United-States more 10 42 Private 159449 Bachelors 13 Married-civ-spouse Exec-managerial Husband White Male 5178 0 40 United-States more In\u00a0[45]: Copied! <pre># Drop the \"class\" column to create the feature set (X).\nX = df.drop(\"class\", axis=1)\n</pre> # Drop the \"class\" column to create the feature set (X). X = df.drop(\"class\", axis=1) In\u00a0[46]: Copied! <pre># Extract the \"class\" column to create the target labels (y).\ny = df[\"class\"].map({'less': 0, 'more': 1})\n</pre> # Extract the \"class\" column to create the target labels (y). y = df[\"class\"].map({'less': 0, 'more': 1}) In\u00a0[47]: Copied! <pre># Splitting the dataset into training and test sets. 10% of the data is reserved for testing, and we set a random seed for reproducibility.\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n</pre> # Splitting the dataset into training and test sets. 10% of the data is reserved for testing, and we set a random seed for reproducibility. X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42) In\u00a0[48]: Copied! <pre># Initialize a KhiopsEncoder with specified transformation types for both categorical and numerical data.\n# Here, \"part_label\" means that the variables will be encoded using labels derived from the Khiops discretization.\nencoder = KhiopsEncoder(transform_type_categorical=\"part_label\", transform_type_numerical=\"part_label\")\n\n# Fit the encoder to the training data and simultaneously transform it.\n# The encoder learns the optimal encoding based on the provided features and target.\nX_encoded = encoder.fit_transform(X_train, y_train)\n</pre> # Initialize a KhiopsEncoder with specified transformation types for both categorical and numerical data. # Here, \"part_label\" means that the variables will be encoded using labels derived from the Khiops discretization. encoder = KhiopsEncoder(transform_type_categorical=\"part_label\", transform_type_numerical=\"part_label\")  # Fit the encoder to the training data and simultaneously transform it. # The encoder learns the optimal encoding based on the provided features and target. X_encoded = encoder.fit_transform(X_train, y_train) In\u00a0[49]: Copied! <pre># Convert the transformed (encoded) training data into a DataFrame.\n# The column names for this DataFrame are fetched from the encoder's 'feature_names_out_' attribute.\nX_transformed = pd.DataFrame(X_encoded, columns = encoder.feature_names_out_)\n\n# Display the first 5 rows of the transformed training data to get a quick overview of the encoding.\nX_transformed.head(5)\n</pre> # Convert the transformed (encoded) training data into a DataFrame. # The column names for this DataFrame are fetched from the encoder's 'feature_names_out_' attribute. X_transformed = pd.DataFrame(X_encoded, columns = encoder.feature_names_out_)  # Display the first 5 rows of the transformed training data to get a quick overview of the encoding. X_transformed.head(5) Out[49]: LabelPage LabelPworkclass LabelPfnlwgt LabelPeducation LabelPeducation_num LabelPmarital_status LabelPoccupation LabelPrelationship LabelPrace LabelPsex LabelPcapital_gain LabelPcapital_loss LabelPhours_per_week LabelPnative_country 0 ]21.5;23.5] {Private, Without-pay, Never-worked} ]-inf;80534] {Bachelors} ]12.5;13.5] {Never-married, Separated} {Other-service, Priv-house-serv} {Own-child} {White, Asian-Pac-Islander} {Male} ]-inf;57] ]-inf;70] ]39.5;40.5] {United-States, Germany, Cuba, ...} 1 ]42.5;55.5] {Private, Without-pay, Never-worked} ]80534;201108.5] {11th, 10th, 7th-8th, ...} ]-inf;8.5] {Married-civ-spouse, Married-AF-spouse} {Prof-specialty, Armed-Forces} {Husband, Wife} {White, Asian-Pac-Islander} {Female} ]-inf;57] ]-inf;70] ]43.5;49.5] {United-States, Germany, Cuba, ...} 2 ]42.5;55.5] {Private, Without-pay, Never-worked} ]80534;201108.5] {HS-grad} ]8.5;9.5] {Married-civ-spouse, Married-AF-spouse} {Exec-managerial} {Husband, Wife} {White, Asian-Pac-Islander} {Female} ]-inf;57] ]-inf;70] ]-inf;34.5] {United-States, Germany, Cuba, ...} 3 ]23.5;25.5] {Private, Without-pay, Never-worked} ]80534;201108.5] {Assoc-voc, Assoc-acdm} ]10.5;12.5] {Married-civ-spouse, Married-AF-spouse} {Craft-repair, Transport-moving} {Husband, Wife} {White, Asian-Pac-Islander} {Male} ]-inf;57] ]-inf;70] ]43.5;49.5] {United-States, Germany, Cuba, ...} 4 ]23.5;25.5] {Private, Without-pay, Never-worked} ]80534;201108.5] {HS-grad} ]8.5;9.5] {Married-civ-spouse, Married-AF-spouse} {Other-service, Priv-house-serv} {Husband, Wife} {White, Asian-Pac-Islander} {Male} ]-inf;57] ]-inf;70] ]39.5;40.5] {Mexico, Puerto-Rico, El-Salvador, ...} <p>Let's compare the number of unique values between the original and encoded datasets. This will give us insights into Khiops' encoding strategy and its granularity, highlighting how it captures patterns relative to the target variable.</p> In\u00a0[50]: Copied! <pre># Retrieve the number of unique values for each column\nunique_vals_orig = X_train.nunique()\nunique_vals_encoded = X_transformed.nunique()\n\n# Create a comparison dataframe with the same index\ncomparison = pd.DataFrame(index=unique_vals_orig.index)\ncomparison['Original'] = unique_vals_orig\ncomparison['Encoded'] = unique_vals_encoded.values  # Use \".values\" to ignore the index and align the values by order\n\n# Display the result to see the reduction of distinct values for each feature\ncomparison\n</pre> # Retrieve the number of unique values for each column unique_vals_orig = X_train.nunique() unique_vals_encoded = X_transformed.nunique()  # Create a comparison dataframe with the same index comparison = pd.DataFrame(index=unique_vals_orig.index) comparison['Original'] = unique_vals_orig comparison['Encoded'] = unique_vals_encoded.values  # Use \".values\" to ignore the index and align the values by order  # Display the result to see the reduction of distinct values for each feature comparison  Out[50]: Original Encoded age 74 12 workclass 8 4 fnlwgt 26655 3 education 16 7 education_num 16 7 marital_status 7 3 occupation 14 7 relationship 6 5 race 5 2 sex 2 2 capital_gain 121 23 capital_loss 99 17 hours_per_week 96 9 native_country 41 3 <p>We can make several remarks here:</p> <ul> <li>For many variables, Khiops has reduced the granularity. Look at age, which originally had 74 unique values but has been discretized into 12 categories. Similarly, fnlwgt had 26,655 unique values, but post-encoding, it's represented with just 3 categories.</li> <li>For variables with fewer unique values like sex, the encoding hasn't changed the cardinality.</li> </ul> <p>Essentially, this encoded representation capture useful patterns in data, since it's optimized for predicting the target based independently on each variable.</p> In\u00a0[51]: Copied! <pre># Reset indices to ensure alignment\nX_reset = X_train.reset_index(drop=True)\nX_transformed_reset = X_transformed.reset_index(drop=True)\n\n# Combine original 'age' values and their transformed values into a single dataframe\ncombined_age_df = pd.DataFrame({\n    'Original_age': X_reset['age'],\n    'Encoded_age': X_transformed_reset['LabelPage']\n})\n\n# Display the first few rows to see the transformation\ncombined_age_df.head(10)\n</pre> # Reset indices to ensure alignment X_reset = X_train.reset_index(drop=True) X_transformed_reset = X_transformed.reset_index(drop=True)  # Combine original 'age' values and their transformed values into a single dataframe combined_age_df = pd.DataFrame({     'Original_age': X_reset['age'],     'Encoded_age': X_transformed_reset['LabelPage'] })  # Display the first few rows to see the transformation combined_age_df.head(10)  Out[51]: Original_age Encoded_age 0 23 ]21.5;23.5] 1 46 ]42.5;55.5] 2 46 ]42.5;55.5] 3 24 ]23.5;25.5] 4 24 ]23.5;25.5] 5 43 ]42.5;55.5] 6 33 ]30.5;33.5] 7 24 ]23.5;25.5] 8 37 ]35.5;42.5] 9 53 ]42.5;55.5] <p>From the original 74 unique values of the \"age\" feature, we've reduced it to 12 encoded intervals. These interval bounds are optimized by Khiops in relation to the target variable.</p> <p>Now, let's first examine the distribution of the original \"age\" values. Following that, we'll delve into the Khiops' discretization intervals.</p> In\u00a0[52]: Copied! <pre># Displaying the original age distribution\nplt.figure(figsize=(12, 7))\nX_train['age'].hist(bins=30, edgecolor='black')\nplt.title('Distribution of Original Age Values')\nplt.xlabel('Age')\nplt.ylabel('Number of Individuals')\nplt.show()\n</pre> # Displaying the original age distribution plt.figure(figsize=(12, 7)) X_train['age'].hist(bins=30, edgecolor='black') plt.title('Distribution of Original Age Values') plt.xlabel('Age') plt.ylabel('Number of Individuals') plt.show() <p>The above discretization is unsupervised, i.e. the target is not used in its construction. This histogram approximates the distribution the variable <code>age</code> without taking the learning task into account.</p> <p>Let's now take a look at the encoding provided by Khiops, which corresponds to another type of discretization, this time supervised. A discretization model trained by Khiops is equivalent to a univariate classifier whose aim is to estimate the class probability as a function of a single input variable (or more precisely, the interval in which the numerical value falls). The aim is therefore to identify the interval bounds that optimize probabilistic inference of the class.</p> <p>Now, let's visualize  the class distribution across these intervals.</p> In\u00a0[53]: Copied! <pre># Merging the encoded ages, original ages, and target classes\ncombined_age_class_df = pd.DataFrame({\n    'Original_age': X_train['age'].reset_index(drop=True),\n    'Encoded_age': X_transformed['LabelPage'].reset_index(drop=True),\n    'Class': y_train.reset_index(drop=True)\n})\n\n# Grouping by encoded age and by class, then counting the occurrences\ngrouped = combined_age_class_df.groupby(['Encoded_age', 'Class']).size().unstack()\n\n# Sorting the bins\nsorted_bins = combined_age_df.sort_values(\"Original_age\").Encoded_age.drop_duplicates().tolist()\ngrouped = grouped.reindex(sorted_bins)\n\n# Displaying the stacked bar chart\ngrouped.plot(kind='bar', stacked=True, figsize=(12, 7))\nplt.title('Distribution of Instances and Class Proportions within Encoded Age Intervals')\nplt.ylabel('Number of Observations')\nplt.xlabel('Encoded Age Interval')\nplt.xticks(rotation=45, ha=\"right\")\nplt.tight_layout()\nplt.show()\n</pre> # Merging the encoded ages, original ages, and target classes combined_age_class_df = pd.DataFrame({     'Original_age': X_train['age'].reset_index(drop=True),     'Encoded_age': X_transformed['LabelPage'].reset_index(drop=True),     'Class': y_train.reset_index(drop=True) })  # Grouping by encoded age and by class, then counting the occurrences grouped = combined_age_class_df.groupby(['Encoded_age', 'Class']).size().unstack()  # Sorting the bins sorted_bins = combined_age_df.sort_values(\"Original_age\").Encoded_age.drop_duplicates().tolist() grouped = grouped.reindex(sorted_bins)  # Displaying the stacked bar chart grouped.plot(kind='bar', stacked=True, figsize=(12, 7)) plt.title('Distribution of Instances and Class Proportions within Encoded Age Intervals') plt.ylabel('Number of Observations') plt.xlabel('Encoded Age Interval') plt.xticks(rotation=45, ha=\"right\") plt.tight_layout() plt.show()  <p>As shown in the figure above, the Khiops discretization doesn't try to account for the original age distribution, but focuses on the distribution of classes in each interval  (intervals are neither of equal width nor of equal number of examples). Note that the intervals are constructed for this purpose, as the proportion of classes in each interval is different from that of its neighbors. The probability of belonging to the richest group evolves smoothly with age. First, it increases, reaching a maximum around age 50, and then decreases.</p> <p>Khiops effectively captures the most relevant nuances in the data to estimate the class distribution, which conventional discretization methods cannot do.</p> In\u00a0[54]: Copied! <pre># Combine original 'age' values and their transformed values into a single dataframe\ncombined_workclass_df = pd.DataFrame({\n    'Original_workclass': X_reset['workclass'],\n    'Encoded_workclass': X_transformed_reset['LabelPworkclass']\n})\n\n# Display the first few rows to see the transformation\ncombined_workclass_df.drop_duplicates().head(10)\n</pre> # Combine original 'age' values and their transformed values into a single dataframe combined_workclass_df = pd.DataFrame({     'Original_workclass': X_reset['workclass'],     'Encoded_workclass': X_transformed_reset['LabelPworkclass'] })  # Display the first few rows to see the transformation combined_workclass_df.drop_duplicates().head(10)  Out[54]: Original_workclass Encoded_workclass 0 Private {Private, Without-pay, Never-worked} 5 Self-emp-inc {Self-emp-inc} 9 Self-emp-not-inc {Self-emp-not-inc, Local-gov, State-gov} 10 State-gov {Self-emp-not-inc, Local-gov, State-gov} 19 Local-gov {Self-emp-not-inc, Local-gov, State-gov} 36 Federal-gov {Federal-gov} 978 Without-pay {Private, Without-pay, Never-worked} 4494 Never-worked {Private, Without-pay, Never-worked} In\u00a0[55]: Copied! <pre># Merging the encoded workclass, original workclass, and target classes\ncombined_workclass_class_df = pd.DataFrame({\n    'Original_workclass': X_train['workclass'].reset_index(drop=True),\n    'Encoded_workclass': X_transformed['LabelPworkclass'].reset_index(drop=True),\n    'Class': y_train.reset_index(drop=True)\n})\n\n# Grouping by encoded workclass and by class, then counting the occurrences\ngrouped = combined_workclass_class_df.groupby(['Encoded_workclass', 'Class']).size().unstack()\n\n# Sorting the categories might be unnecessary as categorical values don't have a natural ordering like numerical ones. However, if you'd like to order them by frequency or another metric, you'd do it here.\n\n# Displaying the stacked bar chart\ngrouped.plot(kind='bar', stacked=True, figsize=(12, 7))\nplt.title('Distribution of Instances and Class Proportions within Encoded Workclass Categories')\nplt.ylabel('Number of Observations')\nplt.xlabel('Encoded Workclass Category')\nplt.xticks(rotation=45, ha=\"right\")\nplt.tight_layout()\nplt.show()\n</pre> # Merging the encoded workclass, original workclass, and target classes combined_workclass_class_df = pd.DataFrame({     'Original_workclass': X_train['workclass'].reset_index(drop=True),     'Encoded_workclass': X_transformed['LabelPworkclass'].reset_index(drop=True),     'Class': y_train.reset_index(drop=True) })  # Grouping by encoded workclass and by class, then counting the occurrences grouped = combined_workclass_class_df.groupby(['Encoded_workclass', 'Class']).size().unstack()  # Sorting the categories might be unnecessary as categorical values don't have a natural ordering like numerical ones. However, if you'd like to order them by frequency or another metric, you'd do it here.  # Displaying the stacked bar chart grouped.plot(kind='bar', stacked=True, figsize=(12, 7)) plt.title('Distribution of Instances and Class Proportions within Encoded Workclass Categories') plt.ylabel('Number of Observations') plt.xlabel('Encoded Workclass Category') plt.xticks(rotation=45, ha=\"right\") plt.tight_layout() plt.show()  <p>Just as with numerical variable encoding, Khiops groups categorical values to differentiate class probabilities within each group. This coding aims to estimate the conditional probabilities of the classes, as would a univariate classifier.</p> In\u00a0[56]: Copied! <pre>X_train_noisy = X_train.copy()\n\n# Percentage of missing values depending on the class\nmissing_rate_class_0 = 0.2  # 20% missing when class is 0\nmissing_rate_class_1 = 0.05  # 5% missing when class is 1\n\n# Identify indices where class is 0\nindices_class_0 = X_train_noisy.index[y.loc[X_train_noisy.index] == 0]\n\n# Identify indices where class is 1\nindices_class_1 = X_train_noisy.index[y.loc[X_train_noisy.index] == 1]\n\n# Number of missing values for each class\nN_0 = int(len(indices_class_0) * missing_rate_class_0)\nN_1 = int(len(indices_class_1) * missing_rate_class_1)\n\n# Iterate over feature columns\nfor col in X_train_noisy.columns:\n\n        # the column index (for the seed)\n        col_index = X_train_noisy.columns.get_loc(col)\n    \n        # Varying the seed by column index to distribute NaNs across different rows.\n        np.random.seed(42 + col_index) ; random_indices_0 = np.random.choice(indices_class_0, N_0, replace=False)\n        np.random.seed(42 + col_index) ; random_indices_1 = np.random.choice(indices_class_1, N_1, replace=False)\n        \n        # Introduce missing values\n        X_train_noisy.loc[random_indices_0, col] = np.nan\n        X_train_noisy.loc[random_indices_1, col] = np.nan\n</pre> X_train_noisy = X_train.copy()  # Percentage of missing values depending on the class missing_rate_class_0 = 0.2  # 20% missing when class is 0 missing_rate_class_1 = 0.05  # 5% missing when class is 1  # Identify indices where class is 0 indices_class_0 = X_train_noisy.index[y.loc[X_train_noisy.index] == 0]  # Identify indices where class is 1 indices_class_1 = X_train_noisy.index[y.loc[X_train_noisy.index] == 1]  # Number of missing values for each class N_0 = int(len(indices_class_0) * missing_rate_class_0) N_1 = int(len(indices_class_1) * missing_rate_class_1)  # Iterate over feature columns for col in X_train_noisy.columns:          # the column index (for the seed)         col_index = X_train_noisy.columns.get_loc(col)              # Varying the seed by column index to distribute NaNs across different rows.         np.random.seed(42 + col_index) ; random_indices_0 = np.random.choice(indices_class_0, N_0, replace=False)         np.random.seed(42 + col_index) ; random_indices_1 = np.random.choice(indices_class_1, N_1, replace=False)                  # Introduce missing values         X_train_noisy.loc[random_indices_0, col] = np.nan         X_train_noisy.loc[random_indices_1, col] = np.nan In\u00a0[57]: Copied! <pre># Introduce noise to 'age'\nnp.random.seed(42)\nnoise_age = np.random.normal(0, 3, len(X_train_noisy))\nX_train_noisy['age'] = X_train_noisy['age'] + np.round(noise_age).astype(int)\n\n# Introduce noise to 'hours_per_week'\nnp.random.seed(43)\nnoise_hours_per_week = np.random.normal(0, 3, len(X_train_noisy))\nX_train_noisy['hours_per_week'] = X_train_noisy['hours_per_week'] + np.round(noise_hours_per_week).astype(int)\n\n# Introduce outliers to 'age'\nnp.random.seed(42)\noutlier_indices_age = np.random.choice(X_train_noisy.index, int(0.1 * len(X_train_noisy)), replace=False)\nX_train_noisy.loc[outlier_indices_age, 'age'] *= np.random.randint(20, 40, size=len(outlier_indices_age))  \n\n# Introduce outliers to 'hours_per_week'\nnp.random.seed(43)\noutlier_indices_hours_per_week = np.random.choice(X_train_noisy.index, int(0.1 * len(X_train_noisy)), replace=False)\nX_train_noisy.loc[outlier_indices_hours_per_week, 'hours_per_week'] *= np.random.randint(20, 40, size=len(outlier_indices_hours_per_week))\n\n# Introduce outliers to 'capital_gain'\nnp.random.seed(44)\noutlier_indices_capital_gain = np.random.choice(X_train_noisy.index, int(0.1 * len(X_train_noisy)), replace=False)\nX_train_noisy.loc[outlier_indices_capital_gain, 'capital_gain'] *= np.random.randint(20, 40, size=len(outlier_indices_capital_gain)) \n\nX_train_noisy.head(10)\n</pre> # Introduce noise to 'age' np.random.seed(42) noise_age = np.random.normal(0, 3, len(X_train_noisy)) X_train_noisy['age'] = X_train_noisy['age'] + np.round(noise_age).astype(int)  # Introduce noise to 'hours_per_week' np.random.seed(43) noise_hours_per_week = np.random.normal(0, 3, len(X_train_noisy)) X_train_noisy['hours_per_week'] = X_train_noisy['hours_per_week'] + np.round(noise_hours_per_week).astype(int)  # Introduce outliers to 'age' np.random.seed(42) outlier_indices_age = np.random.choice(X_train_noisy.index, int(0.1 * len(X_train_noisy)), replace=False) X_train_noisy.loc[outlier_indices_age, 'age'] *= np.random.randint(20, 40, size=len(outlier_indices_age))    # Introduce outliers to 'hours_per_week' np.random.seed(43) outlier_indices_hours_per_week = np.random.choice(X_train_noisy.index, int(0.1 * len(X_train_noisy)), replace=False) X_train_noisy.loc[outlier_indices_hours_per_week, 'hours_per_week'] *= np.random.randint(20, 40, size=len(outlier_indices_hours_per_week))  # Introduce outliers to 'capital_gain' np.random.seed(44) outlier_indices_capital_gain = np.random.choice(X_train_noisy.index, int(0.1 * len(X_train_noisy)), replace=False) X_train_noisy.loc[outlier_indices_capital_gain, 'capital_gain'] *= np.random.randint(20, 40, size=len(outlier_indices_capital_gain))   X_train_noisy.head(10) Out[57]: age workclass fnlwgt education education_num marital_status occupation relationship race sex capital_gain capital_loss hours_per_week native_country Label 21729 24.0 NaN 38238.0 Bachelors 13.0 Never-married Other-service NaN White Male 0.0 0.0 41.0 United-States 11809 46.0 NaN 116952.0 7th-8th 4.0 Married-civ-spouse NaN Wife White Female 0.0 0.0 1302.0 United-States 23968 48.0 Private 191389.0 HS-grad 9.0 Married-civ-spouse Exec-managerial Wife White Female 0.0 0.0 27.0 United-States 13836 29.0 NaN 190290.0 NaN 11.0 Married-civ-spouse NaN NaN White NaN 0.0 0.0 43.0 United-States 40272 759.0 Private 200089.0 NaN NaN Married-civ-spouse Other-service Husband White Male 0.0 0.0 NaN Guatemala 3190 42.0 Self-emp-inc NaN Some-college 10.0 NaN Machine-op-inspct Husband White Male 0.0 0.0 49.0 United-States 6189 38.0 Private 195488.0 NaN 9.0 Married-civ-spouse Adm-clerical Wife White Female 0.0 0.0 53.0 United-States 38671 NaN NaN 166297.0 Bachelors NaN Never-married NaN Own-child Asian-Pac-Islander Male 0.0 0.0 NaN United-States 32547 NaN Private NaN Assoc-acdm 12.0 NaN NaN NaN White Female 0.0 0.0 44.0 NaN 35160 NaN Self-emp-not-inc NaN 7th-8th 4.0 Married-civ-spouse Machine-op-inspct Husband White NaN 0.0 NaN 39.0 NaN In\u00a0[58]: Copied! <pre># Initialize a KhiopsEncoder with specified transformation types for both categorical and numerical data.\n# Here, \"part_label\" means that the variables will be encoded using labels derived from the Khiops discretization.\nencoder_noisy = KhiopsEncoder(transform_type_categorical=\"part_label\", transform_type_numerical=\"part_label\")\n\n# Fit the encoder to the training data and simultaneously transform it.\n# The encoder learns the optimal encoding based on the provided features and target.\nX_encoded_noisy = encoder_noisy.fit_transform(X_train_noisy, y_train)\n</pre> # Initialize a KhiopsEncoder with specified transformation types for both categorical and numerical data. # Here, \"part_label\" means that the variables will be encoded using labels derived from the Khiops discretization. encoder_noisy = KhiopsEncoder(transform_type_categorical=\"part_label\", transform_type_numerical=\"part_label\")  # Fit the encoder to the training data and simultaneously transform it. # The encoder learns the optimal encoding based on the provided features and target. X_encoded_noisy = encoder_noisy.fit_transform(X_train_noisy, y_train) In\u00a0[59]: Copied! <pre># Convert the transformed (encoded) training data into a DataFrame.\n# The column names for this DataFrame are fetched from the encoder's 'feature_names_out_' attribute.\nX_transformed_noisy = pd.DataFrame(X_encoded_noisy, columns = encoder.feature_names_out_)\n\n# Display the first 5 rows of the transformed training data to get a quick overview of the encoding.\nX_transformed_noisy.head(5)\n</pre> # Convert the transformed (encoded) training data into a DataFrame. # The column names for this DataFrame are fetched from the encoder's 'feature_names_out_' attribute. X_transformed_noisy = pd.DataFrame(X_encoded_noisy, columns = encoder.feature_names_out_)  # Display the first 5 rows of the transformed training data to get a quick overview of the encoding. X_transformed_noisy.head(5) Out[59]: LabelPage LabelPworkclass LabelPfnlwgt LabelPeducation LabelPeducation_num LabelPmarital_status LabelPoccupation LabelPrelationship LabelPrace LabelPsex LabelPcapital_gain LabelPcapital_loss LabelPhours_per_week LabelPnative_country 0 ]22.5;24.5] {, Never-worked} ]-inf;68270.5] {Bachelors} ]12.5;13.5] {Never-married} {Other-service, Priv-house-serv} {, Unmarried} {White, Asian-Pac-Islander} {Male} ]-inf;57] ]-inf;100] ]34.5;42.5] {United-States, Philippines, Germany, ...} 1 ]42.5;56.5] {, Never-worked} ]68270.5;187920] {} ]-inf;8.5] {Married-civ-spouse, Married-AF-spouse} {, Handlers-cleaners} {Husband, Wife} {White, Asian-Pac-Islander} {Female} ]-inf;57] ]-inf;100] ]815;1552.5] {United-States, Philippines, Germany, ...} 2 ]42.5;56.5] {Private, Without-pay} ]187920;+inf[ {HS-grad} ]8.5;9.5] {Married-civ-spouse, Married-AF-spouse} {Exec-managerial} {Husband, Wife} {White, Asian-Pac-Islander} {Female} ]-inf;57] ]-inf;100] ]-inf;29.5] {United-States, Philippines, Germany, ...} 3 ]28.5;32.5] {, Never-worked} ]187920;+inf[ {} ]10.5;12.5] {Married-civ-spouse, Married-AF-spouse} {, Handlers-cleaners} {, Unmarried} {White, Asian-Pac-Islander} {} ]-inf;57] ]-inf;100] ]42.5;45.5] {United-States, Philippines, Germany, ...} 4 ]72.5;788] {Private, Without-pay} ]187920;+inf[ {} ]-inf;8.5] {Married-civ-spouse, Married-AF-spouse} {Other-service, Priv-house-serv} {Husband, Wife} {White, Asian-Pac-Islander} {Male} ]-inf;57] ]-inf;100] ]-inf;29.5] {, Mexico, Puerto-Rico, ...} In\u00a0[60]: Copied! <pre># Reset indices to ensure alignment\nX_reset_noisy = X_train_noisy.reset_index(drop=True)\nX_transformed_noisy = X_transformed_noisy.reset_index(drop=True)\n\n# Combine original 'age' values and their transformed values into a single dataframe\ncombined_df_noisy = pd.DataFrame({\n    'Original_age': X_reset_noisy['age'],\n    'Encoded_age': X_transformed_noisy['LabelPage'],\n    'Original_workclass': X_reset_noisy['workclass'],\n    'Encoded_workclass': X_transformed_noisy['LabelPworkclass']\n})\n\n# Display the first few rows to see the transformation\ncombined_df_noisy.head(10)\n</pre> # Reset indices to ensure alignment X_reset_noisy = X_train_noisy.reset_index(drop=True) X_transformed_noisy = X_transformed_noisy.reset_index(drop=True)  # Combine original 'age' values and their transformed values into a single dataframe combined_df_noisy = pd.DataFrame({     'Original_age': X_reset_noisy['age'],     'Encoded_age': X_transformed_noisy['LabelPage'],     'Original_workclass': X_reset_noisy['workclass'],     'Encoded_workclass': X_transformed_noisy['LabelPworkclass'] })  # Display the first few rows to see the transformation combined_df_noisy.head(10) Out[60]: Original_age Encoded_age Original_workclass Encoded_workclass 0 24.0 ]22.5;24.5] NaN {, Never-worked} 1 46.0 ]42.5;56.5] NaN {, Never-worked} 2 48.0 ]42.5;56.5] Private {Private, Without-pay} 3 29.0 ]28.5;32.5] NaN {, Never-worked} 4 759.0 ]72.5;788] Private {Private, Without-pay} 5 42.0 ]37.5;42.5] Self-emp-inc {Self-emp-inc} 6 38.0 ]37.5;42.5] Private {Private, Without-pay} 7 NaN Missing NaN {, Never-worked} 8 NaN Missing Private {Private, Without-pay} 9 NaN Missing Self-emp-not-inc {Self-emp-not-inc, Local-gov, State-gov} <p>Depending on the type of variable, the Khiops encoder has a different way of dealing with outliers and missing values.</p> <p>For numerical variables:</p> <ul> <li>Outliers are simply considered as the largest (or smallest) values, since Khiops is based on rank statistics.</li> <li>Missing values are internally coded as an extreme rank (i.e. equivalent to minus infinity) and labelled \"Missing\". If these absences of value are not random and bring useful information for the target prediction, a specific interval is naturally included in the model, comprising only the \"Missing\" value.</li> </ul> <p>For categorical variables :</p> <ul> <li>Missing values constitute a modality in their own right, which will be included in a group with modalities that have similar class correlations. For example, in the \"Work class\" variable, missing values can be grouped with \"Never worked\" because of a common association with the target class.</li> </ul> In\u00a0[61]: Copied! <pre># the pyCaret setup for the standard models:\nsetup(pd.concat([X_transformed_noisy.reset_index(drop=True), y_train.reset_index(drop=True)], axis=1), target = 'class', session_id=123, verbose=False,preprocess=False)\ncompare_models(include=[\"lightgbm\"])\n</pre> # the pyCaret setup for the standard models: setup(pd.concat([X_transformed_noisy.reset_index(drop=True), y_train.reset_index(drop=True)], axis=1), target = 'class', session_id=123, verbose=False,preprocess=False) compare_models(include=[\"lightgbm\"]) Model Accuracy AUC Recall Prec. F1 Kappa MCC TT (Sec) lightgbm Light Gradient Boosting Machine 0.9226 0.9702 0.8077 0.8608 0.8333 0.7830 0.7838 0.3300 Out[61]: <pre>LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n               importance_type='split', learning_rate=0.1, max_depth=-1,\n               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,\n               random_state=123, reg_alpha=0.0, reg_lambda=0.0, silent='warn',\n               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LGBMClassifier<pre>LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n               importance_type='split', learning_rate=0.1, max_depth=-1,\n               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,\n               random_state=123, reg_alpha=0.0, reg_lambda=0.0, silent='warn',\n               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)</pre> In\u00a0[62]: Copied! <pre>results_lgbm = pull()\nresults_lgbm['Model'].replace({\n    'Light Gradient Boosting Machine': 'LGBM with Khiops Optimal Encoding',\n    }, inplace=True)\n</pre> results_lgbm = pull() results_lgbm['Model'].replace({     'Light Gradient Boosting Machine': 'LGBM with Khiops Optimal Encoding',     }, inplace=True) In\u00a0[63]: Copied! <pre># the pyCaret setup for the standard models:\nsetup(pd.concat([X_train_noisy, y_train], axis=1), target = 'class', session_id=123, verbose=False)\ncompare_models(include=[\"lightgbm\"])\n</pre> # the pyCaret setup for the standard models: setup(pd.concat([X_train_noisy, y_train], axis=1), target = 'class', session_id=123, verbose=False) compare_models(include=[\"lightgbm\"]) Model Accuracy AUC Recall Prec. F1 Kappa MCC TT (Sec) lightgbm Light Gradient Boosting Machine 0.8950 0.9491 0.7267 0.8148 0.7681 0.7005 0.7025 0.1890 Out[63]: <pre>LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n               importance_type='split', learning_rate=0.1, max_depth=-1,\n               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,\n               random_state=123, reg_alpha=0.0, reg_lambda=0.0, silent='warn',\n               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LGBMClassifier<pre>LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n               importance_type='split', learning_rate=0.1, max_depth=-1,\n               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,\n               random_state=123, reg_alpha=0.0, reg_lambda=0.0, silent='warn',\n               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)</pre> In\u00a0[64]: Copied! <pre>results_lgbm = pd.concat([results_lgbm, pull()], ignore_index=True)\nresults_lgbm['Model'].replace({\n    'Light Gradient Boosting Machine': 'LGBM with standard Encoding',\n    }, inplace=True)\nresults_lgbm.sort_values(by=\"Accuracy\",ascending=False)\n</pre> results_lgbm = pd.concat([results_lgbm, pull()], ignore_index=True) results_lgbm['Model'].replace({     'Light Gradient Boosting Machine': 'LGBM with standard Encoding',     }, inplace=True) results_lgbm.sort_values(by=\"Accuracy\",ascending=False) Out[64]: Model Accuracy AUC Recall Prec. F1 Kappa MCC TT (Sec) 0 LGBM with Khiops Optimal Encoding 0.9226 0.9702 0.8077 0.8608 0.8333 0.7830 0.7838 0.330 1 LGBM with standard Encoding 0.8950 0.9491 0.7267 0.8148 0.7681 0.7005 0.7025 0.189 In\u00a0[65]: Copied! <pre>df_plot = results_lgbm.drop(\"TT (Sec)\",axis=1).melt(id_vars=['Model'], var_name='Metric', value_name='Value')\n\nplt.figure(figsize=(14, 6))\n\n# Create a bar plot with Seaborn\nsns.barplot(x='Metric', y='Value', hue='Model', data=df_plot, palette=\"Set3\")\n\nplt.title(\"Comparison of LGBM performances: Khiops Encoding vs Standard Encoding\")\nplt.ylabel('Value')\nplt.xlabel('Metric')\n\nplt.legend(title='Model', bbox_to_anchor=(1.05, 1), loc='upper left')\n\nplt.tight_layout()\nplt.show()\n</pre> df_plot = results_lgbm.drop(\"TT (Sec)\",axis=1).melt(id_vars=['Model'], var_name='Metric', value_name='Value')  plt.figure(figsize=(14, 6))  # Create a bar plot with Seaborn sns.barplot(x='Metric', y='Value', hue='Model', data=df_plot, palette=\"Set3\")  plt.title(\"Comparison of LGBM performances: Khiops Encoding vs Standard Encoding\") plt.ylabel('Value') plt.xlabel('Metric')  plt.legend(title='Model', bbox_to_anchor=(1.05, 1), loc='upper left')  plt.tight_layout() plt.show() <p>In this section, the Khiops encoder has demonstrated its capability to significantly strengthen the performance of LGBM, especially in scenarios dealing with noisy datasets. We strongly encourage leveraging the Khiops encoder in your pipelines to benefit from its advanced encoding capabilities.</p> <p>However, the true potential of this encoder is fully unlocked when coupled with the Selective Naive Bayes classifier of Khiops. Together, they ensure top-tier performance and deliver an end-to-end solution that is naturally robust, interpretable, and scalable.</p> <p>Let's delve deeper into this in the following section, where we'll explore the combined strengths of the Khiops encoder and classifier.</p> In\u00a0[66]: Copied! <pre># we use a KhiopsClassifier to leverage the full pipeline\npkc = KhiopsClassifier(n_trees=0)\n</pre> # we use a KhiopsClassifier to leverage the full pipeline pkc = KhiopsClassifier(n_trees=0) <p>We use a KhiopsClassifier to leverage the full pipeline, and we set the number of trees to zero (<code>n_trees=0</code>). By default, Khiops builds 10 decision trees to enrich training data with categorical variables (each corresponding to the leef of the trees). This improves the performance of the models, but this is not necessary for this simple tutorial.</p> In\u00a0[67]: Copied! <pre># fit \npkc.fit(X_train, y_train)\n</pre> # fit  pkc.fit(X_train, y_train) Out[67]: <pre>KhiopsClassifier(auto_sort=True, internal_sort=None, key=None, n_features=100,\n                 n_pairs=0, n_trees=0, output_dir=None, verbose=False)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KhiopsClassifier<pre>KhiopsClassifier(auto_sort=True, internal_sort=None, key=None, n_features=100,\n                 n_pairs=0, n_trees=0, output_dir=None, verbose=False)</pre> <p>A simple fit function chains optimal coding and parsimonious learning. And it's extremely handy because there's no need to prepare training data, as missing values, noise, outliers and unbalanced classes are all handled for you. Categorical variable encoding is no problem either.</p> <p>Now let's take a closer look at variable selection, which takes place in two successive steps. First to remove variables that are not correlated with the target, then to eliminate variables that are correlated between them:</p> <ul> <li>During the Optimal Encoding step, the level measures the extent to which each variable is correlated with the target (more details here), variables with negative compression gain are removed.</li> <li>Then during the Paricimonious Training step, the weight measures the importance of each variable within the learned classifier which is able to eliminate redundant variables (more details here). For instance, if two input variables are correlated, one of them can be discarded by being assigned a zero weight. In some cases, both variables are retained by the algorithm, and the weight of both variables is reduced because of this redundancy.</li> </ul> <p>The next cell shows the names of the selected variables, sorted by decreasing weight, and the level is also displayed:</p> In\u00a0[68]: Copied! <pre>kpis = {\"Feature\" : [], \"Level\": [], \"Weight\": []}\n\nfor var in pkc.model_report_.modeling_report.get_snb_predictor().selected_variables:\n    kpis[\"Feature\"].append(var.name)\n    kpis[\"Level\"].append(var.level)\n    kpis[\"Weight\"].append(var.weight)\n\n    \ndf_kpis = pd.DataFrame(kpis).sort_values(by = 'Weight', ascending=False)\ndf_kpis.style.set_properties(subset=['Feature'], **{'width': '400px'})\n</pre> kpis = {\"Feature\" : [], \"Level\": [], \"Weight\": []}  for var in pkc.model_report_.modeling_report.get_snb_predictor().selected_variables:     kpis[\"Feature\"].append(var.name)     kpis[\"Level\"].append(var.level)     kpis[\"Weight\"].append(var.weight)       df_kpis = pd.DataFrame(kpis).sort_values(by = 'Weight', ascending=False) df_kpis.style.set_properties(subset=['Feature'], **{'width': '400px'}) Out[68]: Feature Level Weight 0 capital_gain 0.139777 0.890656 5 capital_loss 0.055614 0.804718 3 age 0.117285 0.572296 1 marital_status 0.199115 0.535675 4 education 0.114525 0.500031 6 occupation 0.089358 0.471710 7 hours_per_week 0.069999 0.449249 9 native_country 0.006893 0.421906 2 relationship 0.209471 0.371124 8 education_num 0.114941 0.249054 11 race 0.009712 0.210968 10 workclass 0.022131 0.097687 12 sex 0.045482 0.005890 <p>Note that the variable <code>fnlwgt</code> was removed in the first step, as it is not at all correlated with the target. Next, we can see that the variables <code>marital_status</code> and <code>relationship</code> are probably highly correlated with each other. Endeed, these two variables have one of the highest <code>Levels</code>, so they are highly relevant for predicting the target. But their <code>Weight</code>, which isn't all that important, suggests that these two variables are correlated and therefore bring redundant information.</p> <p>In addition, the <code>capital_loss</code> variable has a relatively low <code>Level</code> and a high <code>Weight</code>. This means that this variable is poorly correlated with the target, but provides complementary information compared with the other variables, which is very useful to the classifier in predicting the target.</p> <p>For further details on model interpretation, please refer to the visualization tool page.</p>"},{"location":"tutorials/Notebooks/Optimal_Encoding/#interpretable-variable-encoding","title":"Interpretable Variable Encoding\u00b6","text":""},{"location":"tutorials/Notebooks/Optimal_Encoding/#introduction","title":"Introduction\u00b6","text":"<p>Variable encoding is essential for feeding learning algorithms from raw data, particularly for categorical data which often has to be numerically encoded. Ensuring this coding is useful for training the model, safe from over-fitting and interpretable, improves both the quality of the models and their relevance in an industrial context. Exploring Khiops' aptitude with raw data, we now delve deeper into its unique approach to variable encoding, ensuring that the transformed data retains its interpretability\u2014a trait often sacrificed in the race for higher performance.</p> <p>Unlike many traditional encoding techniques that demand tricky setup and hyperparameter fine-tuning, Khiops delivers optimal encoding effortlessly thanks to its supervised approach MODL. The encoding directly aligns with the predictive objective, enhancing accuracy and relevance. Moreover, Khiops' encoding systematically refines as your data grows, reaching more granularity and precision.</p> <p>In this notebook, we'll explore the Khiops' encoding capabilities, drawing contrasts with conventional methods and highlighting the interpretability and target-driven optimization. We demonstrate that its optimal encoding seamlessly slots into any machine learning pipeline (here, on a pycaret pipeline), preventing the need for making predetermined choices about encoding strategies and extensive hyperparameter optimization processes.</p>"},{"location":"tutorials/Notebooks/Optimal_Encoding/#installation-and-set-up","title":"Installation and set up\u00b6","text":"<p>If you do not use our official <code>khiops-notebook</code> Jupyter Docker image, you may have to install khiops locally using <code>conda</code>:</p>"},{"location":"tutorials/Notebooks/Optimal_Encoding/#import-and-preparation-of-data","title":"Import and Preparation of Data\u00b6","text":"<p>For this notebook, we use the \"Adult\" UCI standard Dataset. More details available here.</p> <p>This dataset is also available on our khiops-sample repository on Github.</p>"},{"location":"tutorials/Notebooks/Optimal_Encoding/#overview-of-khiops-encoding","title":"Overview of Khiops Encoding\u00b6","text":"<p>In this section, we'll explore how Khiops encodes the Adult dataset.</p> <p>In our previous notebook, we highlighted Khiops' strengths: its robustness, ability to handle raw data, and the absence of any need for parameter tuning. Due to these characteristics, cross-validation is unlikely to change the model or the variable encoding. Consequently, for this experiment, we'll adopt a straightforward train-test split.</p>"},{"location":"tutorials/Notebooks/Optimal_Encoding/#encoding-of-numerical-variables","title":"Encoding of numerical variables\u00b6","text":"<p>The discretization in Khiops is rooted in the MODL approach, which partitions numerical variables into meaningful intervals. At its core, MODL seeks a balance between two competing objectives:</p> <ul> <li>Prior encourages the selection of a simple discretization model, with few intervals (the most probable model has a single interval),</li> <li>Likelihood advocates complex discretization models, i.e. composed of many intervals (the most likely model has as many intervals as there are changes in the sequence of labels).</li> </ul> <p>Thus, optimizing for prior only leads to an under-fit. On the other hand, optimizing for likelihood leads to overfitting. Both terms are derived from Bayes' formula and are mathematically consistent. So, Khiops optimizes between these two extremes, achieving a balance between underfitting and overfitting, ensuring that discretization models are relevant, robust (not too many intervals) and interpretable, capturing the underlying patterns in the data related to the target variable.</p> <p>With this understanding, let's see how the variable <code>age</code> is transformed by this method.</p>"},{"location":"tutorials/Notebooks/Optimal_Encoding/#encoding-of-categorical-variables","title":"Encoding of categorical variables\u00b6","text":"<p>As with numerical variables, Khiops applies the MODL approach to transform categorical variables into modality groups, this encoding being informative for predicting the target variable. During model selection, a balance is achieved between :</p> <ul> <li>The Prior term, which encourages the selection of a simple model, with few groups of modalities (the most probable being composed of a single group);</li> <li>The Likelihood term, which favors complex models that accurately describe the training data (possibly one group per modality).</li> </ul> <p>As before, optimizing the prior alone leads to over-fitting. By contrast, optimizing likelihood alone leads to over-fitting. It is the balance given by these two terms, both derived from Bayes' formula and mathematically consistent, that enables us to achieve a balance between under-fitting and over-fitting. Ultimately, the selected models accurately and robustly describe the patterns underlying the training data, with respect to target variable.</p> <p>MODL's grouping approach having been explained in detail on the dedicated page, we now turn to the encoding of the categorical variable <code>workclass</code>.</p>"},{"location":"tutorials/Notebooks/Optimal_Encoding/#how-khiops-encoder-handles-missing-values-and-outliers","title":"How Khiops Encoder handles missing values and outliers\u00b6","text":"<p>This section returns in more detail to the first notebook, which shows that training data should not be prepared with Khiops. We'll inject missing values, noise, and outliers into our dataset to understand how the Khiops Encoder manages these common challenges. The objective is to showcase the robustness and adaptability of the Khiops Encoder in the face of imperfect data. In the later section, we'll leverage this robust encoding to train an LGBM classifier, demonstrating the potential benefits of utilizing the Khiops encoding in a broader context.</p>"},{"location":"tutorials/Notebooks/Optimal_Encoding/#using-khiops-encoder-to-improve-lgbm-performances","title":"Using Khiops Encoder to improve LGBM performances\u00b6","text":""},{"location":"tutorials/Notebooks/Optimal_Encoding/#interpretability-of-supervised-learning-with-end-to-end-khiops-pipeline","title":"Interpretability of supervised learning with end-to-end Khiops pipeline\u00b6","text":"<p>In this section, we use the complete pipeline provided by Khiops, which chains Optimal Encoding and Parsimonious Training through a simplistic syntax (a simple fit function).</p>"},{"location":"tutorials/Notebooks/Use_in_any_ML_pipeline/","title":"Auto Feature Engineering & More","text":"In\u00a0[3]: Copied! <pre>#!conda install -y -c conda-forge -c khiops khiops\n</pre> #!conda install -y -c conda-forge -c khiops khiops <p>For the experiments, you also need some external libraries you can install via <code>pip</code>:</p> In\u00a0[4]: Copied! <pre># Installation of external libraries\n#!pip install matplotlib seaborn\n\n# Note: Installing PyCaret can sometimes be complex due to its dependencies. \n# If you encounter any issues, please refer to the PyCaret documentation for detailed installation instructions:\n# https://pycaret.gitbook.io/docs/get-started/installation\n#!pip install pycaret\n</pre> # Installation of external libraries #!pip install matplotlib seaborn  # Note: Installing PyCaret can sometimes be complex due to its dependencies.  # If you encounter any issues, please refer to the PyCaret documentation for detailed installation instructions: # https://pycaret.gitbook.io/docs/get-started/installation #!pip install pycaret <p>We now import all the dependencies here:</p> In\u00a0[5]: Copied! <pre>import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\nfrom khiops.sklearn import KhiopsClassifier\nfrom khiops.sklearn import KhiopsEncoder\n\nfrom pycaret.classification import *\n</pre> import pandas as pd import matplotlib.pyplot as plt import numpy as np import seaborn as sns  from khiops.sklearn import KhiopsClassifier from khiops.sklearn import KhiopsEncoder  from pycaret.classification import * In\u00a0[6]: Copied! <pre># Method 1: Load data directly from GitHub (recommended for quick tests or small datasets)\nurl_accidents = \"https://raw.githubusercontent.com/KhiopsML/khiops-samples/10.2.4/Accidents/Accidents.txt\"\naccidents_df = pd.read_csv(url_accidents, delimiter='\\t')\n\n# Method 2: Load data locally after downloading all Khiops samples (best for offline use or large datasets)\n#If the samples have not been downloaded yet:\n#from khiops.tools import download_datasets\n#download_datasets() \n#\n#from os import path\n#from khiops import core as kh\n#accidents_dataset_path = path.join(kh.get_samples_dir(), \"AccidentsSummary\")\n#accidents_df = pd.read_csv(path.join(accidents_dataset_path, \"Accidents.txt\"),sep=\"\\t\")\n\n# Display the first 10 records from the dataset\naccidents_df.head(10)\n</pre> # Method 1: Load data directly from GitHub (recommended for quick tests or small datasets) url_accidents = \"https://raw.githubusercontent.com/KhiopsML/khiops-samples/10.2.4/Accidents/Accidents.txt\" accidents_df = pd.read_csv(url_accidents, delimiter='\\t')  # Method 2: Load data locally after downloading all Khiops samples (best for offline use or large datasets) #If the samples have not been downloaded yet: #from khiops.tools import download_datasets #download_datasets()  # #from os import path #from khiops import core as kh #accidents_dataset_path = path.join(kh.get_samples_dir(), \"AccidentsSummary\") #accidents_df = pd.read_csv(path.join(accidents_dataset_path, \"Accidents.txt\"),sep=\"\\t\")  # Display the first 10 records from the dataset accidents_df.head(10)  Out[6]: AccidentId Gravity Date Hour Light Department Commune InAgglomeration IntersectionType Weather CollisionType PostalAddress 0 201800000001 NonLethal 2018-01-24 15:05:00 Daylight 590 5 No Y-type Normal 2Vehicles-BehindVehicles-Frontal route des Ansereuilles 1 201800000002 NonLethal 2018-02-12 10:15:00 Daylight 590 11 Yes Square VeryGood NoCollision Place du g\u00e9n\u00e9ral de Gaul 2 201800000003 NonLethal 2018-03-04 11:35:00 Daylight 590 477 Yes T-type Normal NoCollision Rue  nationale 3 201800000004 NonLethal 2018-05-05 17:35:00 Daylight 590 52 Yes NoIntersection VeryGood 2Vehicles-Side 30 rue Jules Guesde 4 201800000005 NonLethal 2018-06-26 16:05:00 Daylight 590 477 Yes NoIntersection Normal 2Vehicles-Side 72 rue Victor Hugo In\u00a0[7]: Copied! <pre># Method 1: Load data directly from GitHub (recommended for quick tests or small datasets)\nurl_vehicle = \"https://raw.githubusercontent.com/KhiopsML/khiops-samples/main/Accidents/Vehicles.txt\"\nvehicles_df = pd.read_csv(url_vehicle, delimiter='\\t')\n\n# Method 2: Load data locally after downloading all Khiops samples (best for offline use or large datasets)\n#vehicles_df = pd.read_csv(path.join(accidents_dataset_path, \"Vehicles.txt\"), sep=\"\\t\")\n\n# Display the first 10 records from the dataset\nvehicles_df.head(10)\n</pre> # Method 1: Load data directly from GitHub (recommended for quick tests or small datasets) url_vehicle = \"https://raw.githubusercontent.com/KhiopsML/khiops-samples/main/Accidents/Vehicles.txt\" vehicles_df = pd.read_csv(url_vehicle, delimiter='\\t')  # Method 2: Load data locally after downloading all Khiops samples (best for offline use or large datasets) #vehicles_df = pd.read_csv(path.join(accidents_dataset_path, \"Vehicles.txt\"), sep=\"\\t\")  # Display the first 10 records from the dataset vehicles_df.head(10)  Out[7]: AccidentId VehicleId Direction Category PassengerNumber FixedObstacle MobileObstacle ImpactPoint Maneuver 0 201800000001 A01 Unknown Car&lt;=3.5T 0 NaN Vehicle RightFront TurnToLeft 1 201800000001 B01 Unknown Car&lt;=3.5T 0 NaN Vehicle LeftFront NoDirectionChange 2 201800000002 A01 Unknown Car&lt;=3.5T 0 NaN Pedestrian NaN NoDirectionChange 3 201800000003 A01 Unknown Motorbike&gt;125cm3 0 StationaryVehicle Vehicle Front NoDirectionChange 4 201800000003 B01 Unknown Car&lt;=3.5T 0 NaN Vehicle LeftSide TurnToLeft In\u00a0[8]: Copied! <pre># Method 1: Load data directly from GitHub (recommended for quick tests or small datasets)\nurl_user = \"https://raw.githubusercontent.com/KhiopsML/khiops-samples/main/Accidents/Users.txt\"\nusers_df = pd.read_csv(url_user, delimiter='\\t')\n\n# Method 2: Load data locally after downloading all Khiops samples (best for offline use or large datasets)\n#users_df = pd.read_csv(path.join(accidents_dataset_path, \"Users.txt\"), sep=\"\\t\")\n\n# Display the first 10 records from the dataset\nusers_df.head(10)\n</pre> # Method 1: Load data directly from GitHub (recommended for quick tests or small datasets) url_user = \"https://raw.githubusercontent.com/KhiopsML/khiops-samples/main/Accidents/Users.txt\" users_df = pd.read_csv(url_user, delimiter='\\t')  # Method 2: Load data locally after downloading all Khiops samples (best for offline use or large datasets) #users_df = pd.read_csv(path.join(accidents_dataset_path, \"Users.txt\"), sep=\"\\t\")  # Display the first 10 records from the dataset users_df.head(10) Out[8]: AccidentId VehicleId Seat Category Gravity Gender TripReason SafetyDevice SafetyDeviceUsed PedestrianLocation PedestrianAction PedestrianCompany BirthYear 0 201800000001 A01 1.0 Driver Unscathed Male Leisure SeatBelt Yes NaN NaN Unknown 1960.0 1 201800000001 B01 1.0 Driver InjuredAndHospitalized Male NaN SeatBelt Yes NaN NaN Unknown 1928.0 2 201800000002 A01 1.0 Driver Unscathed Male NaN SeatBelt Yes NaN NaN Unknown 1947.0 3 201800000002 A01 NaN Pedestrian MildlyInjured Male NaN Helmet NaN OnLane&lt;=OnSidewalk0mCrossing Crossing Alone 1959.0 4 201800000003 A01 1.0 Driver InjuredAndHospitalized Male Leisure Helmet Yes NaN NaN Unknown 1987.0 5 201800000003 C01 1.0 Driver Unscathed Male NaN ChildrenDevice NaN NaN NaN Unknown 1977.0 6 201800000004 A01 1.0 Driver Unscathed Male Leisure SeatBelt Yes NaN NaN Unknown 1982.0 7 201800000004 B01 1.0 Driver InjuredAndHospitalized Male Leisure Helmet NaN NaN NaN Unknown 2013.0 8 201800000005 A01 1.0 Driver MildlyInjured Male Leisure Helmet Yes NaN NaN Unknown 2001.0 9 201800000005 B01 1.0 Driver Unscathed Male Leisure SeatBelt Yes NaN NaN Unknown 1946.0 <p>We need a final step to remove the target from the main table :</p> In\u00a0[9]: Copied! <pre>df_accident_train = df_accident.drop(\"Gravity\", axis=1)\ny_accident_train = df_accident[\"Gravity\"].map({'NonLethal': 0, 'Lethal': 1})\n</pre> df_accident_train = df_accident.drop(\"Gravity\", axis=1) y_accident_train = df_accident[\"Gravity\"].map({'NonLethal': 0, 'Lethal': 1}) In\u00a0[10]: Copied! <pre>X_accidents_train = {\n    \"main_table\": \"Accidents\",\n    \"tables\": {\n        \"Accidents\": (df_accident_train, \"AccidentId\"),\n        \"Vehicles\": (df_vehicle, [\"AccidentId\", \"VehicleId\"]),\n    },\n    \"relations\": [\n        (\"Accidents\", \"Vehicles\"),\n    ],\n}\n</pre> X_accidents_train = {     \"main_table\": \"Accidents\",     \"tables\": {         \"Accidents\": (df_accident_train, \"AccidentId\"),         \"Vehicles\": (df_vehicle, [\"AccidentId\", \"VehicleId\"]),     },     \"relations\": [         (\"Accidents\", \"Vehicles\"),     ], } <p>This dictionary includes three attributes:</p> <ul> <li><code>main_table</code> indicating the name of the main table,</li> <li><code>tables</code> describing all tables,</li> <li><code>relations</code> describing the links between tables.</li> </ul> <p><code>main table</code> is itself a dictionary, composed of one record per table. For each record, the key corresponds to the table name and the value is a tuple associating a Pandas Dataframe and a list of keys (first the main key, then the secondary keys). And <code>relations</code> is a tuple list indicating the links between tables.</p> In\u00a0[11]: Copied! <pre>pke = KhiopsEncoder(transform_type_categorical='part_label', transform_type_numerical='part_label',n_trees=0, n_features=1000)\npke.fit(X_accidents_train, y_accident_train)\n</pre> pke = KhiopsEncoder(transform_type_categorical='part_label', transform_type_numerical='part_label',n_trees=0, n_features=1000) pke.fit(X_accidents_train, y_accident_train) Out[11]: <pre>KhiopsEncoder(n_features=1000, transform_type_categorical='part_label',\n              transform_type_numerical='part_label')</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KhiopsEncoder<pre>KhiopsEncoder(n_features=1000, transform_type_categorical='part_label',\n              transform_type_numerical='part_label')</pre> <p>Training is over. Let's deploy the obtained encoder.</p> In\u00a0[12]: Copied! <pre>X_transformed = pke.transform(X_accidents_train)\nX_transformed = pd.DataFrame(X_transformed, columns = pke.feature_names_out_)\nprint('\\n Encoded features of the first 5 rows: \\n')\nX_transformed[:5]\n</pre> X_transformed = pke.transform(X_accidents_train) X_transformed = pd.DataFrame(X_transformed, columns = pke.feature_names_out_) print('\\n Encoded features of the first 5 rows: \\n') X_transformed[:5] <pre>\n Encoded features of the first 5 rows: \n\n</pre> Out[12]: LabelPLight LabelPDepartment LabelPCommune LabelPInAgglomeration LabelPIntersectionType LabelPWeather LabelPCollisionType LabelPPostalAddress LabelPCount(Vehicles) LabelPCountDistinct(Vehicles.Category) ... LabelPSum(Vehicles.PassengerNumber) where Direction not in {Increasing, Decreasing} and PassengerNumber &lt;= 0.5 LabelPSum(Vehicles.PassengerNumber) where ImpactPoint = Front and MobileObstacle = Vehicle LabelPSum(Vehicles.PassengerNumber) where ImpactPoint &lt;&gt; Front and MobileObstacle = Vehicle LabelPSum(Vehicles.PassengerNumber) where ImpactPoint = Front and MobileObstacle &lt;&gt; Vehicle LabelPSum(Vehicles.PassengerNumber) where ImpactPoint &lt;&gt; Front and MobileObstacle &lt;&gt; Vehicle LabelPSum(Vehicles.PassengerNumber) where Maneuver &lt;&gt; NoDirectionChange and MobileObstacle = Vehicle LabelPSum(Vehicles.PassengerNumber) where Maneuver = NoDirectionChange and MobileObstacle &lt;&gt; Vehicle LabelPSum(Vehicles.PassengerNumber) where Maneuver &lt;&gt; NoDirectionChange and MobileObstacle &lt;&gt; Vehicle LabelPSum(Vehicles.PassengerNumber) where Maneuver = NoDirectionChange and PassengerNumber &lt;= 0.5 LabelPSum(Vehicles.PassengerNumber) where Maneuver &lt;&gt; NoDirectionChange and PassengerNumber &lt;= 0.5 0 {Daylight} ]545;635] ]-inf;54.5] {No} {X-type} {Normal, Overcast, HeavyRain} {2Vehicles-BehindVehicles-Frontal, } {A4, A13, AUTOROUTE A1, ...} ]1.5;+inf[ ]-inf;1.5] ... ]-inf;+inf[ Missing ]-inf;+inf[ Missing Missing ]-inf;+inf[ Missing Missing ]-inf;+inf[ ]-inf;+inf[ 1 {Daylight} ]545;635] ]-inf;54.5] {Yes} {X-type} {VeryGood, FogOrSmoke} {Other, NoCollision, 3+Vehicles-Multiple} {A4, A13, AUTOROUTE A1, ...} ]-inf;1.5] ]-inf;1.5] ... ]-inf;+inf[ Missing Missing Missing ]-inf;+inf[ Missing ]-inf;+inf[ Missing ]-inf;+inf[ Missing 2 {Daylight} ]545;635] ]454.5;577.5] {Yes} {X-type} {Normal, Overcast, HeavyRain} {Other, NoCollision, 3+Vehicles-Multiple} {A4, A13, AUTOROUTE A1, ...} ]1.5;+inf[ ]1.5;+inf[ ... ]-inf;+inf[ ]-inf;+inf[ ]-inf;+inf[ Missing ]-inf;+inf[ ]-inf;+inf[ Missing ]-inf;+inf[ ]-inf;+inf[ ]-inf;+inf[ 3 {Daylight} ]545;635] ]-inf;54.5] {Yes} {NoIntersection} {VeryGood, FogOrSmoke} {2Vehicles-Side, 2Vehicles-Behind, 3+Vehicles-... {A4, A13, AUTOROUTE A1, ...} ]1.5;+inf[ ]1.5;+inf[ ... ]-inf;+inf[ Missing ]-inf;+inf[ Missing ]-inf;+inf[ ]-inf;+inf[ Missing ]-inf;+inf[ Missing ]-inf;+inf[ 4 {Daylight} ]545;635] ]454.5;577.5] {Yes} {NoIntersection} {Normal, Overcast, HeavyRain} {2Vehicles-Side, 2Vehicles-Behind, 3+Vehicles-... {A4, A13, AUTOROUTE A1, ...} ]1.5;+inf[ ]1.5;+inf[ ... ]-inf;+inf[ Missing ]-inf;+inf[ Missing Missing ]-inf;+inf[ Missing Missing Missing ]-inf;+inf[ <p>5 rows \u00d7 619 columns</p> <p>Check the new features of the encoded table. Let's notice the following:</p> <ul> <li>8 of the original 11 features were selected by Khiops. The remaining features have been detected as uninformative because they have a negative compression_gain (named <code>Level</code> in Khiops output) and are therefore not correlated with the target.</li> <li>611 new aggregates were automatically created by Khiops which saves up a large amount of time to a data scientist who usually defines and evaluate aggregates manually.</li> <li>The aggregates created by Khiops are labelled with their mathematical formula, which makes them easy to interpret. For example, the aggregate \"Count(Users) where PedestrianCompany &lt;&gt; Unknown\" is simply the number of users for whom <code>PedestrianCompany</code> is known.</li> </ul> In\u00a0[13]: Copied! <pre>kpis = {\"Feature\" : [], \"Level\": []}\nvariables = pke.model_report_.preparation_report.get_variable_names()\nfor var in variables:\n    kpis[\"Feature\"].append(var)\n    level = pke.model_report_.preparation_report.get_variable_statistics(var).level\n    kpis[\"Level\"].append(level)\n    \ndf_kpis = pd.DataFrame(kpis).sort_values(by = 'Level', ascending=False)\ndf_kpis.head(10).style.set_properties(subset=['Feature'], **{'width': '400px'})\n</pre> kpis = {\"Feature\" : [], \"Level\": []} variables = pke.model_report_.preparation_report.get_variable_names() for var in variables:     kpis[\"Feature\"].append(var)     level = pke.model_report_.preparation_report.get_variable_statistics(var).level     kpis[\"Level\"].append(level)      df_kpis = pd.DataFrame(kpis).sort_values(by = 'Level', ascending=False) df_kpis.head(10).style.set_properties(subset=['Feature'], **{'width': '400px'}) Out[13]: Feature Level 0 InAgglomeration 0.062196 1 Department 0.050247 2 CollisionType 0.037656 3 CountDistinct(Vehicles.Direction) where FixedObstacle is empty 0.031881 4 Mode(Vehicles.FixedObstacle) where FixedObstacle not empty 0.031605 5 Mode(Vehicles.Maneuver) where Maneuver &lt;&gt; NoDirectionChange 0.030864 6 Mode(Vehicles.FixedObstacle) 0.029517 7 Mode(Vehicles.FixedObstacle) where PassengerNumber &lt;= 0.5 0.029394 8 Mode(Vehicles.FixedObstacle) where MobileObstacle &lt;&gt; Vehicle 0.028928 9 Mode(Vehicles.FixedObstacle) where MobileObstacle is empty 0.028779 In\u00a0[14]: Copied! <pre># the pyCaret setup for the standard models:\nsetup(pd.concat([df_accident_train, y_accident_train], axis=1), target = 'Gravity', session_id=123, verbose=False)\ncompare_models(include=[\"lightgbm\"])\n</pre> # the pyCaret setup for the standard models: setup(pd.concat([df_accident_train, y_accident_train], axis=1), target = 'Gravity', session_id=123, verbose=False) compare_models(include=[\"lightgbm\"]) Model Accuracy AUC Recall Prec. F1 Kappa MCC TT (Sec) lightgbm Light Gradient Boosting Machine 0.9354 0.6418 0.0251 0.1158 0.0411 0.0216 0.0289 0.4160 Out[14]: <pre>LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n               importance_type='split', learning_rate=0.1, max_depth=-1,\n               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,\n               random_state=123, reg_alpha=0.0, reg_lambda=0.0, silent='warn',\n               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LGBMClassifier<pre>LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n               importance_type='split', learning_rate=0.1, max_depth=-1,\n               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,\n               random_state=123, reg_alpha=0.0, reg_lambda=0.0, silent='warn',\n               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)</pre> In\u00a0[15]: Copied! <pre>results_lgbm = pull()\nresults_lgbm['Model'].replace({\n    'Light Gradient Boosting Machine': 'LGBM with root table only',\n    }, inplace=True)\n</pre> results_lgbm = pull() results_lgbm['Model'].replace({     'Light Gradient Boosting Machine': 'LGBM with root table only',     }, inplace=True) In\u00a0[16]: Copied! <pre># the pyCaret setup for the standard models:\nsetup(pd.concat([X_transformed.reset_index(drop=True), y_accident_train.reset_index(drop=True)], axis=1), target = 'Gravity', session_id=123, verbose=False,preprocess=False)\ncompare_models(include=[\"lightgbm\"])\n</pre> # the pyCaret setup for the standard models: setup(pd.concat([X_transformed.reset_index(drop=True), y_accident_train.reset_index(drop=True)], axis=1), target = 'Gravity', session_id=123, verbose=False,preprocess=False) compare_models(include=[\"lightgbm\"]) Model Accuracy AUC Recall Prec. F1 Kappa MCC TT (Sec) lightgbm Light Gradient Boosting Machine 0.9451 0.8295 0.0489 0.5249 0.0891 0.0805 0.1470 0.7760 Out[16]: <pre>LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n               importance_type='split', learning_rate=0.1, max_depth=-1,\n               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,\n               random_state=123, reg_alpha=0.0, reg_lambda=0.0, silent='warn',\n               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LGBMClassifier<pre>LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n               importance_type='split', learning_rate=0.1, max_depth=-1,\n               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,\n               random_state=123, reg_alpha=0.0, reg_lambda=0.0, silent='warn',\n               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)</pre> In\u00a0[17]: Copied! <pre>results_lgbm = pd.concat([results_lgbm, pull()], ignore_index=True)\nresults_lgbm['Model'].replace({\n    'Light Gradient Boosting Machine': 'LGBM with Khiops AutoFeature Engineering',\n    }, inplace=True)\nresults_lgbm.sort_values(by=\"Accuracy\",ascending=False)\n</pre> results_lgbm = pd.concat([results_lgbm, pull()], ignore_index=True) results_lgbm['Model'].replace({     'Light Gradient Boosting Machine': 'LGBM with Khiops AutoFeature Engineering',     }, inplace=True) results_lgbm.sort_values(by=\"Accuracy\",ascending=False) Out[17]: Model Accuracy AUC Recall Prec. F1 Kappa MCC TT (Sec) 1 LGBM with Khiops AutoFeature Engineering 0.9451 0.8295 0.0489 0.5249 0.0891 0.0805 0.1470 0.776 0 LGBM with root table only 0.9354 0.6418 0.0251 0.1158 0.0411 0.0216 0.0289 0.416 In\u00a0[18]: Copied! <pre>df_plot = results_lgbm.drop(\"TT (Sec)\",axis=1).melt(id_vars=['Model'], var_name='Metric', value_name='Value')\n\nplt.figure(figsize=(14, 6))\n\n# Create a bar plot with Seaborn\nsns.barplot(x='Metric', y='Value', hue='Model', data=df_plot, palette=\"Set3\")\n\nplt.title(\"LGBM performances improvements with autofeature engineering\")\nplt.ylabel('Value')\nplt.xlabel('Metric')\n\nplt.legend(title='Model', bbox_to_anchor=(1.05, 1), loc='upper left')\n\nplt.tight_layout()\nplt.show()\n</pre> df_plot = results_lgbm.drop(\"TT (Sec)\",axis=1).melt(id_vars=['Model'], var_name='Metric', value_name='Value')  plt.figure(figsize=(14, 6))  # Create a bar plot with Seaborn sns.barplot(x='Metric', y='Value', hue='Model', data=df_plot, palette=\"Set3\")  plt.title(\"LGBM performances improvements with autofeature engineering\") plt.ylabel('Value') plt.xlabel('Metric')  plt.legend(title='Model', bbox_to_anchor=(1.05, 1), loc='upper left')  plt.tight_layout() plt.show() <p>Finally, when we compare the performance of the LGBM classifier using, or not, the aggregates generated by Khiops, we appreciate the extent to which the Auto Feature Engineering algorithm is able to extract useful information from the secondary tables. This work would have been painstaking to do by hand, requiring numerous interactions between the data scientist and the business experts, and a great deal of trial and error.</p> In\u00a0[19]: Copied! <pre># we use a KhiopsClassifier to leverage the full pipeline\npkc_accidents = KhiopsClassifier(n_trees=0, n_features=1000)\n\n# Just fit it ! \npkc_accidents.fit(X_accidents_train, y_accident_train)\n</pre> # we use a KhiopsClassifier to leverage the full pipeline pkc_accidents = KhiopsClassifier(n_trees=0, n_features=1000)  # Just fit it !  pkc_accidents.fit(X_accidents_train, y_accident_train) Out[19]: <pre>KhiopsClassifier(auto_sort=True, internal_sort=None, key=None, n_features=1000,\n                 n_pairs=0, n_trees=0, output_dir=None, verbose=False)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KhiopsClassifier<pre>KhiopsClassifier(auto_sort=True, internal_sort=None, key=None, n_features=1000,\n                 n_pairs=0, n_trees=0, output_dir=None, verbose=False)</pre> In\u00a0[20]: Copied! <pre># Let's take a look at the classifier's performance on the training set\ntrain_eval = pkc_accidents.model_report_.train_evaluation_report.get_snb_performance()\n</pre> # Let's take a look at the classifier's performance on the training set train_eval = pkc_accidents.model_report_.train_evaluation_report.get_snb_performance() In\u00a0[21]: Copied! <pre>print(f\"Accident train accuracy: {train_eval.accuracy}\")\nprint(f\"Accident train AUC     : {train_eval.auc}\")\n\nclasses = train_eval.confusion_matrix.values\nconfusion_matrix = pd.DataFrame(\n    train_eval.confusion_matrix.matrix,\n    columns=classes,\n    index=classes,\n)\nprint(\"Accident train confusion matrix:\")\nconfusion_matrix\n</pre> print(f\"Accident train accuracy: {train_eval.accuracy}\") print(f\"Accident train AUC     : {train_eval.auc}\")  classes = train_eval.confusion_matrix.values confusion_matrix = pd.DataFrame(     train_eval.confusion_matrix.matrix,     columns=classes,     index=classes, ) print(\"Accident train confusion matrix:\") confusion_matrix <pre>Accident train accuracy: 0.945036\nAccident train AUC     : 0.818541\nAccident train confusion matrix:\n</pre> Out[21]: 0 1 0 54573 3152 1 24 34 <p>Now let's take a closer look at the trained model, by examining the aggregates generated by the Auto Feature Engineering step and selected by Paricimonuous Training. The next cell shows the names of the selected aggregates, sorted by decreasing weight, and the level is also displayed:</p> <ul> <li>the level measures the extent to which the variable is correlated with the target, reflecting the importance of the variable on its own (more details here).</li> <li>the weight measures the importance of the variable within the learned classifier, reflecting the information provided by this variable orthogonally to the others (more details here).</li> </ul> In\u00a0[22]: Copied! <pre>kpis = {\"Feature\" : [], \"Level\": [], \"Weight\": []}\n\nfor var in pkc_accidents.model_report_.modeling_report.get_snb_predictor().selected_variables:\n    kpis[\"Feature\"].append(var.name)\n    kpis[\"Level\"].append(var.level)\n    kpis[\"Weight\"].append(var.weight)\n\n    \ndf_kpis = pd.DataFrame(kpis).sort_values(by = 'Weight', ascending=False)\ndf_kpis.head(10).style.set_properties(subset=['Feature'], **{'width': '400px'})\n</pre> kpis = {\"Feature\" : [], \"Level\": [], \"Weight\": []}  for var in pkc_accidents.model_report_.modeling_report.get_snb_predictor().selected_variables:     kpis[\"Feature\"].append(var.name)     kpis[\"Level\"].append(var.level)     kpis[\"Weight\"].append(var.weight)       df_kpis = pd.DataFrame(kpis).sort_values(by = 'Weight', ascending=False) df_kpis.head(10).style.set_properties(subset=['Feature'], **{'width': '400px'}) Out[22]: Feature Level Weight 27 Count(Vehicles) where Category = Car&lt;=3.5T 0.000940 0.792969 10 CountDistinct(Vehicles.Direction) 0.003901 0.754883 11 Mode(Vehicles.Category) where MobileObstacle not in {Vehicle, } 0.004293 0.655273 20 Mode(Vehicles.ImpactPoint) where Category &lt;&gt; Car&lt;=3.5T 0.002214 0.541992 0 InAgglomeration 0.062196 0.539062 24 CountDistinct(Vehicles.Category) where ImpactPoint = Front 0.001865 0.534180 1 Department 0.050247 0.504395 13 PostalAddress 0.005056 0.487305 28 Weather 0.001443 0.428711 3 Light 0.025051 0.319336 <p>As shown in the previous cell, the aggregates generated by khiops are identified by their calculation formula, which greatly facilitates their interpretation. For example, here's what the first three aggregates mean:</p> <ul> <li>\"Count(Vehicles) where Category = Car&lt;=3.5T\" is the number of light car involved in the accident;</li> <li>\"CountDistinct(Vehicles.Direction)\" is the number of different directions taken by the vehicles involved;</li> <li>\"Mode(Vehicles.Category) where MobileObstacle not in {Vehicle, }\" majority category of vehicles striking a moving obstacle (excluding vehicles).</li> </ul> <p>In practice, the models trained by khiops are easy to understand and make it easy to interact with technical experts. To make the link with the figure in the introduction to this notebook, let's now look at how the size of the data representation evolves throughout the pipeline:</p> In\u00a0[23]: Copied! <pre># nb generated vs. selected features :\nNb_original_features = len(df_accident_train.head())\nNb_selected_features = len(df_kpis)\n\nprint(\"Nb original features  = \" + str(Nb_original_features))\nprint(\"Nb generated features = 1000\")\nprint(\"Nb selected features  = \" + str(Nb_selected_features))\n</pre>  # nb generated vs. selected features : Nb_original_features = len(df_accident_train.head()) Nb_selected_features = len(df_kpis)  print(\"Nb original features  = \" + str(Nb_original_features)) print(\"Nb generated features = 1000\") print(\"Nb selected features  = \" + str(Nb_selected_features))  <pre>Nb original features  = 5\nNb generated features = 1000\nNb selected features  = 47\n</pre> <p>This training pipeline starts with 5 features in the main table. The Auto Features Engineering step generates 1000 aggregates (this is a maximum number provided by the user). Then, the Parcimonious Training step selects 47 features out of all those available, in order to find the smallest subset of variables that are collectively the most informative and the most independent of each other. In the end, the resulting model is both accurate and easy to interpret. For more in-depth model interpretation, please consult the page describing the visualization tool supplied with Khiops.</p>"},{"location":"tutorials/Notebooks/Use_in_any_ML_pipeline/#auto-feature-engineering-more","title":"Auto Feature Engineering &amp; more\u00b6","text":""},{"location":"tutorials/Notebooks/Use_in_any_ML_pipeline/#introduction","title":"Introduction\u00b6","text":"<p>This Notebook presents two key components of the Auto ML pipeline provided by Khiops:</p> <ul> <li>Auto Feature Engineering which automatically generates a large number of informative aggregates from secondary tables of a multi-table training set,</li> <li>Parsimonious Training which trains a model by selecting a small subset of independent and highly informative variables (native or aggregates).</li> </ul> <p>The sequencing of these two steps greatly improves model interpretability. To be more precise, data representation varies in size over the whole pipeline (see the figure bellow). On the one hand, Auto Feature Engineering explores a large number of aggregates, enriching the data representation with useful but possibly redundant information. On the other hand, Parsimonious Training reduces the data representation by selecting a few informative and independent variables. The contributions of the selected variables are almost additive, since their interactions are reduced to a minimum, making the model easy to interpret.</p> <p></p> <p>Combined with the fact that the aggregates generated in the Auto Feature Engineering step have explicit names, this makes the models produced by Khiops very easy to understand. A visualization tool is provided for this purpose, making it possible to understand and visualize the entire Auto ML pipeline, from optimal encoding to model evaluation.</p> <p>In this notebook, we'll explore the Khiops' Auto Feature Engineering capabilities which is unrivalled considering overfitting prevention, interpretability and scalability. We demonstrate that khiops' Auto Feature Engineering algorithm can be coupled to any classifier (here, using an LGBM classifier), dramatically improving the productivity of data scientists who no longer have to do feature engineering by hand. Finally, we demonstrate the benefits of using the full pipeline provided by Khiops, to leverage parsimonious training and considerably increase model interpretability.</p> <p>We will illustrate this using \"Accidents\" dataset. This dataset describes the characteristics of road accidents that occurred in France in 2018. It has three tables with the following schema:</p> <pre><code>Accidents\n|\n| -- 1:n -- Vehicles\n              |\n              |-- 1:n -- Users\n</code></pre>"},{"location":"tutorials/Notebooks/Use_in_any_ML_pipeline/#installation-and-set-up","title":"Installation and set up\u00b6","text":"<p>If you do not use our official <code>khiops-notebook</code> Jupyter Docker image, you may have to install khiops locally using <code>conda</code>:</p>"},{"location":"tutorials/Notebooks/Use_in_any_ML_pipeline/#import-and-preparation-of-data","title":"Import and preparation of data\u00b6","text":"<p>For this notebook, we use the \"Accident\" French Dataset. More details on the French Government Open Data Site.</p> <p>This dataset is also available on our khiops-samples repository on Github.</p> <p>This dataset has three tables <code>Accident</code>, <code>Vehicle</code>, and <code>User</code> organized in the following relational schema.</p> <pre><code>Accident\n|\n| -- 1:n -- Vehicle\n|             |\n|             |-- 1:n -- User\n</code></pre> <p>Each accident has associated one or more vehicles. The vehicles involved in an accident have in turn associated one or more road users (passengers and pedestrians).</p> <p>The fields of each table are self-explanatory, and so are their values. The target in the <code>Accident</code> table is the constructed variable <code>Gravity</code> which is set to <code>Lethal</code> if there was at least one casualty in the accident.</p>"},{"location":"tutorials/Notebooks/Use_in_any_ML_pipeline/#autofeature-engineering-overview","title":"Autofeature Engineering Overview\u00b6","text":""},{"location":"tutorials/Notebooks/Use_in_any_ML_pipeline/#just-describe-multi-table-data","title":"Just describe multi-table data\u00b6","text":"<p>When the input data is multitable, Khiops expect a dictionary. Khiops provides a simple language for describing multi-table data. Let's create this special <code>X</code> input:</p>"},{"location":"tutorials/Notebooks/Use_in_any_ML_pipeline/#now-let-khiops-do-the-work","title":"Now, let Khiops do the work\u00b6","text":"<p>In this section, we use a <code>KhiopsEncoder</code> to build a flat table containing the aggregates generated by Khiops. In the next section, we'll use this table to train a LGBM classifier. The syntax used to fit this encoder is standard and simply consists of using the special dictionary entry <code>X</code> defined above. To use this encoder :</p> <ul> <li>You can select the number of features <code>n_features</code> to be generated by Khiops (this is a max number). The default setting is 100.</li> <li>You can set the number of trees to zero (<code>n_trees=0</code>). By default, Khiops builds 10 decision trees to enrich the main table with the generated categorical variables (each corresponding to the leef of the trees). This is not necessary for this tutorial.</li> <li>No other parameters are requested from the user. However, for a better visualization of the coded dataset with intervals and groups instead of IDs, the user has the option of replacing <code>part_id</code> with <code>part_label</code> in the two parameters <code>transform_type_numerical</code> and <code>transform_type_categorical</code>.</li> </ul> <p>Note that features are created in a supervised way, taking into account the target variable. This algorithm is intrinsically regularized, i.e. it avoids the risk of over-fitting due to the generation of over-complex aggregates.</p>"},{"location":"tutorials/Notebooks/Use_in_any_ML_pipeline/#using-khiops-autofeature-engineering-in-your-pipeline","title":"Using Khiops Autofeature Engineering in your pipeline\u00b6","text":"<p>In this section, we use the Khiops encoder within a complete pipeline using the pyCaret library. In particular, we will only consider an LGBM classifier.</p>"},{"location":"tutorials/Notebooks/Use_in_any_ML_pipeline/#boosting-model-interpretability-with-khiops-end-to-end-pipeline","title":"Boosting model interpretability with Khiops' end-to-end pipeline\u00b6","text":"<p>In this section, we use the complete pipeline provided by Khiops, which chains Auto Feature Engineering and Parsimonious Training through a simplistic syntax (a simple fit function). Using this pipeline has a number of advantages, it considerably improves model interpretability, it's ultra-easy to use, and it scales very well.</p>"},{"location":"tutorials/Notebooks/multi_table_classifier/","title":"Multi-Table Tutorial","text":"In\u00a0[1]: Copied! <pre>import pandas as pd\nfrom sklearn import metrics\nfrom khiops import core as kh\nfrom khiops.sklearn import KhiopsClassifier\nfrom khiops.sklearn import KhiopsClassifier, train_test_split_dataset\n</pre> import pandas as pd from sklearn import metrics from khiops import core as kh from khiops.sklearn import KhiopsClassifier from khiops.sklearn import KhiopsClassifier, train_test_split_dataset In\u00a0[2]: Copied! <pre># Method 1: Load data directly from GitHub (recommended for quick tests or small datasets)\nurl_datasets = \"https://raw.githubusercontent.com/KhiopsML/khiops-samples/10.2.4\"\naccidents_df = pd.read_csv(f\"{url_datasets}/Accidents/Accidents.txt\", delimiter='\\t')\nvehicles_df = pd.read_csv(f\"{url_datasets}/Accidents/Vehicles.txt\", delimiter='\\t')\nusers_df = pd.read_csv(f\"{url_datasets}/Accidents/Users.txt\", delimiter='\\t')\nplaces_df = pd.read_csv(f\"{url_datasets}/Accidents/Places.txt\", delimiter='\\t', low_memory=False)\n\n# Method 2: Load data locally after downloading all Khiops samples (best for offline use)\n# from khiops.tools import download_datasets\n# download_datasets() \n\n# accidents_dataset_dir = f\"{kh.get_samples_dir()}/Accidents\"\n# accidents_df = pd.read_csv(f\"{accidents_dataset_dir}/Accidents.txt\", sep=\"\\t\")\n# vehicles_df = pd.read_csv(f\"{accidents_dataset_dir}/Vehicles.txt\", sep=\"\\t\")\n# users_df = pd.read_csv(f\"{accidents_dataset_dir}/Users.txt\", sep=\"\\t\")\n# places_df = pd.read_csv(f\"{accidents_dataset_dir}/Places.txt\", sep=\"\\t\", low_memory=False)\n\n# Display the first records from each table\nprint(\"Accidents table:\")\ndisplay(accidents_df.head(5))\nprint(\"Vehicles table:\")\ndisplay(vehicles_df.head(5))\nprint(\"Users table:\")\ndisplay(users_df.head(5))\nprint(\"Places table:\")\ndisplay(places_df.head(5))\n</pre> # Method 1: Load data directly from GitHub (recommended for quick tests or small datasets) url_datasets = \"https://raw.githubusercontent.com/KhiopsML/khiops-samples/10.2.4\" accidents_df = pd.read_csv(f\"{url_datasets}/Accidents/Accidents.txt\", delimiter='\\t') vehicles_df = pd.read_csv(f\"{url_datasets}/Accidents/Vehicles.txt\", delimiter='\\t') users_df = pd.read_csv(f\"{url_datasets}/Accidents/Users.txt\", delimiter='\\t') places_df = pd.read_csv(f\"{url_datasets}/Accidents/Places.txt\", delimiter='\\t', low_memory=False)  # Method 2: Load data locally after downloading all Khiops samples (best for offline use) # from khiops.tools import download_datasets # download_datasets()   # accidents_dataset_dir = f\"{kh.get_samples_dir()}/Accidents\" # accidents_df = pd.read_csv(f\"{accidents_dataset_dir}/Accidents.txt\", sep=\"\\t\") # vehicles_df = pd.read_csv(f\"{accidents_dataset_dir}/Vehicles.txt\", sep=\"\\t\") # users_df = pd.read_csv(f\"{accidents_dataset_dir}/Users.txt\", sep=\"\\t\") # places_df = pd.read_csv(f\"{accidents_dataset_dir}/Places.txt\", sep=\"\\t\", low_memory=False)  # Display the first records from each table print(\"Accidents table:\") display(accidents_df.head(5)) print(\"Vehicles table:\") display(vehicles_df.head(5)) print(\"Users table:\") display(users_df.head(5)) print(\"Places table:\") display(places_df.head(5)) <pre>Accidents table:\n</pre> AccidentId Gravity Date Hour Light Department Commune InAgglomeration IntersectionType Weather CollisionType PostalAddress GPSCode Latitude Longitude 0 201800000001 NonLethal 2018-01-24 15:05:00 Daylight 590 5 No Y-type Normal 2Vehicles-BehindVehicles-Frontal route des Ansereuilles M 50.55737 2.55737 1 201800000002 NonLethal 2018-02-12 10:15:00 Daylight 590 11 Yes Square VeryGood NoCollision Place du g\u00e9n\u00e9ral de Gaul M 50.52936 2.52936 2 201800000003 NonLethal 2018-03-04 11:35:00 Daylight 590 477 Yes T-type Normal NoCollision Rue  nationale M 50.51243 2.51243 3 201800000004 NonLethal 2018-05-05 17:35:00 Daylight 590 52 Yes NoIntersection VeryGood 2Vehicles-Side 30 rue Jules Guesde M 50.51974 2.51974 4 201800000005 NonLethal 2018-06-26 16:05:00 Daylight 590 477 Yes NoIntersection Normal 2Vehicles-Side 72 rue Victor Hugo M 50.51607 2.51607 <pre>Vehicles table:\n</pre> AccidentId VehicleId Direction Category PassengerNumber FixedObstacle MobileObstacle ImpactPoint Maneuver 0 201800000001 A01 Unknown Car&lt;=3.5T 0 NaN Vehicle RightFront TurnToLeft 1 201800000001 B01 Unknown Car&lt;=3.5T 0 NaN Vehicle LeftFront NoDirectionChange 2 201800000002 A01 Unknown Car&lt;=3.5T 0 NaN Pedestrian NaN NoDirectionChange 3 201800000003 A01 Unknown Motorbike&gt;125cm3 0 StationaryVehicle Vehicle Front NoDirectionChange 4 201800000003 B01 Unknown Car&lt;=3.5T 0 NaN Vehicle LeftSide TurnToLeft <pre>Users table:\n</pre> AccidentId VehicleId Seat Category Gender TripReason SafetyDevice SafetyDeviceUsed PedestrianLocation PedestrianAction PedestrianCompany BirthYear 0 201800000001 A01 1.0 Driver Male Leisure SeatBelt Yes NaN NaN Unknown 1960.0 1 201800000001 B01 1.0 Driver Male NaN SeatBelt Yes NaN NaN Unknown 1928.0 2 201800000002 A01 1.0 Driver Male NaN SeatBelt Yes NaN NaN Unknown 1947.0 3 201800000002 A01 NaN Pedestrian Male NaN Helmet NaN OnLane&lt;=OnSidewalk0mCrossing Crossing Alone 1959.0 4 201800000003 A01 1.0 Driver Male Leisure Helmet Yes NaN NaN Unknown 1987.0 <pre>Places table:\n</pre> AccidentId RoadType RoadNumber RoadSecNumber RoadLetter Circulation LaneNumber SpecialLane Slope RoadMarkerId RoadMarkerDistance Layout StripWidth LaneWidth SurfaceCondition Infrastructure Localization SchoolNear 0 201800000001 Departamental 41 NaN C TwoWay 2.0 0 Flat NaN NaN RightCurve NaN NaN Normal Unknown Lane 0.0 1 201800000002 Communal 41 NaN D TwoWay 2.0 0 Flat NaN NaN LeftCurve NaN NaN Normal Unknown Lane 0.0 2 201800000003 Departamental 39 NaN D TwoWay 2.0 0 Flat NaN NaN Straight NaN NaN Normal Unknown Lane 0.0 3 201800000004 Departamental 39 NaN NaN TwoWay 2.0 0 Flat NaN NaN Straight NaN NaN Normal Unknown Lane 0.0 4 201800000005 Communal NaN NaN NaN OneWay 1.0 0 Flat NaN NaN Straight NaN NaN Normal Unknown Lane 0.0 In\u00a0[3]: Copied! <pre>X = {\n    \"main_table\": \"Accidents\",\n    \"tables\": {\n        \"Accidents\": (accidents_df.drop(\"Gravity\", axis=1), \"AccidentId\"), # We drop the target column \"Gravity\"\n        \"Vehicles\": (vehicles_df, [\"AccidentId\", \"VehicleId\"]),\n        \"Users\": (users_df, [\"AccidentId\", \"VehicleId\"]),\n        \"Places\": (places_df, \"AccidentId\"),\n    },\n    \"relations\": [\n        (\"Accidents\", \"Vehicles\"),\n        (\"Vehicles\", \"Users\"),\n        (\"Accidents\", \"Places\", True),\n    ],\n}\ny = accidents_df[\"Gravity\"]\n</pre> X = {     \"main_table\": \"Accidents\",     \"tables\": {         \"Accidents\": (accidents_df.drop(\"Gravity\", axis=1), \"AccidentId\"), # We drop the target column \"Gravity\"         \"Vehicles\": (vehicles_df, [\"AccidentId\", \"VehicleId\"]),         \"Users\": (users_df, [\"AccidentId\", \"VehicleId\"]),         \"Places\": (places_df, \"AccidentId\"),     },     \"relations\": [         (\"Accidents\", \"Vehicles\"),         (\"Vehicles\", \"Users\"),         (\"Accidents\", \"Places\", True),     ], } y = accidents_df[\"Gravity\"] <p>Note the main table has one key (<code>AccidentId</code>) and the secondary table <code>Vehicles</code> has two (<code>AccidentId</code> and <code>VehicleId</code>).</p> <p>To describe relations between tables, the field <code>relations</code> must be added to the dictionary of table specifications. This field is a list of pairs of tables of the form</p> <pre><code>(&lt;parent table name&gt;, &lt;child table name&gt;)\n</code></pre> <p>The khiops library provides the helper function <code>train_test_split_dataset</code> that splits a multi-table specification into two specs for train and test:</p> In\u00a0[4]: Copied! <pre>X_train, X_test, y_train, y_test = train_test_split_dataset(X, y, random_state=123)\n</pre> X_train, X_test, y_train, y_test = train_test_split_dataset(X, y, random_state=123) <p>We now fit our classifier on the train split. By default, the Khiops creates at most 100 multi-table variables (<code>n_features</code>) and 10 random decision trees (<code>n_trees</code>). We change these values for this example:</p> In\u00a0[5]: Copied! <pre>khc = KhiopsClassifier(n_features=1000, n_trees=0)\nkhc.fit(X_train, y_train)\n</pre> khc = KhiopsClassifier(n_features=1000, n_trees=0) khc.fit(X_train, y_train) Out[5]: <pre>KhiopsClassifier(n_features=1000, n_trees=0)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0KhiopsClassifieriFitted<pre>KhiopsClassifier(n_features=1000, n_trees=0)</pre> In\u00a0[6]: Copied! <pre>train_performance = khc.model_report_.train_evaluation_report.get_snb_performance()\nprint(f\"Accidents train accuracy: {train_performance.accuracy}\")\nprint(f\"Accidents train auc     : {train_performance.auc}\")\n</pre> train_performance = khc.model_report_.train_evaluation_report.get_snb_performance() print(f\"Accidents train accuracy: {train_performance.accuracy}\") print(f\"Accidents train auc     : {train_performance.auc}\") <pre>Accidents train accuracy: 0.944205\nAccidents train auc     : 0.845932\n</pre> <p>The <code>PredictorPerformance</code> object has also a confusion matrix attribute:</p> In\u00a0[7]: Copied! <pre>confusion_matrix = pd.DataFrame(\n    train_performance.confusion_matrix.matrix,\n    columns=train_performance.confusion_matrix.values,\n    index=train_performance.confusion_matrix.values,\n)\nprint(\"AccidentsSummary train confusion matrix:\")\nconfusion_matrix\n</pre> confusion_matrix = pd.DataFrame(     train_performance.confusion_matrix.matrix,     columns=train_performance.confusion_matrix.values,     index=train_performance.confusion_matrix.values, ) print(\"AccidentsSummary train confusion matrix:\") confusion_matrix <pre>AccidentsSummary train confusion matrix:\n</pre> Out[7]: Lethal NonLethal Lethal 69 52 NonLethal 2366 40850 <p>If you have installed the Khiops Visualization app you may explore the full learning report by executing the code below.</p> In\u00a0[8]: Copied! <pre># Uncomment the lines below\n# khc.export_report_file(\"./adult_report.khj\")\n# kh.visualize_report(\"./adult_report.khj\")\n</pre> # Uncomment the lines below # khc.export_report_file(\"./adult_report.khj\") # kh.visualize_report(\"./adult_report.khj\") In\u00a0[9]: Copied! <pre>y_pred_test = khc.predict(X_test)\ny_probas_test = khc.predict_proba(X_test)\nprint(\"Classes:\")\ndisplay(khc.classes_)\nprint()\nprint(\"Predictions (first 10 values):\")\ndisplay(y_pred_test[:10])\nprint()\nprint(\"Probabilities (first 10 rows):\")\ndisplay(y_probas_test[:10,])\n</pre> y_pred_test = khc.predict(X_test) y_probas_test = khc.predict_proba(X_test) print(\"Classes:\") display(khc.classes_) print() print(\"Predictions (first 10 values):\") display(y_pred_test[:10]) print() print(\"Probabilities (first 10 rows):\") display(y_probas_test[:10,]) <pre>Classes:\n</pre> <pre>array(['Lethal', 'NonLethal'], dtype='&lt;U9')</pre> <pre>\nPredictions (first 10 values):\n</pre> <pre>array(['NonLethal', 'NonLethal', 'NonLethal', 'NonLethal', 'NonLethal',\n       'NonLethal', 'NonLethal', 'NonLethal', 'NonLethal', 'NonLethal'],\n      dtype='&lt;U9')</pre> <pre>\nProbabilities (first 10 rows):\n</pre> <pre>array([[3.09302957e-02, 9.69069704e-01],\n       [1.56023274e-01, 8.43976726e-01],\n       [3.33082936e-03, 9.96669171e-01],\n       [8.29378593e-04, 9.99170621e-01],\n       [5.21136609e-02, 9.47886339e-01],\n       [4.55396978e-03, 9.95446030e-01],\n       [1.46438522e-01, 8.53561478e-01],\n       [4.71464454e-03, 9.95285355e-01],\n       [5.65335988e-03, 9.94346640e-01],\n       [4.26662644e-02, 9.57333736e-01]])</pre> <p>From these predictions we compute the test accuracy and AUC (One-vs-Rest) scores  using <code>sklearn.metrics</code></p> In\u00a0[10]: Copied! <pre>accuracy_test = metrics.accuracy_score(y_test, y_pred_test)\nauc_test = metrics.roc_auc_score(y_test, y_probas_test[:,1])\nprint(f\"Accidents test accuracy: {accuracy_test}\")\nprint(f\"Accidents test auc     : {auc_test}\")\n</pre> accuracy_test = metrics.accuracy_score(y_test, y_pred_test) auc_test = metrics.roc_auc_score(y_test, y_probas_test[:,1]) print(f\"Accidents test accuracy: {accuracy_test}\") print(f\"Accidents test auc     : {auc_test}\") <pre>Accidents test accuracy: 0.9475979509898934\nAccidents test auc     : 0.831591515559879\n</pre>"},{"location":"tutorials/Notebooks/multi_table_classifier/#multi-table-tutorial-with-the-scikit-learn-api","title":"Multi-Table Tutorial with the Scikit-learn API\u00b6","text":"<p>In this notebook, we will learn how to train a classifier for a simple multi-table dataset. It is recommended to see the single table tutorial first.</p>"},{"location":"tutorials/Notebooks/multi_table_classifier/#the-accidents-dataset","title":"The Accidents Dataset\u00b6","text":"<p>We'll train a multi-table classifier on a the dataset <code>Accidents</code>. This dataset describes the characteristics of traffic accidents that happened in France in 2018. It has two tables with the following schema:</p> <pre><code>Accident\n|\n+----0:n----Vehicle\n|           |\n|           +----0:n----User \n|\n+----0:1----Place            \n</code></pre> <ul> <li>The main table <code>Accident</code></li> <li>The table <code>Vehicle</code> in a <code>0:n</code> relationship with <code>Accident</code></li> <li>The table <code>User</code> in a <code>0:n</code> relationship with <code>Vehicle</code></li> <li>The table <code>Place</code> in a <code>0:1</code> relationship with <code>Accident</code></li> </ul> <p>Let's first check the content of the tables:</p>"},{"location":"tutorials/Notebooks/multi_table_classifier/#training-the-classifier","title":"Training the Classifier\u00b6","text":"<p>We start by creating our <code>X</code> and <code>y</code> for the <code>fit</code> method. For multi-table tasks <code>KhiopsClassifier</code> uses a multi-table dataset specification: It is a dictionary that describes the schema of the dataset:</p>"},{"location":"tutorials/Notebooks/multi_table_classifier/#displaying-the-classifiers-training-accuracy-and-auc","title":"Displaying the Classifiers\u2019 Training Accuracy and AUC\u00b6","text":"<p>The <code>fit</code> method calculates evaluation metrics on the training dataset. We access them via the estimator's attribute <code>model_report_</code> which is an instance of the <code>AnalysisResults</code> class. Let's check this out:</p>"},{"location":"tutorials/Notebooks/multi_table_classifier/#deploying-the-classifier-and-displaying-its-test-performance","title":"Deploying the Classifier and Displaying Its Test Performance\u00b6","text":"<p>Now that we have a fitted <code>KhiopsClassifier</code>, we are now going to deploy it on the test split.</p> <p>This can be done in two different ways:</p> <ul> <li>to predict a class that can be obtained using its <code>predict</code>.</li> <li>to predict class probabilities that can be obtained using its <code>predict_proba</code>.</li> </ul> <p>Let's first predict the <code>Accident</code> labels:</p>"},{"location":"tutorials/Notebooks/multi_table_classifier_core/","title":"Multi-Table Tutorial","text":"In\u00a0[2]: Copied! <pre>import warnings\nimport pandas as pd\nfrom khiops import core as kh\nfrom khiops.tools import download_datasets\n\n# Download the sample datasets from GitHub if not available\nwarnings.filterwarnings(\"ignore\", message=\"Download.*\") # Ignore dataset download warning\ndownload_datasets()\n</pre> import warnings import pandas as pd from khiops import core as kh from khiops.tools import download_datasets  # Download the sample datasets from GitHub if not available warnings.filterwarnings(\"ignore\", message=\"Download.*\") # Ignore dataset download warning download_datasets() In\u00a0[6]: Copied! <pre># Store the locations of the `AccidentsSummary` dataset\naccidents_table_path = f\"{kh.get_samples_dir()}/Accidents/Accidents.txt\"\nvehicles_table_path = f\"{kh.get_samples_dir()}/Accidents/Vehicles.txt\"\nusers_table_path = f\"{kh.get_samples_dir()}/Accidents/Users.txt\"\nplaces_table_path = f\"{kh.get_samples_dir()}/Accidents/Places.txt\"\n\n\n# Print the first lines of the data files\nprint(\"Accidents table:\")\ndisplay(pd.read_csv(accidents_table_path, sep=\"\\t\").head(5))\nprint(\"Vehicles table:\")\ndisplay(pd.read_csv(vehicles_table_path, sep=\"\\t\").head(5))\nprint(\"Users table:\")\ndisplay(pd.read_csv(users_table_path, sep=\"\\t\").head(5))\nprint(\"Places table:\")\ndisplay(pd.read_csv(places_table_path, sep=\"\\t\", low_memory=False).head(5))\n</pre> # Store the locations of the `AccidentsSummary` dataset accidents_table_path = f\"{kh.get_samples_dir()}/Accidents/Accidents.txt\" vehicles_table_path = f\"{kh.get_samples_dir()}/Accidents/Vehicles.txt\" users_table_path = f\"{kh.get_samples_dir()}/Accidents/Users.txt\" places_table_path = f\"{kh.get_samples_dir()}/Accidents/Places.txt\"   # Print the first lines of the data files print(\"Accidents table:\") display(pd.read_csv(accidents_table_path, sep=\"\\t\").head(5)) print(\"Vehicles table:\") display(pd.read_csv(vehicles_table_path, sep=\"\\t\").head(5)) print(\"Users table:\") display(pd.read_csv(users_table_path, sep=\"\\t\").head(5)) print(\"Places table:\") display(pd.read_csv(places_table_path, sep=\"\\t\", low_memory=False).head(5)) <pre>Accidents table:\n</pre> AccidentId Gravity Date Hour Light Department Commune InAgglomeration IntersectionType Weather CollisionType PostalAddress GPSCode Latitude Longitude 0 201800000001 NonLethal 2018-01-24 15:05:00 Daylight 590 5 No Y-type Normal 2Vehicles-BehindVehicles-Frontal route des Ansereuilles M 50.55737 2.55737 1 201800000002 NonLethal 2018-02-12 10:15:00 Daylight 590 11 Yes Square VeryGood NoCollision Place du g\u00e9n\u00e9ral de Gaul M 50.52936 2.52936 2 201800000003 NonLethal 2018-03-04 11:35:00 Daylight 590 477 Yes T-type Normal NoCollision Rue  nationale M 50.51243 2.51243 3 201800000004 NonLethal 2018-05-05 17:35:00 Daylight 590 52 Yes NoIntersection VeryGood 2Vehicles-Side 30 rue Jules Guesde M 50.51974 2.51974 4 201800000005 NonLethal 2018-06-26 16:05:00 Daylight 590 477 Yes NoIntersection Normal 2Vehicles-Side 72 rue Victor Hugo M 50.51607 2.51607 <pre>Vehicles table:\n</pre> AccidentId VehicleId Direction Category PassengerNumber FixedObstacle MobileObstacle ImpactPoint Maneuver 0 201800000001 A01 Unknown Car&lt;=3.5T 0 NaN Vehicle RightFront TurnToLeft 1 201800000001 B01 Unknown Car&lt;=3.5T 0 NaN Vehicle LeftFront NoDirectionChange 2 201800000002 A01 Unknown Car&lt;=3.5T 0 NaN Pedestrian NaN NoDirectionChange 3 201800000003 A01 Unknown Motorbike&gt;125cm3 0 StationaryVehicle Vehicle Front NoDirectionChange 4 201800000003 B01 Unknown Car&lt;=3.5T 0 NaN Vehicle LeftSide TurnToLeft <pre>Users table:\n</pre> AccidentId VehicleId Seat Category Gender TripReason SafetyDevice SafetyDeviceUsed PedestrianLocation PedestrianAction PedestrianCompany BirthYear 0 201800000001 A01 1.0 Driver Male Leisure SeatBelt Yes NaN NaN Unknown 1960.0 1 201800000001 B01 1.0 Driver Male NaN SeatBelt Yes NaN NaN Unknown 1928.0 2 201800000002 A01 1.0 Driver Male NaN SeatBelt Yes NaN NaN Unknown 1947.0 3 201800000002 A01 NaN Pedestrian Male NaN Helmet NaN OnLane&lt;=OnSidewalk0mCrossing Crossing Alone 1959.0 4 201800000003 A01 1.0 Driver Male Leisure Helmet Yes NaN NaN Unknown 1987.0 <pre>Places table:\n</pre> AccidentId RoadType RoadNumber RoadSecNumber RoadLetter Circulation LaneNumber SpecialLane Slope RoadMarkerId RoadMarkerDistance Layout StripWidth LaneWidth SurfaceCondition Infrastructure Localization SchoolNear 0 201800000001 Departamental 41 NaN C TwoWay 2.0 0 Flat NaN NaN RightCurve NaN NaN Normal Unknown Lane 0.0 1 201800000002 Communal 41 NaN D TwoWay 2.0 0 Flat NaN NaN LeftCurve NaN NaN Normal Unknown Lane 0.0 2 201800000003 Departamental 39 NaN D TwoWay 2.0 0 Flat NaN NaN Straight NaN NaN Normal Unknown Lane 0.0 3 201800000004 Departamental 39 NaN NaN TwoWay 2.0 0 Flat NaN NaN Straight NaN NaN Normal Unknown Lane 0.0 4 201800000005 Communal NaN NaN NaN OneWay 1.0 0 Flat NaN NaN Straight NaN NaN Normal Unknown Lane 0.0 <p>To train a classifier with the Khiops core API, we must specify a multi-table dataset. The schema is specified via the Khiops dictionary file, let's see the contents its for the <code>Accidents</code> dataset:</p> In\u00a0[7]: Copied! <pre>accidents_kdic_path = f\"{kh.get_samples_dir()}/Accidents/Accidents.kdic\"\nwith open(accidents_kdic_path) as accidents_kdic_file:\n    print(accidents_kdic_file.read())\n</pre> accidents_kdic_path = f\"{kh.get_samples_dir()}/Accidents/Accidents.kdic\" with open(accidents_kdic_path) as accidents_kdic_file:     print(accidents_kdic_file.read()) <pre>Root Dictionary Accident(AccidentId)\n{\n  Categorical AccidentId;\n  Categorical Gravity;\n  Date Date;\n  Time Hour;\n  Categorical Light;\n  Categorical Department;\n  Categorical Commune;\n  Categorical InAgglomeration;\n  Categorical IntersectionType;\n  Categorical Weather;\n  Categorical CollisionType;\n  Categorical PostalAddress;\n  Categorical GPSCode;\n  Numerical Latitude;\n  Numerical Longitude;\n  Entity(Place) Place;\n  Table(Vehicle) Vehicles;\n};\n\nDictionary Place(AccidentId)\n{\n  Categorical AccidentId;\n  Categorical RoadType;\n  Categorical RoadNumber;\n  Categorical RoadSecNumber;\n  Categorical RoadLetter;\n  Categorical Circulation;\n  Numerical LaneNumber;\n  Categorical SpecialLane;\n  Categorical Slope;\n  Categorical RoadMarkerId;\n  Numerical RoadMarkerDistance;\n  Categorical Layout;\n  Numerical StripWidth;\n  Numerical LaneWidth;\n  Categorical SurfaceCondition;\n  Categorical Infrastructure;\n  Categorical Localization;\n  Categorical SchoolNear;\n};\n\n\nDictionary Vehicle(AccidentId, VehicleId)\n{\n  Categorical AccidentId;\n  Categorical VehicleId;\n  Categorical Direction;\n  Categorical Category;\n  Numerical PassengerNumber;\n  Categorical FixedObstacle;\n  Categorical MobileObstacle;\n  Categorical ImpactPoint;\n  Categorical Maneuver;\n  Table(User) Users;\n};\n\nDictionary User(AccidentId, VehicleId) {\n  Categorical AccidentId;\n  Categorical VehicleId;\n  Categorical Seat;\n  Categorical Category;\n  Categorical Gender;\n  Categorical TripReason;\n  Categorical SafetyDevice;\n  Categorical SafetyDeviceUsed;\n  Categorical PedestrianLocation;\n  Categorical PedestrianAction;\n  Categorical PedestrianCompany;\n  Numerical BirthYear;\n};\n\n</pre> <p>We note that the <code>Accident</code> table contains a special <code>Table</code> variable. This special variable allows to create a <code>1:n</code> relation. The target table is in its argument between parentheses (<code>Vehicle</code>).</p> In\u00a0[9]: Copied! <pre>model_report_path, model_kdic_path = kh.train_predictor(\n    accidents_kdic_path,\n    \"Accident\",\n    accidents_table_path,\n    \"Gravity\",\n    \"./mt_results\",\n    additional_data_tables={\n        \"Accident`Vehicles\": vehicles_table_path,\n        \"Accident`Vehicles`Users\": users_table_path,\n        \"Accident`Place\": places_table_path,\n    },\n    max_constructed_variables=1000,\n    max_trees=0,\n)\n</pre> model_report_path, model_kdic_path = kh.train_predictor(     accidents_kdic_path,     \"Accident\",     accidents_table_path,     \"Gravity\",     \"./mt_results\",     additional_data_tables={         \"Accident`Vehicles\": vehicles_table_path,         \"Accident`Vehicles`Users\": users_table_path,         \"Accident`Place\": places_table_path,     },     max_constructed_variables=1000,     max_trees=0, ) In\u00a0[10]: Copied! <pre>model_report = kh.read_analysis_results_file(model_report_path)\ntrain_performance = model_report.train_evaluation_report.get_snb_performance()\ntest_performance = model_report.test_evaluation_report.get_snb_performance()\n\nprint(f\"Accidents train accuracy: {train_performance.accuracy}\")\nprint(f\"Accidents train auc     : {train_performance.auc}\")\nprint(f\"Accidents test accuracy : {test_performance.accuracy}\")\nprint(f\"Accidents test auc      : {test_performance.auc}\")\n</pre> model_report = kh.read_analysis_results_file(model_report_path) train_performance = model_report.train_evaluation_report.get_snb_performance() test_performance = model_report.test_evaluation_report.get_snb_performance()  print(f\"Accidents train accuracy: {train_performance.accuracy}\") print(f\"Accidents train auc     : {train_performance.auc}\") print(f\"Accidents test accuracy : {test_performance.accuracy}\") print(f\"Accidents test auc      : {test_performance.auc}\") <pre>Accidents train accuracy: 0.94475\nAccidents train auc     : 0.844525\nAccidents test accuracy : 0.945303\nAccidents test auc      : 0.839569\n</pre> <p>We are now going to deploy the <code>Accidents</code> classifier that we have just trained.</p> <p>To this end we use the model dictionary file that the <code>train_predictor</code> function created in conjunction the the <code>deploy_model</code> core API function. Note that the name of the dictionary for the model is <code>SNB_Accident</code>.</p> <p>Similarly to the model training we must set the <code>additional_data_tables</code> parameter to take into account the secondary table.</p> <p>For simplicity, we'll just deploy on the whole data table file (one usually would do this on new data):</p> In\u00a0[11]: Copied! <pre>accidents_deployed_path = \"./mt_results/accidents_deployed.txt\"\nkh.deploy_model(\n    model_kdic_path,             # Path of the model dictionary file\n    \"SNB_Accident\",              # Name of the model dictionary\n    accidents_table_path,        # Path of the table to deploy the model\n    accidents_deployed_path,     # Path of the output (deployed) file\n    additional_data_tables = {   # Pairs of {\"data-path\": \"file-path\"} describing the other tables\n        \"SNB_Accident`Vehicles\": vehicles_table_path,\n        \"SNB_Accident`Vehicles`Users\": users_table_path,\n        \"SNB_Accident`Place\": places_table_path,\n    },\n)\n</pre> accidents_deployed_path = \"./mt_results/accidents_deployed.txt\" kh.deploy_model(     model_kdic_path,             # Path of the model dictionary file     \"SNB_Accident\",              # Name of the model dictionary     accidents_table_path,        # Path of the table to deploy the model     accidents_deployed_path,     # Path of the output (deployed) file     additional_data_tables = {   # Pairs of {\"data-path\": \"file-path\"} describing the other tables         \"SNB_Accident`Vehicles\": vehicles_table_path,         \"SNB_Accident`Vehicles`Users\": users_table_path,         \"SNB_Accident`Place\": places_table_path,     }, ) <p>The deployed model is in the path in the variable <code>accidents_deployed_path</code>, let's have a look at it</p> In\u00a0[12]: Copied! <pre>display(pd.read_csv(accidents_deployed_path, sep=\"\\t\").head(10))\n</pre> display(pd.read_csv(accidents_deployed_path, sep=\"\\t\").head(10)) AccidentId PredictedGravity ProbGravityLethal ProbGravityNonLethal 0 201800000001 NonLethal 0.153842 0.846158 1 201800000002 NonLethal 0.121561 0.878439 2 201800000003 NonLethal 0.067390 0.932610 3 201800000004 NonLethal 0.025705 0.974295 4 201800000005 NonLethal 0.012496 0.987504 5 201800000006 NonLethal 0.121613 0.878387 6 201800000007 NonLethal 0.095323 0.904677 7 201800000008 NonLethal 0.096077 0.903923 8 201800000009 NonLethal 0.167294 0.832706 9 201800000010 NonLethal 0.055217 0.944783 <p>The deployed data table file contains three columns</p> <ul> <li><code>PredictedGravity</code>: Which contains the class prediction</li> <li><code>ProbGravityLethal</code>, <code>ProbGravityNonLethal</code>: Which contain the probability of each class of <code>Accidents</code>.</li> </ul>"},{"location":"tutorials/Notebooks/multi_table_classifier_core/#multi-table-tutorial-with-the-core-api","title":"Multi-Table Tutorial with the core API\u00b6","text":"<p>In this notebook, we will learn how to train a classifier for a simple multi-table dataset. It is recommended to see the single table tutorial first and understand the basics of Khiops dictionary files.</p>"},{"location":"tutorials/Notebooks/multi_table_classifier_core/#the-accidents-dataset","title":"The Accidents Dataset\u00b6","text":"<p>We'll train a multi-table classifier on a the dataset <code>Accidents</code>. This dataset describes the characteristics of traffic accidents that happened in France in 2018. It has two tables with the following schema:</p> <pre><code>Accident\n|\n+----0:n----Vehicle\n|           |\n|           +----0:n----User\n|\n+----0:1----Place            \n</code></pre> <ul> <li>The main table <code>Accident</code></li> <li>The table <code>Vehicle</code> in a <code>0:n</code> relationship with <code>Accident</code></li> <li>The table <code>User</code> in a <code>0:n</code> relationship with <code>Vehicle</code></li> <li>The table <code>Place</code> in a <code>0:1</code> relationship with <code>Accident</code></li> </ul> <p>Let's first check the content of the tables:</p>"},{"location":"tutorials/Notebooks/multi_table_classifier_core/#training-the-classifier","title":"Training the Classifier\u00b6","text":"<p>While the dictionary file specifies the table schemas and their relations, it does not contain any information about the data files. On a single table task the third mandatory parameter of <code>train_predictor</code> specifies the data table file. For multi-table tasks this parameter is still used to specify the main table; to specify the rest of the tables we use the optional parameter <code>additional_data_tables</code>.</p> <p>The <code>additional_data_tables</code> parameter is a Python <code>dict</code> whose keys are the data paths of each table and the values are their file paths (in our case just a single pair). For more information about data-paths see basics of Khiops dictionary files.</p> <p>By default, the Khiops creates at most 100 multi-table variables (<code>max_variables</code>) and 10 random decision trees (<code>max_trees</code>). We change these values for this example:</p>"},{"location":"tutorials/Notebooks/multi_table_classifier_core/#displaying-the-classifiers-accuracy-and-auc","title":"Displaying the Classifier\u2019s Accuracy and AUC\u00b6","text":"<p>Khiops calculates evaluation metrics for the train/test split datasets. We access them by loading the report file into an <code>AnalysisResults</code> object. Let's check this out:</p>"},{"location":"tutorials/Notebooks/multi_table_classifier_core/#deploying-the-classifier","title":"Deploying the Classifier\u00b6","text":""},{"location":"tutorials/Notebooks/single_table_classifier/","title":"Single-Table Tutorial","text":"In\u00a0[1]: Copied! <pre>import pandas as pd\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\nfrom khiops import core as kh\nfrom khiops.sklearn import KhiopsClassifier\n</pre> import pandas as pd from sklearn import metrics from sklearn.model_selection import train_test_split from khiops import core as kh from khiops.sklearn import KhiopsClassifier In\u00a0[2]: Copied! <pre># Method 1: Load data directly from GitHub (recommended for quick tests or small datasets)\nurl = \"https://raw.githubusercontent.com/KhiopsML/khiops-samples/10.2.4/Iris/Iris.txt\"\niris_df = pd.read_csv(url, delimiter='\\t')\n\n# Method 2: Load data locally after downloading all Khiops samples (best for offline use)\n# from khiops.tools import download_datasets\n# download_datasets() \n# iris_path = f\"{kh.get_samples_dir()}/Iris/Iris.txt\"\n# iris_df = pd.read_csv(iris_path, sep=\"\\t\")\n\n# Display the first 10 records from the dataset\niris_df[:10]\n</pre> # Method 1: Load data directly from GitHub (recommended for quick tests or small datasets) url = \"https://raw.githubusercontent.com/KhiopsML/khiops-samples/10.2.4/Iris/Iris.txt\" iris_df = pd.read_csv(url, delimiter='\\t')  # Method 2: Load data locally after downloading all Khiops samples (best for offline use) # from khiops.tools import download_datasets # download_datasets()  # iris_path = f\"{kh.get_samples_dir()}/Iris/Iris.txt\" # iris_df = pd.read_csv(iris_path, sep=\"\\t\")  # Display the first 10 records from the dataset iris_df[:10] Out[2]: SepalLength SepalWidth PetalLength PetalWidth Class 0 5.1 3.5 1.4 0.2 Iris-setosa 1 4.9 3.0 1.4 0.2 Iris-setosa 2 4.7 3.2 1.3 0.2 Iris-setosa 3 4.6 3.1 1.5 0.2 Iris-setosa 4 5.0 3.6 1.4 0.2 Iris-setosa 5 5.4 3.9 1.7 0.4 Iris-setosa 6 4.6 3.4 1.4 0.3 Iris-setosa 7 5.0 3.4 1.5 0.2 Iris-setosa 8 4.4 2.9 1.4 0.2 Iris-setosa 9 4.9 3.1 1.5 0.1 Iris-setosa In\u00a0[3]: Copied! <pre># Drop the \"class\" column to create the feature set (X).\nX = iris_df.drop(\"Class\", axis=1)\n\n# Extract the \"class\" column to create the target labels (y).\ny = iris_df[\"Class\"]\n</pre> # Drop the \"class\" column to create the feature set (X). X = iris_df.drop(\"Class\", axis=1)  # Extract the \"class\" column to create the target labels (y). y = iris_df[\"Class\"] <p>Then we can construct our final train/test dataset</p> In\u00a0[4]: Copied! <pre>X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=123)\n</pre> X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=123) <p>Let's check the contents of the feature matrix and the target vector:</p> In\u00a0[5]: Copied! <pre>print(\"Features:\")\ndisplay(X_train)\nprint(\"Labels:\")\ndisplay(y_train.unique())\n</pre> print(\"Features:\") display(X_train) print(\"Labels:\") display(y_train.unique()) <pre>Features:\n</pre> SepalLength SepalWidth PetalLength PetalWidth 16 5.4 3.9 1.3 0.4 82 5.8 2.7 3.9 1.2 60 5.0 2.0 3.5 1.0 35 5.0 3.2 1.2 0.2 143 6.8 3.2 5.9 2.3 ... ... ... ... ... 17 5.1 3.5 1.4 0.3 98 5.1 2.5 3.0 1.1 66 5.6 3.0 4.5 1.5 126 6.2 2.8 4.8 1.8 109 7.2 3.6 6.1 2.5 <p>112 rows \u00d7 4 columns</p> <pre>Labels:\n</pre> <pre>array(['Iris-setosa', 'Iris-versicolor', 'Iris-virginica'], dtype=object)</pre> <p>We are ready to train the <code>KhiopsClassifier</code>: We use the <code>fit</code> method on the training data. After its execution, the <code>KhiopsClassifier</code> instance is ready to classify new Iris plants:</p> In\u00a0[6]: Copied! <pre>khc = KhiopsClassifier()\nkhc.fit(X_train, y_train)\n</pre> khc = KhiopsClassifier() khc.fit(X_train, y_train) Out[6]: <pre>KhiopsClassifier()</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0KhiopsClassifieriFitted<pre>KhiopsClassifier()</pre> In\u00a0[7]: Copied! <pre>train_performance = khc.model_report_.train_evaluation_report.get_snb_performance()\n</pre> train_performance = khc.model_report_.train_evaluation_report.get_snb_performance() <p>This object <code>train_performance</code> is of class <code>PredictorPerformance</code> and has <code>accuracy</code> and <code>auc</code> attributes:</p> In\u00a0[8]: Copied! <pre>print(f\"Iris train accuracy: {train_performance.accuracy}\")\nprint(f\"Iris train AUC     : {train_performance.auc}\")\n</pre> print(f\"Iris train accuracy: {train_performance.accuracy}\") print(f\"Iris train AUC     : {train_performance.auc}\") <pre>Iris train accuracy: 0.964286\nIris train AUC     : 0.993257\n</pre> <p>The <code>PredictorPerformance</code> object has also a confusion matrix attribute:</p> In\u00a0[9]: Copied! <pre>confusion_matrix = pd.DataFrame(\n    train_performance.confusion_matrix.matrix,\n    columns=train_performance.confusion_matrix.values,\n    index=train_performance.confusion_matrix.values,\n)\nprint(\"Iris train confusion matrix:\")\nconfusion_matrix\n</pre> confusion_matrix = pd.DataFrame(     train_performance.confusion_matrix.matrix,     columns=train_performance.confusion_matrix.values,     index=train_performance.confusion_matrix.values, ) print(\"Iris train confusion matrix:\") confusion_matrix <pre>Iris train confusion matrix:\n</pre> Out[9]: Iris-setosa Iris-versicolor Iris-virginica Iris-setosa 34 0 0 Iris-versicolor 0 41 3 Iris-virginica 0 1 33 <p>If you have installed the Khiops Visualization app you may explore the full learning report by executing the code below.</p> In\u00a0[10]: Copied! <pre># Uncomment the lines below\n# khc.export_report_file(\"./iris_report.khj\")\n# kh.visualize_report(\"./iris_report.khj\")\n</pre> # Uncomment the lines below # khc.export_report_file(\"./iris_report.khj\") # kh.visualize_report(\"./iris_report.khj\") In\u00a0[11]: Copied! <pre>y_pred_test = khc.predict(X_test)\ny_probas_test = khc.predict_proba(X_test)\nprint(\"Classes:\")\ndisplay(khc.classes_)\nprint()\nprint(\"Predictions (first 10 values):\")\ndisplay(y_pred_test[:10])\nprint()\nprint(\"Probabilities (first 10 rows):\")\ndisplay(y_probas_test[:10,])\n</pre> y_pred_test = khc.predict(X_test) y_probas_test = khc.predict_proba(X_test) print(\"Classes:\") display(khc.classes_) print() print(\"Predictions (first 10 values):\") display(y_pred_test[:10]) print() print(\"Probabilities (first 10 rows):\") display(y_probas_test[:10,]) <pre>Classes:\n</pre> <pre>array(['Iris-setosa', 'Iris-versicolor', 'Iris-virginica'], dtype='&lt;U15')</pre> <pre>\nPredictions (first 10 values):\n</pre> <pre>array(['Iris-versicolor', 'Iris-virginica', 'Iris-virginica',\n       'Iris-versicolor', 'Iris-setosa', 'Iris-virginica',\n       'Iris-versicolor', 'Iris-setosa', 'Iris-setosa', 'Iris-versicolor'],\n      dtype='&lt;U15')</pre> <pre>\nProbabilities (first 10 rows):\n</pre> <pre>array([[0.00164867, 0.9298149 , 0.06853643],\n       [0.00169434, 0.03944314, 0.95886252],\n       [0.00169434, 0.03944314, 0.95886252],\n       [0.00164867, 0.9298149 , 0.06853643],\n       [0.99482629, 0.00347827, 0.00169544],\n       [0.0019504 , 0.31942133, 0.67862827],\n       [0.00164867, 0.9298149 , 0.06853643],\n       [0.99482629, 0.00347827, 0.00169544],\n       [0.99482629, 0.00347827, 0.00169544],\n       [0.00164867, 0.9298149 , 0.06853643]])</pre> <p>From these predictions we compute the test accuracy and AUC (One-vs-Rest) scores  using <code>sklearn.metrics</code></p> In\u00a0[12]: Copied! <pre>accuracy_test = metrics.accuracy_score(y_test, y_pred_test)\nauc_test = metrics.roc_auc_score(y_test, y_probas_test, multi_class=\"ovr\")\nprint(f\"Iris test accuracy: {accuracy_test}\")\nprint(f\"Iris test AUC     : {auc_test}\")\n</pre> accuracy_test = metrics.accuracy_score(y_test, y_pred_test) auc_test = metrics.roc_auc_score(y_test, y_probas_test, multi_class=\"ovr\") print(f\"Iris test accuracy: {accuracy_test}\") print(f\"Iris test AUC     : {auc_test}\") <pre>Iris test accuracy: 0.9473684210526315\nIris test AUC     : 1.0\n</pre>"},{"location":"tutorials/Notebooks/single_table_classifier/#single-table-tutorial-with-the-scikit-learn-api","title":"Single-Table Tutorial with the Scikit-learn API\u00b6","text":"<p>On this tutorial we'll a classifier on a single table dataset.</p>"},{"location":"tutorials/Notebooks/single_table_classifier/#the-iris-dataset","title":"The Iris Dataset\u00b6","text":"<p>We'll train a classifier for the <code>Iris</code> dataset. This is a classical dataset containing data of different plants belonging to the genus Iris. It contains 150 records, 50 for each of the three Iris's variants: Setosa, Virginica and Versicolor. Each record contains the length and the width of both the petal and the sepal of the plant. The standard task, when using this dataset, is to construct a classifier for the type of the Iris, based on the petal and sepal characteristics.</p> <p>To train a classifier with Khiops, we only need a dataframe containing the <code>Iris</code> data:</p>"},{"location":"tutorials/Notebooks/single_table_classifier/#training-the-classifier","title":"Training the Classifier\u00b6","text":"<p>Before training the classifier, we split the data into the feature matrix (sepal length, width, etc) and the target vector containing the labels (the <code>Class</code> column).</p>"},{"location":"tutorials/Notebooks/single_table_classifier/#displaying-the-classifiers-training-accuracy-and-auc","title":"Displaying the Classifiers\u2019 Training Accuracy and AUC\u00b6","text":"<p>The <code>fit</code> method calculates evaluation metrics on the training dataset. We access them via the estimator's attribute <code>model_report_</code> which is an instance of the <code>AnalysisResults</code> class. Let's check this out:</p>"},{"location":"tutorials/Notebooks/single_table_classifier/#deploying-the-classifier-and-displaying-its-test-performance","title":"Deploying the Classifier and Displaying Its Test Performance\u00b6","text":"<p>Now that we have a fitted <code>KhiopsClassifier</code>, we are now going to deploy it on the test split.</p> <p>This can be done in two different ways:</p> <ul> <li>to predict a class that can be obtained using its <code>predict</code>.</li> <li>to predict class probabilities that can be obtained using its <code>predict_proba</code>.</li> </ul> <p>Let's first predict the <code>Iris</code> labels:</p>"},{"location":"tutorials/Notebooks/single_table_classifier_core/","title":"Single-Table Tutorial","text":"In\u00a0[1]: Copied! <pre>import warnings\nimport pandas as pd\nfrom khiops import core as kh\nfrom khiops.tools import download_datasets\n\n# Download the sample datasets from GitHub if not available\nwarnings.filterwarnings(\"ignore\", message=\"Download.*\") # Ignore dataset download warning\ndownload_datasets()\n</pre> import warnings import pandas as pd from khiops import core as kh from khiops.tools import download_datasets  # Download the sample datasets from GitHub if not available warnings.filterwarnings(\"ignore\", message=\"Download.*\") # Ignore dataset download warning download_datasets() In\u00a0[2]: Copied! <pre># Store the locations of the `Iris` dataset files\niris_table_path = f\"{kh.get_samples_dir()}/Iris/Iris.txt\"\niris_kdic_path = f\"{kh.get_samples_dir()}/Iris/Iris.kdic\"\n\n# Print the first lines of the data file\nprint(\"Iris table file:\")\ndisplay(pd.read_csv(iris_table_path, sep=\"\\t\"))\n\n# Print the Khiops dictionary file\nprint(\"Iris dictionary file:\", end=\"\")\nwith open(iris_kdic_path) as iris_kdic_file:\n    print(iris_kdic_file.read(), end=\"\")\n</pre> # Store the locations of the `Iris` dataset files iris_table_path = f\"{kh.get_samples_dir()}/Iris/Iris.txt\" iris_kdic_path = f\"{kh.get_samples_dir()}/Iris/Iris.kdic\"  # Print the first lines of the data file print(\"Iris table file:\") display(pd.read_csv(iris_table_path, sep=\"\\t\"))  # Print the Khiops dictionary file print(\"Iris dictionary file:\", end=\"\") with open(iris_kdic_path) as iris_kdic_file:     print(iris_kdic_file.read(), end=\"\") <pre>Iris table file:\n</pre> SepalLength SepalWidth PetalLength PetalWidth Class 0 5.1 3.5 1.4 0.2 Iris-setosa 1 4.9 3.0 1.4 0.2 Iris-setosa 2 4.7 3.2 1.3 0.2 Iris-setosa 3 4.6 3.1 1.5 0.2 Iris-setosa 4 5.0 3.6 1.4 0.2 Iris-setosa ... ... ... ... ... ... 145 6.7 3.0 5.2 2.3 Iris-virginica 146 6.3 2.5 5.0 1.9 Iris-virginica 147 6.5 3.0 5.2 2.0 Iris-virginica 148 6.2 3.4 5.4 2.3 Iris-virginica 149 5.9 3.0 5.1 1.8 Iris-virginica <p>150 rows \u00d7 5 columns</p> <pre>Iris dictionary file:\nDictionary\tIris\n{\n\tNumerical\tSepalLength\t;\t\n\tNumerical\tSepalWidth\t;\t\n\tNumerical\tPetalLength\t;\t\n\tNumerical\tPetalWidth\t;\t\n\tCategorical\tClass\t;\t\n};\n</pre> <p>Note that the columns described in the dictionary file are coherent with those in the data file. For this training task the features are the first four columns which are all numerical, whereas the target is the Categorical <code>class</code> column.</p> In\u00a0[3]: Copied! <pre>report_path, model_kdic_path = kh.train_predictor(\n    iris_kdic_path,  # Dictionary file path\n    \"Iris\",          # Name of the data dictionary for the table\n    iris_table_path, # Data table file path,\n    \"Class\",         # Target column\n    \"./st_results\"   # Directory to store the target files\n)\n</pre> report_path, model_kdic_path = kh.train_predictor(     iris_kdic_path,  # Dictionary file path     \"Iris\",          # Name of the data dictionary for the table     iris_table_path, # Data table file path,     \"Class\",         # Target column     \"./st_results\"   # Directory to store the target files ) <p>The <code>train_predictor</code> method by default splits the data in 70% train and 30% test; it uses the test split evaluate the model. The method returns the paths of its two output files:</p> <ul> <li>A report file containing the model's information (including evaluation metrics on the train/test split), which can be explored with the Khiops Visualization app or used the Khiops core API.</li> <li>A Khiops dictionary file containing the classifier model</li> </ul> <p>As you can see, Khiops dictionary files may be used to encode classifiers. In fact, they are a very powerful language to transform databases. You may learn more about them here.</p> In\u00a0[4]: Copied! <pre>model_report = kh.read_analysis_results_file(report_path)\ntrain_performance = model_report.train_evaluation_report.get_snb_performance()\ntest_performance = model_report.test_evaluation_report.get_snb_performance()\n</pre> model_report = kh.read_analysis_results_file(report_path) train_performance = model_report.train_evaluation_report.get_snb_performance() test_performance = model_report.test_evaluation_report.get_snb_performance() <p>The <code>iris_train_performance</code> and <code>iris_test_performance</code> are of class <code>PredictorPerformance</code> which has <code>accuracy</code> and <code>auc</code> attributes:</p> In\u00a0[5]: Copied! <pre>print(f\"Iris train accuracy: {train_performance.accuracy}\")\nprint(f\"Iris train AUC     : {train_performance.auc}\")\nprint(f\"Iris test accuracy : {test_performance.accuracy}\")\nprint(f\"Iris test  AUC     : {test_performance.auc}\")\n</pre> print(f\"Iris train accuracy: {train_performance.accuracy}\") print(f\"Iris train AUC     : {train_performance.auc}\") print(f\"Iris test accuracy : {test_performance.accuracy}\") print(f\"Iris test  AUC     : {test_performance.auc}\") <pre>Iris train accuracy: 0.980952\nIris train AUC     : 0.997868\nIris test accuracy : 0.955556\nIris test  AUC     : 0.984362\n</pre> <p>The <code>PredictorPerformance</code> objects have also a confusion matrix attribute:</p> In\u00a0[6]: Copied! <pre>iris_classes = train_performance.confusion_matrix.values\ntrain_confusion_matrix = pd.DataFrame(\n    train_performance.confusion_matrix.matrix,\n    columns=iris_classes,\n    index=iris_classes,\n)\ntest_confusion_matrix = pd.DataFrame(\n    test_performance.confusion_matrix.matrix,\n    columns=iris_classes,\n    index=iris_classes,\n)\nprint(\"Iris train confusion matrix:\")\ndisplay(train_confusion_matrix)\n\nprint(\"Iris test confusion matrix:\")\ndisplay(test_confusion_matrix)\n</pre> iris_classes = train_performance.confusion_matrix.values train_confusion_matrix = pd.DataFrame(     train_performance.confusion_matrix.matrix,     columns=iris_classes,     index=iris_classes, ) test_confusion_matrix = pd.DataFrame(     test_performance.confusion_matrix.matrix,     columns=iris_classes,     index=iris_classes, ) print(\"Iris train confusion matrix:\") display(train_confusion_matrix)  print(\"Iris test confusion matrix:\") display(test_confusion_matrix) <pre>Iris train confusion matrix:\n</pre> Iris-setosa Iris-versicolor Iris-virginica Iris-setosa 38 0 0 Iris-versicolor 0 31 1 Iris-virginica 0 1 34 <pre>Iris test confusion matrix:\n</pre> Iris-setosa Iris-versicolor Iris-virginica Iris-setosa 12 0 0 Iris-versicolor 0 18 2 Iris-virginica 0 0 13 In\u00a0[7]: Copied! <pre>iris_deployed_path = \"./st_results/iris_deployed.txt\"\nkh.deploy_model(\n    model_kdic_path,     # Path of the model dictionary file\n    \"SNB_Iris\",          # Name of the model dictionary\n    iris_table_path,     # Path of the table to deploy the model\n    iris_deployed_path,  # Path of the output (deployed) file\n)\n</pre> iris_deployed_path = \"./st_results/iris_deployed.txt\" kh.deploy_model(     model_kdic_path,     # Path of the model dictionary file     \"SNB_Iris\",          # Name of the model dictionary     iris_table_path,     # Path of the table to deploy the model     iris_deployed_path,  # Path of the output (deployed) file ) <p>The deployed model is in the path in the variable <code>iris_deployed_path</code>, let's have a look at it</p> In\u00a0[8]: Copied! <pre>display(pd.read_csv(iris_deployed_path, sep=\"\\t\"))\n</pre> display(pd.read_csv(iris_deployed_path, sep=\"\\t\")) PredictedClass ProbClassIris-setosa ProbClassIris-versicolor ProbClassIris-virginica 0 Iris-setosa 0.988190 0.008858 0.002951 1 Iris-setosa 0.988190 0.008858 0.002951 2 Iris-setosa 0.988190 0.008858 0.002951 3 Iris-setosa 0.988190 0.008858 0.002951 4 Iris-setosa 0.988190 0.008858 0.002951 ... ... ... ... ... 145 Iris-virginica 0.003303 0.014047 0.982650 146 Iris-virginica 0.003752 0.151320 0.844929 147 Iris-virginica 0.003303 0.014047 0.982650 148 Iris-virginica 0.003303 0.014047 0.982650 149 Iris-virginica 0.003752 0.151320 0.844929 <p>150 rows \u00d7 4 columns</p> <p>The deployed data table file contains four columns</p> <ul> <li><code>PredictedClass</code>: Which contains the class prediction</li> <li><code>ProbClassIris-setosa</code>, <code>ProbClassIris-versicolor</code> and <code>ProbClassIris-virginica</code>: Which contain the probability of each class of <code>Iris</code>.</li> </ul>"},{"location":"tutorials/Notebooks/single_table_classifier_core/#single-table-tutorial-with-the-core-api","title":"Single-Table Tutorial with the core API\u00b6","text":"<p>In this tutorial, we're going to create a classifier on a single-table dataset.</p>"},{"location":"tutorials/Notebooks/single_table_classifier_core/#the-iris-dataset","title":"The Iris Dataset\u00b6","text":"<p>We'll train a classifier for the <code>Iris</code> dataset. This is a classical dataset containing data of different plants belonging to the genus Iris. It contains 150 records, 50 for each of the three Iris's variants: Setosa, Virginica and Versicolor. Each record contains the length and the width of both the petal and the sepal of the plant. The standard task, when using this dataset, is to construct a classifier for the type of the Iris, based on the petal and sepal characteristics.</p> <p>The Khiops core API is file-oriented: It reads and outputs files. In particular, train the classifier on the <code>Iris</code> dataset we need two input files:</p> <ul> <li>A data table file: Usually a CSV or TSV file</li> <li>A Khiops dictionary file: Contains the data table schema under the  KDIC format</li> </ul> <p>The <code>Iris</code> sample dataset contains already these two files. We'll store their locations into variables and take a look both files:</p>"},{"location":"tutorials/Notebooks/single_table_classifier_core/#training-the-classifier","title":"Training the Classifier\u00b6","text":"<p>Let's now train the classifier with <code>train_predictor</code> Khiops core API function:</p>"},{"location":"tutorials/Notebooks/single_table_classifier_core/#displaying-the-classifierss-accuracy-and-auc","title":"Displaying the Classifiers\u2019s Accuracy and AUC\u00b6","text":"<p>Khiops calculates evaluation metrics for the train/test split datasets. We access them by loading the report file into an <code>AnalysisResults</code> object. Let's check this out:</p>"},{"location":"tutorials/Notebooks/single_table_classifier_core/#deploying-the-classifier","title":"Deploying the Classifier\u00b6","text":"<p>We are now going to deploy the <code>Iris</code> classifier that we have just trained.</p> <p>To this end we use the model dictionary file that the <code>train_predictor</code> function created in conjunction the the <code>deploy_model</code> core API function. Note that the name of the dictionary for the model is <code>SNB_Iris</code>.</p> <p>For simplicity, we'll just deploy on the whole data table file (one usually would do this on new data):</p>"}]}